{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üë®üèª‚Äçüè´ Create Deployment </span>\n",
    "\n",
    "Creating a deployment for the recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hopsworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-15 16:16:21,943 INFO: Initializing external client\n",
      "2025-06-15 16:16:21,946 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-06-15 16:16:23,829 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Connect to Hopsworks Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "dataset_api = project.get_dataset_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üöÄ Ranking Model Deployment </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ranking model is a CatBoost model we need to implement a `Predict` class that tells Hopsworks how to load the model and how to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name: 'weather_ranking_model', version: 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_ranking_model = mr.get_best_model(\n",
    "    name=\"weather_ranking_model\", \n",
    "    metric=\"fscore\", \n",
    "    direction=\"max\",\n",
    ")\n",
    "weather_ranking_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name: 'no_weather_ranking_model', version: 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_weather_ranking_model = mr.get_best_model(\n",
    "    name=\"no_weather_ranking_model\", \n",
    "    metric=\"fscore\", \n",
    "    direction=\"max\",\n",
    ")\n",
    "no_weather_ranking_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_ranking_transformer.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import hopsworks\n",
    "import logging\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        # Connect to Hopsworks\n",
    "        project = hopsworks.connection().get_project()\n",
    "        self.fs = project.get_feature_store()\n",
    "        self.events_fv = self.fs.get_feature_view(name=\"events\", version=1)\n",
    "        self.event_features = [feat.name for feat in self.events_fv.schema]\n",
    "        self.users_fv = self.fs.get_feature_view(name=\"users\", version=1)\n",
    "        self.candidate_index = self.fs.get_feature_view(name=\"candidate_embeddings\", version=1)\n",
    "        mr = project.get_model_registry()\n",
    "        model = mr.get_model(name=\"weather_ranking_model\", version=1)\n",
    "        input_schema = model.model_schema[\"input_schema\"][\"columnar_schema\"]\n",
    "        self.ranking_model_feature_names = [feat[\"name\"] for feat in input_schema]\n",
    "        self.current_event_ids = []\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        print(\"Transformer preprocess input:\", inputs)\n",
    "        if isinstance(inputs, dict) and \"instances\" in inputs:\n",
    "            instance = inputs[\"instances\"][0]\n",
    "        else:\n",
    "            instance = inputs[0] if isinstance(inputs, list) else inputs\n",
    "\n",
    "        user_id = instance.get(\"user_id\")\n",
    "        query_emb = instance.get(\"query_emb\")\n",
    "        if user_id is None or query_emb is None:\n",
    "            raise ValueError(\"user_id or query_emb missing in input\")\n",
    "\n",
    "        # Find candidate event IDs\n",
    "        neighbors = self.candidate_index.find_neighbors(query_emb, k=100)\n",
    "        candidate_ids = [n[0] for n in neighbors]\n",
    "\n",
    "        # Retrieve event data\n",
    "        events_data = [self.events_fv.get_feature_vector({\"event_id\": eid}) for eid in candidate_ids]\n",
    "        events_df = pd.DataFrame(events_data, columns=self.event_features)\n",
    "\n",
    "        # Filter to future events only\n",
    "        current_date = datetime.datetime.now().date()\n",
    "        events_df[\"start_date\"] = pd.to_datetime(events_df[\"start_time\"]).dt.date\n",
    "        valid_events = events_df[events_df[\"start_date\"] >= current_date]\n",
    "\n",
    "        # If no valid events, return empty instances and store empty event_ids\n",
    "        if valid_events.empty:\n",
    "            self.current_event_ids = []\n",
    "            print(\"No valid events found for user\", user_id)\n",
    "            return {\"instances\": []}\n",
    "\n",
    "        # Merge user features\n",
    "        user_features = self.users_fv.get_feature_vector({\"user_id\": user_id}, return_type=\"pandas\")\n",
    "        required_user_cols = [\n",
    "            \"user_id\", \"user_city\", \"age\", \"user_interests\", \"indoor_outdoor_preference\",\n",
    "            \"user_weather_condition\", \"user_temperature\", \"user_precipitation\"\n",
    "        ]\n",
    "        for col in required_user_cols:\n",
    "            if col not in user_features.columns:\n",
    "                raise ValueError(f\"Missing user feature: {col}\")\n",
    "        valid_events[required_user_cols] = user_features[required_user_cols].values[0]\n",
    "\n",
    "        # Select only the features required by the ranking model\n",
    "        ranking_features = valid_events[self.ranking_model_feature_names]\n",
    "\n",
    "        # Store event IDs for postprocess\n",
    "        self.current_event_ids = valid_events[\"event_id\"].tolist()\n",
    "        print(\"Number of valid events:\", len(self.current_event_ids))\n",
    "        print(\"Output from preprocess:\", {\"instances\": ranking_features.values.tolist()})\n",
    "\n",
    "        return {\"instances\": ranking_features.values.tolist()}\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        print(\"Transformer postprocess input:\", outputs)\n",
    "        predictions = outputs.get(\"predictions\", [])\n",
    "        if len(predictions) != len(self.current_event_ids):\n",
    "            print(\"Mismatch between predictions and event IDs\")\n",
    "            raise ValueError(\"Mismatch between predictions and event IDs\")\n",
    "        ranking = list(zip(predictions, self.current_event_ids))\n",
    "        ranking.sort(reverse=True)\n",
    "        print(\"Postprocess returning\", len(ranking), \"ranked results\")\n",
    "        # Return both ranking and debug info\n",
    "        return {\n",
    "            \"ranking\": ranking,\n",
    "            \"debug\": f\"Number of valid events: {len(self.current_event_ids)}\"\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f67488d91b48f49d147cbc82a1efb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_transformer.py: 0.000%|         ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer script path in Hopsworks: /Projects/Weather_BasedEventRecSys/Resources/weather_ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "# Upload the transformer script\n",
    "weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"weather_ranking_transformer.py\",  # File name to be uploaded\n",
    "    \"Resources\",                       # Destination directory in Hopsworks File System\n",
    "    overwrite=True,                    # Overwrite the file if it already exists\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded transformer script\n",
    "weather_transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                       # Root directory for projects in Hopsworks\n",
    "    project.name,                      # Name of the current project\n",
    "    weather_uploaded_file_path,        # Path to the uploaded file within the project\n",
    ")\n",
    "\n",
    "print(\"Transformer script path in Hopsworks:\", weather_transformer_script_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    def __init__(self):\n",
    "        # Load the model from the environment variable\n",
    "        model_path = os.environ[\"MODEL_FILES_PATH\"]\n",
    "        self.model = joblib.load(os.path.join(model_path, \"weather_ranking_model.pkl\"))\n",
    "        logging.info(\"Model loaded successfully\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # The inputs will be a dict with a list of lists under \"instances\"\n",
    "        # Example: {\"instances\": [[feature1, feature2, ...], ...]}\n",
    "        features = inputs[\"instances\"]\n",
    "        logging.info(f\"Predict received {len(features)} instances\")\n",
    "        logging.info(f\"Feature shape (if available): {np.array(features).shape if features else 'empty'}\")\n",
    "\n",
    "        # Predict probabilities for the positive class\n",
    "        # (Assuming your model is a binary classifier with predict_proba)\n",
    "        scores = self.model.predict_proba(features)[:, 1].tolist()\n",
    "\n",
    "        # Return the scores (event_ids are not passed here, handle in postprocessing if needed)\n",
    "        return {\"predictions\": scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db05addcdee40e996b030783acc5db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_predictor.py: 0.000%|          |‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor_uploaded_file_path = dataset_api.upload(\n",
    "    \"weather_ranking_predictor.py\",\n",
    "    \"Resources\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "weather_predictor_script_path = os.path.join(\n",
    "    \"/Projects\",\n",
    "    project.name,\n",
    "    predictor_uploaded_file_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1220788/deployments/375818\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "ranking_deployment_name = \"weatherrankingdeployment\"\n",
    "\n",
    "# Define transformer\n",
    "weather_ranking_transformer=Transformer(\n",
    "    script_file=weather_transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy ranking model\n",
    "weather_ranking_deployment = weather_ranking_model.deploy(\n",
    "    name=ranking_deployment_name,\n",
    "    description=\"Deployment that search for event candidates and scores them based on user metadata\",\n",
    "    script_file=weather_predictor_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=weather_ranking_transformer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d295c13fa08b48e38b052af4e4f370cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "weather_ranking_deployment.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelServingException",
     "evalue": "Instances field should contain a 2-dim list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelServingException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 16\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [candidate[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m ranked_candidates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mranking\u001b[39m\u001b[38;5;124m'\u001b[39m][:k]]\n\u001b[1;32m      4\u001b[0m test_ranking_input \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLT819S\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }]\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 16\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mweather_ranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ranking_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(ranked_candidates, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:231\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     data: Union[Dict, InferInput] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     inputs: Union[List, Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m ):\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        `hopsworks.client.exceptions.RestAPIError`: In case the backend encounters an issue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:570\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance\u001b[38;5;241m.\u001b[39mmodel_server \u001b[38;5;241m==\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mMODEL_SERVER_VLLM:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference requests to LLM deployments are not supported by the `predict` method. Please, use any OpenAI API-compatible client instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     )\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inference_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# build inference payload based on API protocol\u001b[39;00m\n\u001b[1;32m    573\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_payload(\n\u001b[1;32m    574\u001b[0m     deployment_instance\u001b[38;5;241m.\u001b[39mapi_protocol, data, inputs\n\u001b[1;32m    575\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:613\u001b[0m, in \u001b[0;36mServingEngine._validate_inference_payload\u001b[0;34m(self, api_protocol, data, inputs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# check data or inputs\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inference_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inference_inputs(api_protocol, inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:641\u001b[0m, in \u001b[0;36mServingEngine._validate_inference_data\u001b[0;34m(self, api_protocol, data)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference data cannot contain an empty list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(payload[\u001b[38;5;241m0\u001b[39m], List):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstances field should contain a 2-dim list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(payload[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference data cannot contain an empty list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n",
      "\u001b[0;31mModelServingException\u001b[0m: Instances field should contain a 2-dim list."
     ]
    }
   ],
   "source": [
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates['ranking'][:k]]\n",
    "\n",
    "test_ranking_input = {\n",
    "    \"instances\": [{\n",
    "        \"user_id\": \"LT819S\",\n",
    "        \"query_emb\": [0.214135289, 0.571055949, 0.330709577, -0.225899458, -0.308674961, \n",
    "                 -0.0115124583, 0.0730511621, -0.495835781, 0.625569344, -0.0438038409, \n",
    "                 0.263472944, -0.58485353, -0.307070434, 0.0414443575, -0.321789205, \n",
    "                 0.966559, 0.127463, -0.392714, 0.845132, -0.512387, 0.253901, \n",
    "                 -0.764589, 0.431267, 0.087342, -0.629045, 0.318976, -0.146782, \n",
    "                 0.573921, -0.087625, 0.934261, -0.271843, 0.652197]  # Your full query_emb\n",
    "    }]\n",
    "}\n",
    "\n",
    "ranked_candidates = weather_ranking_deployment.predict(test_ranking_input)\n",
    "recommendations = get_top_recommendations(ranked_candidates, k=3)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_ranking_transformer.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import hopsworks\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "import logging\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        project = hopsworks.connection().get_project()\n",
    "        self.fs = project.get_feature_store()\n",
    "        self.events_fv = self.fs.get_feature_view(name=\"events\", version=1)\n",
    "        self.event_features = [feat.name for feat in self.events_fv.schema]\n",
    "        self.users_fv = self.fs.get_feature_view(name=\"users\", version=1)\n",
    "        self.candidate_index = self.fs.get_feature_view(name=\"candidate_embeddings\", version=1)\n",
    "        mr = project.get_model_registry()\n",
    "        model = mr.get_model(name=\"weather_ranking_model\", version=1)\n",
    "        input_schema = model.model_schema[\"input_schema\"][\"columnar_schema\"]\n",
    "        self.ranking_model_feature_names = [feat[\"name\"] for feat in input_schema]\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        # Extract the input instance (supports both list and dict input)\n",
    "        if isinstance(inputs, dict) and \"instances\" in inputs:\n",
    "            instance = inputs[\"instances\"][0]\n",
    "        else:\n",
    "            instance = inputs[0] if isinstance(inputs, list) else inputs\n",
    "        # Get user_id and query_emb (handle both dict and list of dicts)\n",
    "        user_id = instance.get(\"user_id\") if isinstance(instance, dict) else instance[0].get(\"user_id\")\n",
    "        query_emb = instance.get(\"query_emb\") if isinstance(instance, dict) else instance[0].get(\"query_emb\")\n",
    "        if user_id is None or query_emb is None:\n",
    "            raise ValueError(\"user_id or query_emb missing in input\")\n",
    "\n",
    "        # Get candidate event IDs\n",
    "        neighbors = self.candidate_index.find_neighbors(query_emb, k=100)\n",
    "        candidate_ids = [n[0] for n in neighbors]\n",
    "\n",
    "        # Get full event data\n",
    "        events_data = [self.events_fv.get_feature_vector({\"event_id\": eid}) for eid in candidate_ids]\n",
    "        events_df = pd.DataFrame(events_data, columns=self.event_features)\n",
    "\n",
    "        # Filter future events\n",
    "        current_date = datetime.datetime.now().date()\n",
    "        events_df[\"start_date\"] = pd.to_datetime(events_df[\"start_time\"]).dt.date\n",
    "        valid_events = events_df[events_df[\"start_date\"] >= current_date]\n",
    "\n",
    "        # Merge user features\n",
    "        user_features = self.users_fv.get_feature_vector({\"user_id\": user_id}, return_type=\"pandas\")\n",
    "        # Make sure user_features is not empty and has the required columns\n",
    "        required_user_cols = [\"user_id\", \"user_city\", \"age\", \"user_interests\", \"indoor_outdoor_preference\",\n",
    "                             \"user_weather_condition\", \"user_temperature\", \"user_precipitation\"]\n",
    "        for col in required_user_cols:\n",
    "            if col not in user_features.columns:\n",
    "                raise ValueError(f\"Missing user feature: {col}\")\n",
    "        # Assign user features to all rows\n",
    "        valid_events[required_user_cols] = user_features[required_user_cols].values[0]\n",
    "\n",
    "        # Select only the features required by the ranking model\n",
    "        ranking_features = valid_events[self.ranking_model_feature_names]\n",
    "        return {\n",
    "            \"inputs\": [{\n",
    "                \"ranking_features\": ranking_features.values.tolist(),\n",
    "                \"event_ids\": valid_events[\"event_id\"].tolist()\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        preds = outputs[\"predictions\"]\n",
    "        ranking = list(zip(preds[\"scores\"], preds[\"event_ids\"]))\n",
    "        ranking.sort(reverse=True)\n",
    "        return {\"ranking\": ranking}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61766966d36f40808827ffe4f3adebdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_transformer.py: 0.000%|         ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System \n",
    "weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"weather_ranking_transformer.py\",    # File name to be uploaded\n",
    "    \"Resources\",                 # Destination directory in Hopsworks File System \n",
    "    overwrite=True,              # Overwrite the file if it already exists\n",
    ") \n",
    "\n",
    "# Construct the path to the uploaded transformer script\n",
    "weather_transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                 # Root directory for projects in Hopsworks\n",
    "    project.name,                # Name of the current project\n",
    "    weather_uploaded_file_path,          # Path to the uploaded file within the project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile weather_ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    def __init__(self):\n",
    "        model_path = os.environ[\"MODEL_FILES_PATH\"]\n",
    "        self.model = joblib.load(os.path.join(model_path, \"weather_ranking_model.pkl\"))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if isinstance(inputs, list) and len(inputs) > 0 and isinstance(inputs[0], dict):\n",
    "            features = inputs[0].pop(\"ranking_features\")\n",
    "            event_ids = inputs[0].pop(\"event_ids\")\n",
    "        else:\n",
    "            raise ValueError(\"Inputs must be a list with one dict\")\n",
    "        logging.info(f\"predict -> features shape: {len(features)}x{len(features[0]) if features else 0}\")\n",
    "        scores = self.model.predict_proba(features)[:, 1].tolist()\n",
    "        return {\"scores\": scores, \"event_ids\": event_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8011263ef04dae9ba8ff4aacbe5ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_predictor.py: 0.000%|          |‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Upload predictor file to Hopsworks\n",
    "weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"weather_ranking_predictor.py\", \n",
    "    \"Resources\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "weather_predictor_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    weather_uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1220788/deployments/374791\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from hsml.transformer import Transformer\n",
    "\n",
    "ranking_deployment_name = \"weatherrankingdeployment\"\n",
    "\n",
    "# Define transformer\n",
    "weather_ranking_transformer=Transformer(\n",
    "    script_file=weather_transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy ranking model\n",
    "weather_ranking_deployment = weather_ranking_model.deploy(\n",
    "    name=ranking_deployment_name,\n",
    "    description=\"Deployment that search for event candidates and scores them based on user metadata\",\n",
    "    script_file=weather_predictor_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=weather_ranking_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c928423be4460b9228a724365fbd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "weather_ranking_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelServingException",
     "evalue": "Instances field should contain a 2-dim list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelServingException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 17\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Test ranking deployment\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_ranking_input \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLT819S\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }]\n\u001b[1;32m     15\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mweather_ranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ranking_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(ranked_candidates, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:231\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     data: Union[Dict, InferInput] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     inputs: Union[List, Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m ):\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        `hopsworks.client.exceptions.RestAPIError`: In case the backend encounters an issue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:570\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance\u001b[38;5;241m.\u001b[39mmodel_server \u001b[38;5;241m==\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mMODEL_SERVER_VLLM:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference requests to LLM deployments are not supported by the `predict` method. Please, use any OpenAI API-compatible client instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     )\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inference_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# build inference payload based on API protocol\u001b[39;00m\n\u001b[1;32m    573\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_payload(\n\u001b[1;32m    574\u001b[0m     deployment_instance\u001b[38;5;241m.\u001b[39mapi_protocol, data, inputs\n\u001b[1;32m    575\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:613\u001b[0m, in \u001b[0;36mServingEngine._validate_inference_payload\u001b[0;34m(self, api_protocol, data, inputs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# check data or inputs\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inference_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inference_inputs(api_protocol, inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:641\u001b[0m, in \u001b[0;36mServingEngine._validate_inference_data\u001b[0;34m(self, api_protocol, data)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference data cannot contain an empty list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(payload[\u001b[38;5;241m0\u001b[39m], List):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstances field should contain a 2-dim list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(payload[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference data cannot contain an empty list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n",
      "\u001b[0;31mModelServingException\u001b[0m: Instances field should contain a 2-dim list."
     ]
    }
   ],
   "source": [
    "\n",
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates['ranking'][:k]]\n",
    "\n",
    "\n",
    "# Define a test input example for the ranking model\n",
    "test_ranking_input = {\"instances\": [[{\n",
    "    \"user_id\": \"LT819S\",  \n",
    "    \"query_emb\": [0.214135289, 0.571055949, 0.330709577, -0.225899458, -0.308674961, \n",
    "                 -0.0115124583, 0.0730511621, -0.495835781, 0.625569344, -0.0438038409, \n",
    "                 0.263472944, -0.58485353, -0.307070434, 0.0414443575, -0.321789205, \n",
    "                 0.966559, 0.127463, -0.392714, 0.845132, -0.512387, 0.253901, \n",
    "                 -0.764589, 0.431267, 0.087342, -0.629045, 0.318976, -0.146782, \n",
    "                 0.573921, -0.087625, 0.934261, -0.271843, 0.652197]\n",
    "}]]}\n",
    "\n",
    "# Test ranking deployment\n",
    "ranked_candidates = weather_ranking_deployment.predict(test_ranking_input)\n",
    "\n",
    "# Retrieve event IDs of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates, k=3)\n",
    "recommendations\n",
    "\n",
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates['ranking'][:k]]\n",
    "\n",
    "# Test ranking deployment\n",
    "test_ranking_input = {\n",
    "    \"instances\": [{\n",
    "        \"user_id\": \"LT819S\",\n",
    "        \"query_emb\": [0.214135289, 0.571055949, 0.330709577, -0.225899458, -0.308674961, \n",
    "                 -0.0115124583, 0.0730511621, -0.495835781, 0.625569344, -0.0438038409, \n",
    "                 0.263472944, -0.58485353, -0.307070434, 0.0414443575, -0.321789205, \n",
    "                 0.966559, 0.127463, -0.392714, 0.845132, -0.512387, 0.253901, \n",
    "                 -0.764589, 0.431267, 0.087342, -0.629045, 0.318976, -0.146782, \n",
    "                 0.573921, -0.087625, 0.934261, -0.271843, 0.652197]\n",
    "    }]\n",
    "}\n",
    "\n",
    "ranked_candidates = weather_ranking_deployment.predict(test_ranking_input)\n",
    "recommendations = get_top_recommendations(ranked_candidates, k=3)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1220788/deployments/374789\n",
      "\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1220788/serving/374789/logs). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":240027,\"errorMsg\":\"Server logs not available\"}', error code: 240027, error msg: Server logs not available, user msg: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check logs in case of failure\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mweather_ranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:270\u001b[0m, in \u001b[0;36mDeployment.get_logs\u001b[0;34m(self, component, tail)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m components:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComponent \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not valid. Possible values are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    266\u001b[0m             component, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(components)\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m logs:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:554\u001b[0m, in \u001b[0;36mServingEngine.get_logs\u001b[0;34m(self, deployment_instance, component, tail)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment is starting, server logs might not be ready yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplore all the logs and filters in the Kibana logs at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;241m+\u001b[39m deployment_instance\u001b[38;5;241m.\u001b[39mget_url(),\n\u001b[1;32m    551\u001b[0m     end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    552\u001b[0m )\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/core/serving_api.py:409\u001b[0m, in \u001b[0;36mServingApi.get_logs\u001b[0;34m(self, deployment_instance, component, tail)\u001b[0m\n\u001b[1;32m    400\u001b[0m path_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    402\u001b[0m     _client\u001b[38;5;241m.\u001b[39m_project_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m ]\n\u001b[1;32m    407\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m\"\u001b[39m: component, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m: tail}\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deployable_component_logs\u001b[38;5;241m.\u001b[39mDeployableComponentLogs\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[0;32m--> 409\u001b[0m     \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/decorators.py:48\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/client/base.py:186\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[0m\n\u001b[1;32m    181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[1;32m    182\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1220788/serving/374789/logs). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":240027,\"errorMsg\":\"Server logs not available\"}', error code: 240027, error msg: Server logs not available, user msg: "
     ]
    }
   ],
   "source": [
    "# Check logs in case of failure\n",
    "weather_ranking_deployment.get_logs(component=\"predictor\", tail=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_ranking_transformer.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import hopsworks\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            # Connect to Hopsworks\n",
    "            project = hopsworks.connection().get_project()\n",
    "            self.fs = project.get_feature_store()\n",
    "            \n",
    "            # Retrieve the 'events' feature view\n",
    "            self.events_fv = self.fs.get_feature_view(\n",
    "                name=\"events\", \n",
    "                version=1,\n",
    "            )\n",
    "            \n",
    "            # Get list of feature names for events\n",
    "            self.event_features = [feat.name for feat in self.events_fv.schema]\n",
    "            logger.info(f\"Event features: {self.event_features}\")\n",
    "            \n",
    "            # Retrieve the 'users' feature view\n",
    "            self.users_fv = self.fs.get_feature_view(\n",
    "                name=\"users\", \n",
    "                version=1,\n",
    "            )\n",
    "            \n",
    "            # Get list of user features\n",
    "            self.user_features_list = [feat.name for feat in self.users_fv.schema]\n",
    "            logger.info(f\"User features: {self.user_features_list}\")\n",
    "\n",
    "            # Retrieve the 'candidate_embeddings' feature view\n",
    "            self.candidate_index = self.fs.get_feature_view(\n",
    "                name=\"candidate_embeddings\", \n",
    "                version=1,\n",
    "            )\n",
    "\n",
    "            # Retrieve ranking model\n",
    "            mr = project.get_model_registry()\n",
    "            model = mr.get_model(\n",
    "                name=\"weather_ranking_model\", \n",
    "                version=1,\n",
    "            )\n",
    "            \n",
    "            # Extract input schema from the model\n",
    "            input_schema = model.model_schema[\"input_schema\"][\"columnar_schema\"]\n",
    "            \n",
    "            # Get the names of features expected by the ranking model\n",
    "            self.ranking_model_feature_names = [feat[\"name\"] for feat in input_schema]\n",
    "            logger.info(f\"Ranking model features: {self.ranking_model_feature_names}\")\n",
    "            \n",
    "            # Define query and candidate features\n",
    "            self.query_features = [\"user_id\", \"user_city\", \"age\", \"user_interests\"]\n",
    "            self.candidate_features = [\"event_id\", \"title\", \"event_type\", \"event_city\"]\n",
    "            \n",
    "            logger.info(\"Transformer initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing transformer: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def preprocess(self, inputs):\n",
    "        try:\n",
    "            logger.info(f\"Input structure: {inputs}\")\n",
    "            \n",
    "            # Handle different input formats\n",
    "            if isinstance(inputs[\"instances\"], list):\n",
    "                if len(inputs[\"instances\"]) > 0:\n",
    "                    if isinstance(inputs[\"instances\"][0], list):\n",
    "                        # Handle double nested list format: {\"instances\": [[{...}]]}\n",
    "                        instance = inputs[\"instances\"][0][0]\n",
    "                    else:\n",
    "                        # Handle single nested list format: {\"instances\": [{...}]}\n",
    "                        instance = inputs[\"instances\"][0]\n",
    "                else:\n",
    "                    logger.warning(\"Empty instances list\")\n",
    "                    return {\"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]}\n",
    "            else:\n",
    "                # Handle direct format: {\"instances\": {...}}\n",
    "                instance = inputs[\"instances\"]\n",
    "\n",
    "            # Extract user_id from inputs\n",
    "            user_id = instance[\"user_id\"]\n",
    "            logger.info(f\"Processing for user_id: {user_id}\")\n",
    "            \n",
    "            # Check if query_emb exists\n",
    "            if \"query_emb\" not in instance:\n",
    "                logger.error(\"No query_emb found in input\")\n",
    "                return {\"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]}\n",
    "            \n",
    "            # Log query embedding size\n",
    "            logger.info(f\"Query embedding size: {len(instance['query_emb'])}\")\n",
    "            \n",
    "            # Search for candidate items\n",
    "            try:\n",
    "                neighbors = self.candidate_index.find_neighbors(\n",
    "                    instance[\"query_emb\"], \n",
    "                    k=100,\n",
    "                )\n",
    "                neighbors = [neighbor[0] for neighbor in neighbors]\n",
    "                logger.info(f\"Found {len(neighbors)} neighbors\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error finding neighbors: {str(e)}\")\n",
    "                return {\"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]}\n",
    "            \n",
    "            # If no neighbors found, return empty result\n",
    "            if not neighbors:\n",
    "                logger.warning(\"No neighbors found\")\n",
    "                return {\"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]}\n",
    "            \n",
    "            # Filter candidate events \n",
    "            event_id_list = [event_id for event_id in neighbors]\n",
    "            event_id_df = pd.DataFrame({\"event_id\": event_id_list})\n",
    "            \n",
    "            # Retrieve event data for candidate events\n",
    "            events_data = []\n",
    "            for event_id in event_id_list:\n",
    "                try:\n",
    "                    event = self.events_fv.get_feature_vector({\"event_id\": event_id})\n",
    "                    events_data.append(event)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not get features for event {event_id}: {str(e)}\")\n",
    "            \n",
    "            # If no events data, return empty result\n",
    "            if not events_data:\n",
    "                logger.warning(\"No valid events found\")\n",
    "                return {\"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]}\n",
    "            \n",
    "            events_df = pd.DataFrame(\n",
    "                data=events_data, \n",
    "                columns=self.event_features,\n",
    "            )\n",
    "            logger.info(f\"Retrieved {len(events_df)} events\")\n",
    "            \n",
    "            # Join candidate items with their features\n",
    "            ranking_model_inputs = event_id_df.merge(\n",
    "                events_df, \n",
    "                on=\"event_id\", \n",
    "                how=\"inner\",\n",
    "            )\n",
    "            logger.info(f\"After merge: {len(ranking_model_inputs)} events\")\n",
    "            \n",
    "            # Add user features\n",
    "            try:\n",
    "                user_features = self.users_fv.get_feature_vector(\n",
    "                    {\"user_id\": user_id}, \n",
    "                    return_type=\"pandas\",\n",
    "                )\n",
    "                logger.info(f\"User features columns: {user_features.columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting user features: {str(e)}\")\n",
    "                # Continue with empty user features\n",
    "                user_features = pd.DataFrame()\n",
    "            \n",
    "            # Add user features from query features list\n",
    "            for feature in self.query_features:\n",
    "                if feature in user_features.columns and len(user_features[feature].values) > 0:\n",
    "                    ranking_model_inputs[feature] = user_features[feature].values[0]\n",
    "                else:\n",
    "                    # Add default value if feature not found\n",
    "                    ranking_model_inputs[feature] = None\n",
    "                    logger.warning(f\"User feature {feature} not found, using None\")\n",
    "            \n",
    "            # Check if we have all required features\n",
    "            missing_features = [f for f in self.ranking_model_feature_names if f not in ranking_model_inputs.columns]\n",
    "            if missing_features:\n",
    "                logger.warning(f\"Missing features: {missing_features}\")\n",
    "                # Add missing features with None values\n",
    "                for feature in missing_features:\n",
    "                    ranking_model_inputs[feature] = None\n",
    "            \n",
    "            # Select only the features required by the ranking model\n",
    "            available_features = [f for f in self.ranking_model_feature_names if f in ranking_model_inputs.columns]\n",
    "            \n",
    "            # If no available features, return empty result\n",
    "            if not available_features:\n",
    "                logger.error(\"No available features for ranking model\")\n",
    "                return {\"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]}\n",
    "            \n",
    "            ranking_model_inputs = ranking_model_inputs[available_features]\n",
    "            \n",
    "            # Check for NaN values and replace with default values\n",
    "            ranking_model_inputs = ranking_model_inputs.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f3b8397969443da57ee419f2b4ee66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_transformer.py: 0.000%|         ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System \n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"/home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_transformer.py\",    # File name to be uploaded\n",
    "    \"Resources\",                 # Destination directory in Hopsworks File System \n",
    "    overwrite=True,              # Overwrite the file if it already exists\n",
    ") \n",
    "\n",
    "# Construct the path to the uploaded transformer script\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                 # Root directory for projects in Hopsworks\n",
    "    project.name,                # Name of the current project\n",
    "    uploaded_file_path,          # Path to the uploaded file within the project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # List the directory contents to debug\n",
    "        artifact_path = os.environ[\"ARTIFACT_FILES_PATH\"]\n",
    "        model_path = os.environ[\"MODEL_FILES_PATH\"]\n",
    "        logging.info(f\"Artifact path contents: {os.listdir(artifact_path)}\")\n",
    "        logging.info(f\"Model path contents: {os.listdir(model_path)}\")\n",
    "        \n",
    "        # Try loading from MODEL_FILES_PATH instead\n",
    "        self.model = joblib.load(os.path.join(model_path, \"weather_ranking_model.pkl\"))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        try:\n",
    "            # Add detailed logging to help diagnose the issue\n",
    "            logging.info(f\"Input structure: {inputs}\")\n",
    "            \n",
    "            # Handle different input formats\n",
    "            if isinstance(inputs, list) and len(inputs) > 0:\n",
    "                # If inputs is a list, get the first element\n",
    "                input_data = inputs[0]\n",
    "            else:\n",
    "                # If inputs is not a list, use it directly\n",
    "                input_data = inputs\n",
    "            \n",
    "            # Check if input_data has the expected keys\n",
    "            if not isinstance(input_data, dict) or \"ranking_features\" not in input_data or \"event_ids\" not in input_data:\n",
    "                logging.error(f\"Invalid input format: {input_data}\")\n",
    "                return {\"scores\": [], \"event_ids\": []}\n",
    "            \n",
    "            # Extract ranking features and event IDs\n",
    "            features = input_data[\"ranking_features\"]\n",
    "            event_ids = input_data[\"event_ids\"]\n",
    "            \n",
    "            # Log the extracted features\n",
    "            logging.info(f\"Features: {features}\")\n",
    "            logging.info(f\"Event IDs: {event_ids}\")\n",
    "            \n",
    "            # If features is empty, return empty results\n",
    "            if not features:\n",
    "                return {\"scores\": [], \"event_ids\": []}\n",
    "            \n",
    "            # Predict probabilities for the positive class\n",
    "            scores = self.model.predict_proba(features).tolist()\n",
    "            \n",
    "            # Get scores of positive class\n",
    "            scores = np.asarray(scores)[:,1].tolist() \n",
    "            \n",
    "            # Return the predicted scores along with the corresponding event IDs\n",
    "            return {\n",
    "                \"scores\": scores, \n",
    "                \"event_ids\": event_ids,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Add detailed logging to help diagnose the issue\n",
    "            logging.error(f\"Error in predict: {str(e)}\")\n",
    "            logging.error(f\"Input structure: {inputs}\")\n",
    "            # Return empty result on error\n",
    "            return {\"scores\": [], \"event_ids\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing weather_ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile weather_ranking_transformer.py\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# import hopsworks\n",
    "# from opensearchpy import OpenSearch\n",
    "\n",
    "# import logging\n",
    "\n",
    "\n",
    "# class Transformer(object):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         # Connect to Hopsworks\n",
    "#         project = hopsworks.connection().get_project()\n",
    "#         self.fs = project.get_feature_store()\n",
    "        \n",
    "#         # Retrieve the 'events' feature view\n",
    "#         self.events_fv = self.fs.get_feature_view(\n",
    "#             name=\"events\", \n",
    "#             version=1,\n",
    "#         )\n",
    "        \n",
    "#         # Get list of feature names for events\n",
    "#         self.event_features = [feat.name for feat in self.events_fv.schema]\n",
    "        \n",
    "#         # Retrieve the 'users' feature view\n",
    "#         self.users_fv = self.fs.get_feature_view(\n",
    "#             name=\"users\", \n",
    "#             version=1,\n",
    "#         )\n",
    "\n",
    "#         # Retrieve the 'candidate_embeddings' feature view\n",
    "#         self.candidate_index = self.fs.get_feature_view(\n",
    "#             name=\"candidate_embeddings\", \n",
    "#             version=1,\n",
    "#         )\n",
    "\n",
    "#         # Retrieve ranking model\n",
    "#         mr = project.get_model_registry()\n",
    "#         model = mr.get_model(\n",
    "#             name=\"weather_ranking_model\", \n",
    "#             version=1,\n",
    "#         )\n",
    "        \n",
    "#         # Extract input schema from the model\n",
    "#         input_schema = model.model_schema[\"input_schema\"][\"columnar_schema\"]\n",
    "        \n",
    "#         # Get the names of features expected by the ranking model\n",
    "#         self.ranking_model_feature_names = [feat[\"name\"] for feat in input_schema]\n",
    "        \n",
    "#         # Define specific features we need based on the provided lists\n",
    "#         self.query_features = [\"user_id\", \"user_city\", \"age\", \"user_interests\"]\n",
    "#         self.candidate_features = [\"event_id\", \"title\", \"event_type\", \"event_city\"]\n",
    "            \n",
    "#     def preprocess(self, inputs):\n",
    "#         try:\n",
    "#             # Extract the input instance - handle both formats\n",
    "#             if isinstance(inputs[\"instances\"], list) and len(inputs[\"instances\"]) > 0:\n",
    "#                 if isinstance(inputs[\"instances\"][0], list):\n",
    "#                     # Handle double nested list format: {\"instances\": [[{...}]]}\n",
    "#                     instance = inputs[\"instances\"][0][0]\n",
    "#                 else:\n",
    "#                     # Handle single nested list format: {\"instances\": [{...}]}\n",
    "#                     instance = inputs[\"instances\"][0]\n",
    "#             else:\n",
    "#                 # Handle direct format: {\"instances\": {...}}\n",
    "#                 instance = inputs[\"instances\"]\n",
    "\n",
    "#             # Extract user_id from inputs\n",
    "#             user_id = instance[\"user_id\"]\n",
    "            \n",
    "#             # Search for candidate items\n",
    "#             neighbors = self.candidate_index.find_neighbors(\n",
    "#                 instance[\"query_emb\"], \n",
    "#                 k=100,\n",
    "#             )\n",
    "#             neighbors = [neighbor[0] for neighbor in neighbors]\n",
    "            \n",
    "#             # Get user features - focusing on the specific query features\n",
    "#             user_features = self.users_fv.get_feature_vector(\n",
    "#                 {\"user_id\": user_id}, \n",
    "#                 return_type=\"pandas\",\n",
    "#             )\n",
    "            \n",
    "#             # Get user interests\n",
    "#             user_interests = user_features[\"user_interests\"].values[0].split(\",\") if \"user_interests\" in user_features.columns else []\n",
    "            \n",
    "#             # Get current date\n",
    "#             current_date = datetime.now()\n",
    "            \n",
    "#             # Retrieve event data for candidate events\n",
    "#             events_data = [\n",
    "#                 self.events_fv.get_feature_vector({\"event_id\": event_id}) \n",
    "#                 for event_id \n",
    "#                 in neighbors\n",
    "#             ]\n",
    "\n",
    "#             events_df = pd.DataFrame(\n",
    "#                 data=events_data, \n",
    "#                 columns=self.event_features,\n",
    "#             )\n",
    "            \n",
    "#             # Filter logic implementation\n",
    "#             filtered_events = []\n",
    "#             filtered_event_ids = []\n",
    "            \n",
    "#             for index, row in events_df.iterrows():\n",
    "#                 event_id = row[\"event_id\"]\n",
    "#                 event_type = row[\"event_type\"] if \"event_type\" in events_df.columns else None\n",
    "                \n",
    "#                 # Skip if event category doesn't match user interests and user has interests\n",
    "#                 if event_type and user_interests and event_type not in user_interests:\n",
    "#                     continue\n",
    "                    \n",
    "#                 # If passed all filters, add to filtered list\n",
    "#                 filtered_events.append(row)\n",
    "#                 filtered_event_ids.append(event_id)\n",
    "            \n",
    "#             # Create DataFrame from filtered events\n",
    "#             filtered_events_df = pd.DataFrame(filtered_events) if filtered_events else pd.DataFrame(columns=self.event_features)\n",
    "#             event_id_df = pd.DataFrame({\"event_id\": filtered_event_ids})\n",
    "            \n",
    "#             # If no events passed the filters, return empty result\n",
    "#             if filtered_events_df.empty:\n",
    "#                 return {\n",
    "#                     \"inputs\": [{\"ranking_features\": [], \"event_ids\": []}]\n",
    "#                 }\n",
    "            \n",
    "#             # Join candidate items with their features - focus on candidate features\n",
    "#             candidate_df = filtered_events_df[self.candidate_features].copy() if all(feat in filtered_events_df.columns for feat in self.candidate_features) else pd.DataFrame()\n",
    "            \n",
    "#             # Create the ranking model inputs\n",
    "#             ranking_model_inputs = candidate_df.copy()\n",
    "            \n",
    "#             # Add user features - focus on query features\n",
    "#             for feature in self.query_features:\n",
    "#                 if feature in user_features.columns:\n",
    "#                     ranking_model_inputs[feature] = user_features[feature].values[0]\n",
    "            \n",
    "#             # Select only the features required by the ranking model\n",
    "#             available_features = [f for f in self.ranking_model_feature_names if f in ranking_model_inputs.columns]\n",
    "#             ranking_model_inputs = ranking_model_inputs[available_features]\n",
    "                    \n",
    "#             return { \n",
    "#                 \"inputs\": [{\"ranking_features\": ranking_model_inputs.values.tolist(), \"event_ids\": filtered_event_ids}]\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             # Add detailed logging to help diagnose the issue\n",
    "#             logging.error(f\"Error in preprocess: {str(e)}\")\n",
    "#             logging.error(f\"Input structure: {inputs}\")\n",
    "#             raise e\n",
    "\n",
    "#     def postprocess(self, outputs):\n",
    "#         try:\n",
    "#             # Extract predictions from the outputs\n",
    "#             preds = outputs[\"predictions\"]\n",
    "            \n",
    "#             # Merge prediction scores and corresponding event IDs into a list of tuples\n",
    "#             ranking = list(zip(preds[\"scores\"], preds[\"event_ids\"]))\n",
    "            \n",
    "#             # Sort the ranking list by score in descending order\n",
    "#             ranking.sort(reverse=True)\n",
    "            \n",
    "#             # Return the sorted ranking list\n",
    "#             return { \n",
    "#                 \"ranking\": ranking,\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             # Add detailed logging to help diagnose the issue\n",
    "#             logging.error(f\"Error in postprocess: {str(e)}\")\n",
    "#             logging.error(f\"Output structure: {outputs}\")\n",
    "#             raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting no_weather_ranking_transformer.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18680954b76439b9a7881f55ff1ceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_transformer.py: 0.000%|         ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy weather transformer file into Hopsworks File System \n",
    "weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"/home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_transformer.py\",    # File name to be uploaded\n",
    "    \"Resources\",                 # Destination directory in Hopsworks File System \n",
    "    overwrite=True,              # Overwrite the file if it already exists\n",
    ") \n",
    "\n",
    "# Construct the path to the uploaded weather transformer script\n",
    "weather_transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                 # Root directory for projects in Hopsworks\n",
    "    project.name,                # Name of the current project\n",
    "    weather_uploaded_file_path,          # Path to the uploaded file within the project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc32fe74cfe4be2ac305f8bb919332a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/src/no_weather_ranking_transformer.py: 0.000%|          | ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy no_weather transformer file into Hopsworks File System \n",
    "no_weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"/home/nkama/masters_thesis_project/thesis/src/no_weather_ranking_transformer.py\",    # File name to be uploaded\n",
    "    \"Resources\",                 # Destination directory in Hopsworks File System \n",
    "    overwrite=True,              # Overwrite the file if it already exists\n",
    ") \n",
    "\n",
    "# Construct the path to the uploaded no_weather transformer script\n",
    "no_weather_transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                 # Root directory for projects in Hopsworks\n",
    "    project.name,                # Name of the current project\n",
    "    no_weather_uploaded_file_path,          # Path to the uploaded file within the project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # List the directory contents to debug\n",
    "        artifact_path = os.environ[\"ARTIFACT_FILES_PATH\"]\n",
    "        model_path = os.environ[\"MODEL_FILES_PATH\"]\n",
    "        logging.info(f\"Artifact path contents: {os.listdir(artifact_path)}\")\n",
    "        logging.info(f\"Model path contents: {os.listdir(model_path)}\")\n",
    "        \n",
    "        # Try loading from MODEL_FILES_PATH instead\n",
    "        self.model = joblib.load(os.path.join(model_path, \"weather_ranking_model.pkl\"))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Extract ranking features and event IDs from the inputs\n",
    "        features = inputs[0][\"ranking_features\"]\n",
    "        event_ids = inputs[0][\"event_ids\"]\n",
    "        \n",
    "        # Log the extracted features\n",
    "        logging.info(\"predict -> \" + str(features))\n",
    "\n",
    "        # Predict probabilities for the positive class\n",
    "        scores = self.model.predict_proba(features).tolist()\n",
    "        \n",
    "        # Get scores of positive class\n",
    "        scores = np.asarray(scores)[:,1].tolist() \n",
    "\n",
    "        # Return the predicted scores along with the corresponding event IDs\n",
    "        return {\n",
    "            \"scores\": scores, \n",
    "            \"event_ids\": event_ids,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting no_weather_ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile no_weather_ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # List the directory contents to debug\n",
    "        artifact_path = os.environ[\"ARTIFACT_FILES_PATH\"]\n",
    "        model_path = os.environ[\"MODEL_FILES_PATH\"]\n",
    "        logging.info(f\"Artifact path contents: {os.listdir(artifact_path)}\")\n",
    "        logging.info(f\"Model path contents: {os.listdir(model_path)}\")\n",
    "        \n",
    "        # Try loading from MODEL_FILES_PATH instead\n",
    "        self.model = joblib.load(os.path.join(model_path, \"no_weather_ranking_model.pkl\"))\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Extract ranking features and event IDs from the inputs\n",
    "        features = inputs[0][\"ranking_features\"]\n",
    "        event_ids = inputs[0][\"event_ids\"]\n",
    "        \n",
    "        # Log the extracted features\n",
    "        logging.info(\"predict -> \" + str(features))\n",
    "\n",
    "        # Predict probabilities for the positive class\n",
    "        scores = self.model.predict_proba(features).tolist()\n",
    "        \n",
    "        # Get scores of positive class\n",
    "        scores = np.asarray(scores)[:,1].tolist() \n",
    "\n",
    "        # Return the predicted scores along with the corresponding event IDs\n",
    "        return {\n",
    "            \"scores\": scores, \n",
    "            \"event_ids\": event_ids,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47f4ae3fd942edb428744cdc6bfae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_predictor.py: 0.000%|          |‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload weather predictor file to Hopsworks\n",
    "weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"/home/nkama/masters_thesis_project/thesis/notebooks/weather_ranking_predictor.py\", \n",
    "    \"Resources\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "weather_predictor_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    weather_uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1930e04ed7a4d2383d667132fb7094f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/no_weather_ranking_predictor.py: 0.000%|        ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload no-weather predictor file to Hopsworks\n",
    "no_weather_uploaded_file_path = dataset_api.upload(\n",
    "    \"no_weather_ranking_predictor.py\", \n",
    "    \"Resources\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "no_weather_predictor_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    no_weather_uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1220788/deployments/372738\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from hsml.transformer import Transformer\n",
    "\n",
    "weather_ranking_deployment_name = \"weatherrankingdeployment\"\n",
    "\n",
    "# Define transformer\n",
    "weather_ranking_transformer=Transformer(\n",
    "    script_file=weather_transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy ranking model\n",
    "weather_ranking_deployment = weather_ranking_model.deploy(\n",
    "    name=weather_ranking_deployment_name,\n",
    "    description=\"Deployment that search for event candidates and scores them based on user metadata\",\n",
    "    script_file=weather_predictor_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=weather_ranking_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1220788/deployments/371728\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "no_weather_ranking_deployment_name = \"noweatherrankingdeployment\"\n",
    "\n",
    "# Define transformer\n",
    "no_weather_ranking_transformer=Transformer(\n",
    "    script_file=no_weather_transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy ranking model\n",
    "no_weather_ranking_deployment = no_weather_ranking_model.deploy(\n",
    "    name=no_weather_ranking_deployment_name,\n",
    "    description=\"Deployment that search for event candidates and scores them based on user metadata\",\n",
    "    script_file=no_weather_predictor_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=no_weather_ranking_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73c80a8293b4e97b717e93cbae9dd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModelServingException",
     "evalue": "Failed to run: transformer terminated unsuccessfully. \n\nINFO:root:Loading component module...\nTraceback (most recent call last):\n  File \"/serving/workspace/../kserve_server.py\", line 146, in <module>\n    model = _load_component_module(args)\n  File \"/serving/workspace/../kserve_server.py\", line 99, in _load_component_module\n    spec.loader.exec_module(mod)\n  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/mnt/artifacts/transformer-weather_ranking_transformer.py\", line 192\n    ranking_model_inputs = ranking_model_inputs.fillna(0)\n                                                         ^\nSyntaxError: expected 'except' or 'finally' block\n. Please, check the server logs using `.get_logs(component='transformer')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelServingException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start the deployment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mweather_ranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/usage.py:246\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    245\u001b[0m     exception \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/usage.py:242\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:95\u001b[0m, in \u001b[0;36mDeployment.start\u001b[0;34m(self, await_running)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@usage\u001b[39m\u001b[38;5;241m.\u001b[39mmethod_logger\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m, await_running: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Start the deployment\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    # Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m        `hopsworks.client.exceptions.RestAPIError`: In case the backend encounters an issue\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mawait_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_running\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:135\u001b[0m, in \u001b[0;36mServingEngine.start\u001b[0;34m(self, deployment_instance, await_status)\u001b[0m\n\u001b[1;32m    124\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_deployment_status(  \u001b[38;5;66;03m# wait for preparation\u001b[39;00m\n\u001b[1;32m    125\u001b[0m             deployment_instance,\n\u001b[1;32m    126\u001b[0m             PREDICTOR_STATE\u001b[38;5;241m.\u001b[39mSTATUS_CREATED,\n\u001b[1;32m    127\u001b[0m             await_status,\n\u001b[1;32m    128\u001b[0m             update_progress,\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serving_api\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    132\u001b[0m         deployment_instance, DEPLOYMENT\u001b[38;5;241m.\u001b[39mACTION_START\n\u001b[1;32m    133\u001b[0m     )  \u001b[38;5;66;03m# start deployment\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_deployment_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# wait for status\u001b[39;49;00m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPREDICTOR_STATE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATUS_RUNNING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mawait_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop(deployment_instance, await_status\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:91\u001b[0m, in \u001b[0;36mServingEngine._poll_deployment_status\u001b[0;34m(self, deployment_instance, status, await_status, update_progress)\u001b[0m\n\u001b[1;32m     81\u001b[0m             component \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mcondition\u001b[38;5;241m.\u001b[39mreason\n\u001b[1;32m     84\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             )\n\u001b[1;32m     86\u001b[0m             error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     87\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please, check the server logs using `.get_logs(component=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;241m+\u001b[39m component\n\u001b[1;32m     89\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             )\n\u001b[0;32m---> 91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(error_msg)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment has not reached the desired status within the expected awaiting time. Check the current status by using `.get_state()`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplore the server logs using `.get_logs()` or set a higher value for await_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;241m+\u001b[39m status\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     96\u001b[0m )\n",
      "\u001b[0;31mModelServingException\u001b[0m: Failed to run: transformer terminated unsuccessfully. \n\nINFO:root:Loading component module...\nTraceback (most recent call last):\n  File \"/serving/workspace/../kserve_server.py\", line 146, in <module>\n    model = _load_component_module(args)\n  File \"/serving/workspace/../kserve_server.py\", line 99, in _load_component_module\n    spec.loader.exec_module(mod)\n  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/mnt/artifacts/transformer-weather_ranking_transformer.py\", line 192\n    ranking_model_inputs = ranking_model_inputs.fillna(0)\n                                                         ^\nSyntaxError: expected 'except' or 'finally' block\n. Please, check the server logs using `.get_logs(component='transformer')`"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "weather_ranking_deployment.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment is already running\n"
     ]
    }
   ],
   "source": [
    "no_weather_ranking_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370339bde9974df18ced06052e2d5df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e1b7b52d42462eac528c4e9fc92448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop the ranking model deployment\n",
    "weather_ranking_deployment.stop()\n",
    "\n",
    "# Stop the query model deployment\n",
    "no_weather_ranking_deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 'query_model' from the Model Registry\n",
    "query_model = mr.get_model(\n",
    "    name=\"query_model\",\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting querymodel_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile querymodel_transformer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self): \n",
    "        # Connect to the Hopsworks\n",
    "        project = hopsworks.connection().get_project()\n",
    "        ms = project.get_model_serving()\n",
    "        \n",
    "        # Retrieve the 'users' feature view\n",
    "        fs = project.get_feature_store()\n",
    "        self.users_fv = fs.get_feature_view(\n",
    "            name=\"users\", \n",
    "            version=1,\n",
    "        )\n",
    "        # Retrieve the ranking deployment \n",
    "        self.ranking_server = ms.get_deployment(\"weatherrankingdeployment\")\n",
    "        \n",
    "    \n",
    "    def preprocess(self, inputs):\n",
    "        # Check if the input data contains a key named \"instances\"\n",
    "        # and extract the actual data if present\n",
    "        inputs = inputs[\"instances\"] if \"instances\" in inputs else inputs\n",
    "\n",
    "        # Extract user_id from the inputs\n",
    "        user_id = inputs[\"user_id\"]\n",
    "\n",
    "        # Get user features\n",
    "        user_features = self.users_fv.get_feature_vector(\n",
    "            {\"user_id\": user_id}, \n",
    "            return_type=\"pandas\",\n",
    "        )\n",
    "\n",
    "        # Enrich inputs with user features\n",
    "        inputs[\"user_city\"] = user_features['user_city'].values[0]\n",
    "        inputs[\"age\"] = user_features['age'].values[0] \n",
    "        inputs[\"user_interests\"] = user_features['user_interests'].values[0]\n",
    "        \n",
    "        return {\n",
    "            \"instances\": [inputs]\n",
    "        }\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        # Return ordered ranking predictions\n",
    "        return {\n",
    "            \"predictions\": self.ranking_server.predict({\"instances\": outputs[\"predictions\"]}),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fb2d67ddcc49809c390d4acc019c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /home/nkama/masters_thesis_project/thesis/notebooks/querymodel_transformer.py: 0.000%|          | 0/‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"querymodel_transformer.py\", \n",
    "    \"Models\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/1220788/deployments/371731\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "query_model_deployment_name = \"querydeployment\"\n",
    "\n",
    "# Define transformer\n",
    "query_model_transformer=Transformer(\n",
    "    script_file=transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy the query model\n",
    "query_model_deployment = query_model.deploy(\n",
    "    name=query_model_deployment_name,\n",
    "    description=\"Deployment that generates query embeddings from user and event features using the query model\",\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=query_model_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ac7455e381452fb1e726d32e9208f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "query_model_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates['ranking'][:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ranking'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m weather_ranking_deployment\u001b[38;5;241m.\u001b[39mpredict(test_ranking_input)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Retrieve event IDs of the top recommended items\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mget_top_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mranked_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m recommendations\n",
      "Cell \u001b[0;32mIn[190], line 2\u001b[0m, in \u001b[0;36mget_top_recommendations\u001b[0;34m(ranked_candidates, k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_top_recommendations\u001b[39m(ranked_candidates, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [candidate[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m \u001b[43mranked_candidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mranking\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:k]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ranking'"
     ]
    }
   ],
   "source": [
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[1] for candidate in ranked_candidates['ranking'][:k]]\n",
    "\n",
    "# Define a test input example for the ranking model\n",
    "test_ranking_input = {\"instances\": [[{\n",
    "    \"user_id\": \"LT819S\",  \n",
    "    \"query_emb\": [0.214135289, 0.571055949, 0.330709577, -0.225899458, -0.308674961, \n",
    "                 -0.0115124583, 0.0730511621, -0.495835781, 0.625569344, -0.0438038409, \n",
    "                 0.263472944, -0.58485353, -0.307070434, 0.0414443575, -0.321789205, \n",
    "                 0.966559, 0.127463, -0.392714, 0.845132, -0.512387, 0.253901, \n",
    "                 -0.764589, 0.431267, 0.087342, -0.629045, 0.318976, -0.146782, \n",
    "                 0.573921, -0.087625, 0.934261, -0.271843, 0.652197, -0.418359, \n",
    "                 0.123456, -0.789012, 0.345678, -0.901234, 0.567890, -0.234567, \n",
    "                 0.890123, -0.456789, 0.012345, -0.678901, 0.234567, -0.890123, \n",
    "                 0.456789, -0.012345, 0.678901, -0.234567, 0.890123, -0.456789, \n",
    "                 0.012345, -0.678901, 0.234567, -0.890123, 0.456789, -0.012345, \n",
    "                 0.678901, -0.234567, 0.890123, -0.456789, 0.012345, -0.678901]\n",
    "}]]}\n",
    "\n",
    "# Test ranking deployment\n",
    "ranked_candidates = weather_ranking_deployment.predict(test_ranking_input)\n",
    "\n",
    "# Retrieve event IDs of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates, k=3)\n",
    "recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': {'ranking': []}}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_candidates\n",
    "Output:\n",
    "{'predictions': {'ranking': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test input example for the query model\n",
    "data = {\n",
    "    \"instances\": [[{\n",
    "        \"user_id\": \"UP287J\"\n",
    "    }]]\n",
    "}\n",
    "\n",
    "# Test the deployment\n",
    "ranked_candidates = query_model_deployment.predict(data)\n",
    "\n",
    "# Retrieve event IDs of the top recommended items\n",
    "recommendations = get_top_recommendations(\n",
    "    ranked_candidates['predictions'], \n",
    "    k=3,\n",
    ")\n",
    "recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1220788/deployments/371733\n",
      "\n",
      "DeployableComponentLogs(instance_name: 'weatherrankingdeployment-predictor-00001-deployment-6bb688n8znf', date: datetime.datetime(2025, 5, 21, 23, 37, 16, 64328)) \n",
      "INFO:root:Loading component module...\n",
      "INFO:root:[PredictorModel] Initializing predictor for model: weatherrankingdeployment\n",
      "INFO:root:[HopsworksModel] Initializing for model: weatherrankingdeployment\n",
      "INFO:root:Artifact path contents: ['predictor-weather_ranking_predictor.py', 'transformer-simple_weather_transformer.py']\n",
      "INFO:root:Model path contents: ['weather_ranking_model.pkl']\n",
      "... execution time: 1.603723 seconds\n",
      "INFO:root:Starting KServe server...\n",
      "2025-05-21 21:36:33.556 8 kserve INFO [model_server.py:register_model():363] Registering model: weatherrankingdeployment\n",
      "2025-05-21 21:36:33.556 8 kserve INFO [model_server.py:start():298] Setting max asyncio worker threads as 12\n",
      "2025-05-21 21:36:33.557 8 kserve INFO [model_server.py:_serve_rest():244] Starting uvicorn with 1 workers\n",
      "2025-05-21 21:36:33.583 uvicorn.error INFO:     Started server process [8]\n",
      "2025-05-21 21:36:33.583 uvicorn.error INFO:     Waiting for application startup.\n",
      "2025-05-21 21:36:33.586 8 kserve INFO [server.py:start():68] Starting gRPC server on [::]:8081\n",
      "2025-05-21 21:36:33.586 uvicorn.error INFO:     Application startup complete.\n",
      "2025-05-21 21:36:33.586 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "INFO:root:Received request via 'v1 protocol'\n",
      "INFO:root:predict -> []\n",
      "2025-05-21 21:36:33.657 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 0.0007293224334716797 ['http_status:500', 'http_method:POST', 'time:wall']\n",
      "2025-05-21 21:36:33.657 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 0.0007189999999999142 ['http_status:500', 'http_method:POST', 'time:cpu']\n",
      "2025-05-21 21:36:33.657 8 kserve ERROR [errors.py:generic_exception_handler():123] Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/serving/kserve_models.py\", line 328, in _do_predict\n",
      "    outputs = self._component.predict(inputs)\n",
      "  File \"/mnt/artifacts/predictor-weather_ranking_predictor.py\", line 28, in predict\n",
      "    scores = self.model.predict_proba(features).tolist()\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 5314, in predict_proba\n",
      "    return self._predict(X, 'Probability', ntree_start, ntree_end, thread_count, verbose, 'predict_proba', task_type)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 2605, in _predict\n",
      "    data, data_is_single_object = self._process_predict_input_data(data, parent_method_name, thread_count)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 2585, in _process_predict_input_data\n",
      "    data = Pool(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 764, in __init__\n",
      "    self._check_data_empty(data)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 948, in _check_data_empty\n",
      "    raise CatBoostError(\"Input data must have at least one feature\")\n",
      "_catboost.CatBoostError: Input data must have at least one feature\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/serving/kserve_models.py\", line 69, in sync_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/serving/kserve_models.py\", line 330, in _do_predict\n",
      "    raise InferenceError(str(e), status=HTTPStatus.INTERNAL_SERVER_ERROR)\n",
      "kserve.errors.InferenceError: <exception str() failed>\n",
      "2025-05-21 21:36:33.659 uvicorn.access INFO:     10.2.3.34:0 8 - \"POST /v1/models/weatherrankingdeployment%3Apredict HTTP/1.1\" 500 Internal Server Error\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/timing_asgi/middleware.py\", line 70, in __call__\n",
      "    await self.app(scope, receive, send_wrapper)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/rest/v1_endpoints.py\", line 83, in predict\n",
      "    response, response_headers = await self.dataplane.infer(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/dataplane.py\", line 374, in infer\n",
      "    response = await model(request, headers=headers)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 241, in __call__\n",
      "    else self.predict(payload, headers)\n",
      "  File \"/serving/kserve_models.py\", line 71, in sync_wrapper\n",
      "    parse_exception(err)\n",
      "  File \"/serving/kserve_models.py\", line 53, in parse_exception\n",
      "    raise tornado.web.HTTPError(\n",
      "tornado.web.HTTPError: HTTP 500: Input data must have at least one feature\n",
      "2025-05-21 21:36:33.659 uvicorn.error ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/serving/kserve_models.py\", line 328, in _do_predict\n",
      "    outputs = self._component.predict(inputs)\n",
      "  File \"/mnt/artifacts/predictor-weather_ranking_predictor.py\", line 28, in predict\n",
      "    scores = self.model.predict_proba(features).tolist()\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 5314, in predict_proba\n",
      "    return self._predict(X, 'Probability', ntree_start, ntree_end, thread_count, verbose, 'predict_proba', task_type)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 2605, in _predict\n",
      "    data, data_is_single_object = self._process_predict_input_data(data, parent_method_name, thread_count)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 2585, in _process_predict_input_data\n",
      "    data = Pool(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 764, in __init__\n",
      "    self._check_data_empty(data)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/catboost/core.py\", line 948, in _check_data_empty\n",
      "    raise CatBoostError(\"Input data must have at least one feature\")\n",
      "_catboost.CatBoostError: Input data must have at least one feature\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/serving/kserve_models.py\", line 69, in sync_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/serving/kserve_models.py\", line 330, in _do_predict\n",
      "    raise InferenceError(str(e), status=HTTPStatus.INTERNAL_SERVER_ERROR)\n",
      "kserve.errors.InferenceError: <exception str() failed>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 436, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/timing_asgi/middleware.py\", line 70, in __call__\n",
      "    await self.app(scope, receive, send_wrapper)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/rest/v1_endpoints.py\", line 83, in predict\n",
      "    response, response_headers = await self.dataplane.infer(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/dataplane.py\", line 374, in infer\n",
      "    response = await model(request, headers=headers)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 241, in __call__\n",
      "    else self.predict(payload, headers)\n",
      "  File \"/serving/kserve_models.py\", line 71, in sync_wrapper\n",
      "    parse_exception(err)\n",
      "  File \"/serving/kserve_models.py\", line 53, in parse_exception\n",
      "    raise tornado.web.HTTPError(\n",
      "tornado.web.HTTPError: HTTP 500: Input data must have at least one feature\n",
      "2025-05-21 21:36:54.065 uvicorn.access INFO:     127.0.0.1:55456 8 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "2025-05-21 21:36:54.065 kserve.trace kserve.io.kserve.protocol.rest.server.metrics_handler: 0.0010182857513427734 ['http_status:200', 'http_method:GET', 'time:wall']\n",
      "2025-05-21 21:36:54.065 kserve.trace kserve.io.kserve.protocol.rest.server.metrics_handler: 0.0010079999999998979 ['http_status:200', 'http_method:GET', 'time:cpu']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_ranking_deployment.get_logs(component=\"predictor\", tail=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1220788/deployments/371733\n",
      "\n",
      "DeployableComponentLogs(instance_name: 'weatherrankingdeployment-transformer-00001-deployment-6896zq4tj', date: datetime.datetime(2025, 5, 21, 23, 37, 6, 848375)) \n",
      "INFO:root:Loading component module...\n",
      "INFO:root:[TransformerModel] Initializing transformer for model: weatherrankingdeployment\n",
      "INFO:root:[HopsworksModel] Initializing for model: weatherrankingdeployment\n",
      "INFO:hsfs.engine.python:Python Engine initialized.\n",
      "Downloading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1094/1094 elapsed<00:00 remaining<00:00\n",
      "INFO:transformer-simple_weather_transformer:Transformer initialized successfully\n",
      "... execution time: 7.065520 seconds\n",
      "INFO:root:Starting KServe server...\n",
      "2025-05-21 21:35:37.328 8 kserve INFO [model_server.py:register_model():363] Registering model: weatherrankingdeployment\n",
      "2025-05-21 21:35:37.328 8 kserve INFO [model_server.py:start():298] Setting max asyncio worker threads as 12\n",
      "2025-05-21 21:35:37.329 8 kserve INFO [model_server.py:_serve_rest():244] Starting uvicorn with 1 workers\n",
      "WARNING:py.warnings:DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions\n",
      "\n",
      "WARNING:py.warnings:DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated\n",
      "\n",
      "2025-05-21 21:35:37.355 uvicorn.error INFO:     Started server process [8]\n",
      "2025-05-21 21:35:37.355 uvicorn.error INFO:     Waiting for application startup.\n",
      "2025-05-21 21:35:37.361 8 kserve INFO [server.py:start():68] Starting gRPC server on [::]:8081\n",
      "2025-05-21 21:35:37.362 uvicorn.error INFO:     Application startup complete.\n",
      "2025-05-21 21:35:37.362 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "2025-05-21 21:35:57.743 uvicorn.access INFO:     127.0.0.1:36198 8 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "2025-05-21 21:35:57.743 kserve.trace kserve.io.kserve.protocol.rest.server.metrics_handler: 0.000789642333984375 ['http_status:200', 'http_method:GET', 'time:wall']\n",
      "2025-05-21 21:35:57.743 kserve.trace kserve.io.kserve.protocol.rest.server.metrics_handler: 0.0007779999999999454 ['http_status:200', 'http_method:GET', 'time:cpu']\n",
      "INFO:root:Received request via 'v1 protocol'\n",
      "INFO:transformer-simple_weather_transformer:Input structure: {'instances': [[{'user_id': 'LT819S', 'query_emb': [0.214135289, 0.571055949, 0.330709577, -0.225899458, -0.308674961, -0.0115124583, 0.0730511621, -0.495835781, 0.625569344, -0.0438038409, 0.263472944, -0.58485353, -0.307070434, 0.0414443575, -0.321789205, 0.966559, 0.127463, -0.392714, 0.845132, -0.512387, 0.253901, -0.764589, 0.431267, 0.087342, -0.629045, 0.318976, -0.146782, 0.573921, -0.087625, 0.934261, -0.271843, 0.652197, -0.418359, 0.123456, -0.789012, 0.345678, -0.901234, 0.56789, -0.234567, 0.890123, -0.456789, 0.012345, -0.678901, 0.234567, -0.890123, 0.456789, -0.012345, 0.678901, -0.234567, 0.890123, -0.456789, 0.012345, -0.678901, 0.234567, -0.890123, 0.456789, -0.012345, 0.678901, -0.234567, 0.890123, -0.456789, 0.012345, -0.678901]}]]}\n",
      "ERROR:transformer-simple_weather_transformer:Error in preprocess: list indices must be integers or slices, not str\n",
      "INFO:httpx:HTTP Request: POST http://weatherrankingdeployment-predictor.weather-basedeventrecsys/v1/models/weatherrankingdeployment:predict \"HTTP/1.1 500 Internal Server Error\"\n",
      "2025-05-21 21:36:33.667 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 14.437893629074097 ['http_status:500', 'http_method:POST', 'time:wall']\n",
      "2025-05-21 21:36:33.667 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 0.04127499999999973 ['http_status:500', 'http_method:POST', 'time:cpu']\n",
      "2025-05-21 21:36:33.667 8 kserve ERROR [errors.py:generic_exception_handler():123] Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/timing_asgi/middleware.py\", line 70, in __call__\n",
      "    await self.app(scope, receive, send_wrapper)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/rest/v1_endpoints.py\", line 83, in predict\n",
      "    response, response_headers = await self.dataplane.infer(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/dataplane.py\", line 374, in infer\n",
      "    response = await model(request, headers=headers)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 239, in __call__\n",
      "    (await self.predict(payload, headers))\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 404, in predict\n",
      "    return await self._http_predict(payload, headers)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 356, in _http_predict\n",
      "    response = await self._http_client.infer(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/inference_client.py\", line 501, in infer\n",
      "2025-05-21 21:36:33.669 uvicorn.access INFO:     10.2.3.34:0 8 - \"POST /v1/models/weatherrankingdeployment%3Apredict HTTP/1.1\" 500 Internal Server Error\n",
      "    raise self._consturct_http_status_error(response)\n",
      "httpx.HTTPStatusError: HTTPError : HTTP 500: Input data must have at least one feature, '500 Internal Server Error' for url 'http://weatherrankingdeployment-predictor.weather-basedeventrecsys/v1/models/weatherrankingdeployment:predict'\n",
      "2025-05-21 21:36:33.669 uvicorn.error ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 436, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/timing_asgi/middleware.py\", line 70, in __call__\n",
      "    await self.app(scope, receive, send_wrapper)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/rest/v1_endpoints.py\", line 83, in predict\n",
      "    response, response_headers = await self.dataplane.infer(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/protocol/dataplane.py\", line 374, in infer\n",
      "    response = await model(request, headers=headers)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 239, in __call__\n",
      "    (await self.predict(payload, headers))\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 404, in predict\n",
      "    return await self._http_predict(payload, headers)\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/model.py\", line 356, in _http_predict\n",
      "    response = await self._http_client.infer(\n",
      "  File \"/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/kserve/inference_client.py\", line 501, in infer\n",
      "    raise self._consturct_http_status_error(response)\n",
      "httpx.HTTPStatusError: HTTPError : HTTP 500: Input data must have at least one feature, '500 Internal Server Error' for url 'http://weatherrankingdeployment-predictor.weather-basedeventrecsys/v1/models/weatherrankingdeployment:predict'\n",
      "2025-05-21 21:36:57.736 kserve.trace kserve.io.kserve.protocol.rest.server.metrics_handler: 0.001377105712890625 ['http_status:200', 'http_method:GET', 'time:wall']\n",
      "2025-05-21 21:36:57.736 kserve.trace kserve.io.kserve.protocol.rest.server.metrics_handler: 0.0013760000000000439 ['http_status:200', 'http_method:GET', 'time:cpu']\n",
      "2025-05-21 21:36:57.736 uvicorn.access INFO:     127.0.0.1:36712 8 - \"GET /metrics HTTP/1.1\" 200 OK\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_ranking_deployment.get_logs(component=\"transformer\", tail=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: http://15.235.46.163/v1/models/weatherrankingdeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"HTTPError : HTTP 500: list indices must be integers or slices, not str\"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test ranking deployment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mweather_ranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ranking_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Retrieve article ids of the top recommended items\u001b[39;00m\n\u001b[1;32m      5\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(ranked_candidates, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:231\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     data: Union[Dict, InferInput] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     inputs: Union[List, Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m ):\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        `hopsworks.client.exceptions.RestAPIError`: In case the backend encounters an issue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:597\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    592\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m    594\u001b[0m re\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    595\u001b[0m     re\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the model server logs by using `.get_logs()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    596\u001b[0m )\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m re\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:581\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m through_hopsworks \u001b[38;5;241m=\u001b[39m serving_tool \u001b[38;5;241m!=\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mSERVING_TOOL_KSERVE\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inference_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrough_hopsworks\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    586\u001b[0m         re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m RestAPIError\u001b[38;5;241m.\u001b[39mSTATUS_CODE_NOT_FOUND\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m re\u001b[38;5;241m.\u001b[39merror_code\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;241m==\u001b[39m ModelServingException\u001b[38;5;241m.\u001b[39mERROR_CODE_DEPLOYMENT_NOT_RUNNING\n\u001b[1;32m    589\u001b[0m     ):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/core/serving_api.py:238\u001b[0m, in \u001b[0;36mServingApi.send_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to a deployment with a certain id\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m:param deployment_instance: metadata object of the deployment to be used for the prediction\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m:rtype: Union[Dict, List[InferOutput]]\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance\u001b[38;5;241m.\u001b[39mapi_protocol \u001b[38;5;241m==\u001b[39m IE\u001b[38;5;241m.\u001b[39mAPI_PROTOCOL_REST:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# REST protocol, use hopsworks or istio client\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_inference_request_via_rest_protocol\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrough_hopsworks\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# gRPC protocol, use the deployment grpc channel\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_inference_request_via_grpc_protocol(\n\u001b[1;32m    244\u001b[0m         deployment_instance, data\n\u001b[1;32m    245\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/core/serving_api.py:282\u001b[0m, in \u001b[0;36mServingApi._send_inference_request_via_rest_protocol\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    279\u001b[0m         with_base_path_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# send inference request\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_base_path_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_base_path_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/decorators.py:48\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/client/base.py:186\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[0m\n\u001b[1;32m    181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[1;32m    182\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: http://15.235.46.163/v1/models/weatherrankingdeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"HTTPError : HTTP 500: list indices must be integers or slices, not str\"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`"
     ]
    }
   ],
   "source": [
    "# Test ranking deployment\n",
    "ranked_candidates = weather_ranking_deployment.predict(test_ranking_input)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates, k=3)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelServingException",
     "evalue": "Instances field should contain a 2-dim list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelServingException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZO502T\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#TODO - make this a valid 'user_id' value from your feature group\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Test the deployment\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Retrieve article ids of the top recommended items\u001b[39;00m\n\u001b[1;32m     12\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(\n\u001b[1;32m     13\u001b[0m     ranked_candidates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     14\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:231\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     data: Union[Dict, InferInput] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     inputs: Union[List, Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    195\u001b[0m ):\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        `hopsworks.client.exceptions.RestAPIError`: In case the backend encounters an issue\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:570\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployment_instance\u001b[38;5;241m.\u001b[39mmodel_server \u001b[38;5;241m==\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mMODEL_SERVER_VLLM:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference requests to LLM deployments are not supported by the `predict` method. Please, use any OpenAI API-compatible client instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     )\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inference_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# build inference payload based on API protocol\u001b[39;00m\n\u001b[1;32m    573\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_payload(\n\u001b[1;32m    574\u001b[0m     deployment_instance\u001b[38;5;241m.\u001b[39mapi_protocol, data, inputs\n\u001b[1;32m    575\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:613\u001b[0m, in \u001b[0;36mServingEngine._validate_inference_payload\u001b[0;34m(self, api_protocol, data, inputs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# check data or inputs\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inference_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inference_inputs(api_protocol, inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:633\u001b[0m, in \u001b[0;36mServingEngine._validate_inference_data\u001b[0;34m(self, api_protocol, data)\u001b[0m\n\u001b[1;32m    631\u001b[0m payload \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(payload, List):\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstances field should contain a 2-dim list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(payload) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference data cannot contain an empty list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m     )\n",
      "\u001b[0;31mModelServingException\u001b[0m: Instances field should contain a 2-dim list."
     ]
    }
   ],
   "source": [
    "# Define a test input example\n",
    "data = {\n",
    "    \"instances\": {\n",
    "        \"user_id\": \"ZO502T\", #TODO - make this a valid 'user_id' value from your feature group\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test the deployment\n",
    "ranked_candidates = query_model_deployment.predict(data)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(\n",
    "    ranked_candidates['predictions'], \n",
    "    k=3,\n",
    ")\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1220788/deployments/371721\n",
      "\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1220788/serving/371721/logs). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":240027,\"errorMsg\":\"Server logs not available\"}', error code: 240027, error msg: Server logs not available, user msg: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweather_ranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/deployment.py:270\u001b[0m, in \u001b[0;36mDeployment.get_logs\u001b[0;34m(self, component, tail)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m components:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComponent \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not valid. Possible values are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    266\u001b[0m             component, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(components)\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m logs:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/engine/serving_engine.py:554\u001b[0m, in \u001b[0;36mServingEngine.get_logs\u001b[0;34m(self, deployment_instance, component, tail)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment is starting, server logs might not be ready yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplore all the logs and filters in the Kibana logs at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;241m+\u001b[39m deployment_instance\u001b[38;5;241m.\u001b[39mget_url(),\n\u001b[1;32m    551\u001b[0m     end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    552\u001b[0m )\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsml/core/serving_api.py:409\u001b[0m, in \u001b[0;36mServingApi.get_logs\u001b[0;34m(self, deployment_instance, component, tail)\u001b[0m\n\u001b[1;32m    400\u001b[0m path_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    402\u001b[0m     _client\u001b[38;5;241m.\u001b[39m_project_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m ]\n\u001b[1;32m    407\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m\"\u001b[39m: component, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m: tail}\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deployable_component_logs\u001b[38;5;241m.\u001b[39mDeployableComponentLogs\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[0;32m--> 409\u001b[0m     \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/decorators.py:48\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hopsworks_common/client/base.py:186\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[0m\n\u001b[1;32m    181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[1;32m    182\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1220788/serving/371721/logs). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":240027,\"errorMsg\":\"Server logs not available\"}', error code: 240027, error msg: Server logs not available, user msg: "
     ]
    }
   ],
   "source": [
    "weather_ranking_deployment.get_logs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
