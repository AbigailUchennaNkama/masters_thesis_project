{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346262d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89195d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 243\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# ====================== #\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# 8. Execution #\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# ====================== #\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 243\u001b[0m     users_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_users\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     events_df \u001b[38;5;241m=\u001b[39m generate_events()\n\u001b[1;32m    245\u001b[0m     interactions_df \u001b[38;5;241m=\u001b[39m generate_interactions(users_df, events_df)\n",
      "Cell \u001b[0;32mIn[3], line 103\u001b[0m, in \u001b[0;36mgenerate_users\u001b[0;34m(n_users)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Generate core interest + 1-3 additional interests\u001b[39;00m\n\u001b[1;32m    102\u001b[0m core_interest \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(interest_event_map\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m--> 103\u001b[0m interests \u001b[38;5;241m=\u001b[39m [core_interest] \u001b[38;5;241m+\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m interests \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(interests))[:\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# Max 4 unique interests\u001b[39;00m\n\u001b[1;32m    106\u001b[0m users\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: generic\u001b[38;5;241m.\u001b[39mperson\u001b[38;5;241m.\u001b[39midentifier(mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@@###@\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m: gender,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msocial_connectedness\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpoisson(lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m    119\u001b[0m })\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/random.py:482\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    484\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "from mimesis import Generic\n",
    "import re\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ====================== #\n",
    "# Global City Configurations #\n",
    "# ====================== #\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', \n",
    "          'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060),\n",
    "    'London': (51.5074, -0.1278),\n",
    "    'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503),\n",
    "    'Sydney': (-33.8688, 151.2093),\n",
    "    'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777),\n",
    "    'São Paulo': (-23.5505, -46.6333),\n",
    "    'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "# ====================== #\n",
    "# 1. Location Generation #\n",
    "# ====================== #\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:  # 80% nearby, 20% further out\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "# ====================== #\n",
    "# 2. User Generation #\n",
    "# ====================== #\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', \n",
    "                'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        # City selection uses global cities and city_probs\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Rest of user generation logic...\n",
    "\n",
    "interest_event_map = {\n",
    "    'music': ['Music & Concerts', 'Festivals'],\n",
    "    'sports': ['Sports Competitions', 'Fitness Events'],\n",
    "    'tech': ['Tech Conferences', 'Startup Events'],\n",
    "    'food': ['Food Festivals', 'Culinary Workshops'],\n",
    "    'art': ['Art Exhibitions', 'Museum Events'],\n",
    "    'literature': ['Book Fairs', 'Writing Workshops'],\n",
    "    'cinema': ['Film Festivals', 'Movie Premieres'],\n",
    "    'travel': ['Travel Expos', 'Adventure Tours'],\n",
    "    'fitness': ['Marathons', 'Yoga Retreats'],\n",
    "    'fashion': ['Fashion Shows', 'Designer Events']\n",
    "}\n",
    "\n",
    "event_type_weights = {\n",
    "    'Music & Concerts': 0.18, 'Sports Competitions': 0.15, \n",
    "    'Tech Conferences': 0.12, 'Food Festivals': 0.15,\n",
    "    'Art Exhibitions': 0.10, 'Book Fairs': 0.08,\n",
    "    'Film Festivals': 0.10, 'Travel Expos': 0.05,\n",
    "    'Marathons': 0.04, 'Fashion Shows': 0.03\n",
    "}\n",
    "\n",
    "# ====================== #\n",
    "# 2. User Generation #\n",
    "# ====================== #\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    all_interests = ['music', 'sports', 'tech', 'food', 'art', \n",
    "                    'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Gender-aware preferences\n",
    "        gender = generic.person.gender()\n",
    "        age_group = 'young' if age < 30 else 'middle' if age < 50 else 'senior'\n",
    "        \n",
    "        # Generate core interest + 1-3 additional interests\n",
    "        core_interest = random.choice(list(interest_event_map.keys()))\n",
    "        interests = [core_interest] + random.sample(interests, k=random.randint(1, 3))\n",
    "        interests = list(set(interests))[:4]  # Max 4 unique interests\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'gender': gender,\n",
    "            'age_group': age_group,\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'user_weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], \n",
    "                                                      p=[0.2, 0.3, 0.5]),\n",
    "            'age': age,\n",
    "            'core_interest': core_interest,\n",
    "            'secondary_interests': ','.join(interests[1:]),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "# ====================== #\n",
    "# 3. Event Generation #\n",
    "# ====================== #\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = list(event_type_weights.keys())\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        # Weighted event type selection\n",
    "        event_type = np.random.choice(event_types, p=list(event_type_weights.values()))\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Temporal patterns\n",
    "        start_time = generate_time_based_on_type(event_type)\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type}\",\n",
    "            'event_type': event_type,\n",
    "            'primary_category': event_type.split()[0],\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240], p=[0.4, 0.4, 0.2]),\n",
    "            'expected_attendance': int(np.random.lognormal(mean=6, sigma=0.5))\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "# ====================== #\n",
    "# 4. Interaction Generation #\n",
    "# ====================== #\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    hard_negatives = []\n",
    "    \n",
    "    # Create popularity baseline\n",
    "    event_popularity = events['expected_attendance'].rank(pct=True)\n",
    "    \n",
    "    # Generate positive interactions\n",
    "    for _ in range(n_interactions):\n",
    "        user, event = sample_pair(users, events)\n",
    "        \n",
    "        # Calculate match score using theoretical framework\n",
    "        match_score = calculate_match_score(user, event, event_popularity)\n",
    "        \n",
    "        if np.random.rand() < match_score:\n",
    "            interactions.append(create_interaction(user, event, 'positive'))\n",
    "            \n",
    "    # Generate hard negatives (HNS theory from paper [3])\n",
    "    for _ in range(n_interactions//10):\n",
    "        user, event = sample_pair(users, events)\n",
    "        \n",
    "        if is_hard_negative(user, event):\n",
    "            hard_negatives.append(create_interaction(user, event, 'hard_negative'))\n",
    "    \n",
    "    return pd.concat([pd.DataFrame(interactions), pd.DataFrame(hard_negatives)])\n",
    "\n",
    "# ====================== #\n",
    "# 5. Core Matching Logic #\n",
    "# ====================== #\n",
    "def calculate_match_score(user, event, popularity):\n",
    "    # Interest alignment (paper [5] correlation preservation)\n",
    "    interest_score = len(set(get_user_interests(user)) & \n",
    "                        set(interest_event_map.get(event['primary_category'], [])))\n",
    "    \n",
    "    # Geographic proximity\n",
    "    distance = geodesic((user['user_lat'], user['user_lon']),\n",
    "                       (event['event_lat'], event['event_lon'])).km\n",
    "    geo_score = np.exp(-distance/15)\n",
    "    \n",
    "    # Temporal relevance\n",
    "    time_score = 1.2 if event['start_time'].weekday() >= 5 else 0.8\n",
    "    \n",
    "    # Popularity bias (paper [3] HNS theory)\n",
    "    popularity_score = popularity[event.name]\n",
    "    \n",
    "    # Demographic alignment\n",
    "    demo_score = 1.0\n",
    "    if user['age_group'] == 'young' and event['event_type'] in ['Music & Concerts', 'Festivals']:\n",
    "        demo_score *= 1.3\n",
    "    elif user['age_group'] == 'senior' and event['event_type'] in ['Book Fairs']:\n",
    "        demo_score *= 1.4\n",
    "        \n",
    "    # Combine scores using learned weights\n",
    "    return sigmoid(0.4*interest_score + 0.3*geo_score + 0.2*time_score + 0.1*popularity_score)\n",
    "\n",
    "# ====================== #\n",
    "# 6. Hard Negative Sampling #\n",
    "# ====================== #\n",
    "def is_hard_negative(user, event):\n",
    "    # Partial match criteria (paper [3] theory)\n",
    "    user_interests = get_user_interests(user)\n",
    "    event_categories = interest_event_map.get(event['primary_category'], [])\n",
    "    \n",
    "    return (len(set(user_interests) & set(event_categories)) > 0) and \\\n",
    "           (event['event_id'] not in user['interacted_events'])\n",
    "\n",
    "# ====================== #\n",
    "# 7. Helper Functions #\n",
    "# ====================== #\n",
    "def get_user_interests(user):\n",
    "    return [user['core_interest']] + user['secondary_interests'].split(',')\n",
    "\n",
    "def generate_time_based_on_type(event_type):\n",
    "    base_date = datetime(2025, 1, 1)\n",
    "    if 'Music' in event_type:\n",
    "        return fake.date_time_between(start_date=base_date, end_date=base_date+timedelta(days=180))\n",
    "    elif 'Conference' in event_type:\n",
    "        return fake.date_time_between(start_date=base_date+timedelta(days=180), end_date=base_date+timedelta(days=360))\n",
    "    else:\n",
    "        return fake.date_time_between(start_date=base_date, end_date=base_date+timedelta(days=365))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ====================== #\n",
    "# 8. Execution #\n",
    "# ====================== #\n",
    "if __name__ == \"__main__\":\n",
    "    users_df = generate_users()\n",
    "    events_df = generate_events()\n",
    "    interactions_df = generate_interactions(users_df, events_df)\n",
    "    \n",
    "    # # Save datasets with correlation preservation (paper [5])\n",
    "    # users_df.to_csv('synthetic_users.csv', index=False)\n",
    "    # events_df.to_csv('synthetic_events.csv', index=False)\n",
    "    # interactions_df.to_csv('synthetic_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57a01ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm\n",
    "from mimesis import Generic\n",
    "import re\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ====================== #\n",
    "# Global City Configurations #\n",
    "# ====================== #\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', \n",
    "          'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060),\n",
    "    'London': (51.5074, -0.1278),\n",
    "    'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503),\n",
    "    'Sydney': (-33.8688, 151.2093),\n",
    "    'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777),\n",
    "    'São Paulo': (-23.5505, -46.6333),\n",
    "    'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "# ====================== #\n",
    "# 1. Location Generation #\n",
    "# ====================== #\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:  # 80% nearby, 20% further out\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "# ====================== #\n",
    "# 2. User Generation #\n",
    "# ====================== #\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    all_interests = ['music', 'sports', 'tech', 'food', 'art', \n",
    "                    'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Gender-aware preferences\n",
    "        gender = generic.person.gender()\n",
    "        age_group = 'young' if age < 30 else 'middle' if age < 50 else 'senior'\n",
    "        \n",
    "        # Generate core interest + 1-3 additional interests\n",
    "        core_interest = random.choice(all_interests)\n",
    "        k = random.randint(1, 3)\n",
    "        additional_interests = random.sample(\n",
    "            [i for i in all_interests if i != core_interest], \n",
    "            k=min(k, len(all_interests)-1)\n",
    "        )\n",
    "        user_interests = list(set([core_interest] + additional_interests))\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'gender': gender,\n",
    "            'age_group': age_group,\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'user_weather_preference': np.random.choice(\n",
    "                ['indoor', 'outdoor', 'any'], \n",
    "                p=[0.2, 0.3, 0.5]\n",
    "            ),\n",
    "            'age': age,\n",
    "            'core_interest': core_interest,\n",
    "            'secondary_interests': ','.join(user_interests[1:]),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "# ====================== #\n",
    "# 3. Event Generation #\n",
    "# ====================== #\n",
    "interest_event_map = {\n",
    "    'music': ['Music & Concerts', 'Festivals'],\n",
    "    'sports': ['Sports Competitions', 'Fitness Events'],\n",
    "    'tech': ['Tech Conferences', 'Startup Events'],\n",
    "    'food': ['Food Festivals', 'Culinary Workshops'],\n",
    "    'art': ['Art Exhibitions', 'Museum Events'],\n",
    "    'literature': ['Book Fairs', 'Writing Workshops'],\n",
    "    'cinema': ['Film Festivals', 'Movie Premieres'],\n",
    "    'travel': ['Travel Expos', 'Adventure Tours'],\n",
    "    'fitness': ['Marathons', 'Yoga Retreats'],\n",
    "    'fashion': ['Fashion Shows', 'Designer Events']\n",
    "}\n",
    "\n",
    "event_type_weights = list(interest_event_map.values())\n",
    "event_type_weights = [item for sublist in event_type_weights for item in sublist]\n",
    "event_type_weights = {\n",
    "    et: 0.5/len(event_type_weights) for et in event_type_weights\n",
    "}  # Equal weights for simplicity\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = list(event_type_weights.keys())\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        event_type = np.random.choice(event_types)\n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': random.choice(event_types),\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': fake.date_time_between(\n",
    "                start_date=datetime(2025, 1, 1), \n",
    "                end_date=datetime(2025, 12, 31)\n",
    "            ),\n",
    "            'duration': random.choice([60, 90, 120]),\n",
    "            'expected_attendance': int(np.random.lognormal(mean=6, sigma=0.5))\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "# ====================== #\n",
    "# 4. Interaction Generation #\n",
    "# ====================== #\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    \n",
    "    # Create popularity baseline\n",
    "    event_popularity = events['expected_attendance'].rank(pct=True)\n",
    "    \n",
    "    for _ in range(n_interactions):\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        # Calculate geographic distance\n",
    "        distance = geodesic(\n",
    "            (user['user_lat'], user['user_lon']),\n",
    "            (event['event_lat'], event['event_lon'])\n",
    "        ).km\n",
    "        \n",
    "        # Calculate interest alignment\n",
    "        user_interests = [user['core_interest']] + user['secondary_interests'].split(',')\n",
    "        event_categories = interest_event_map.get(\n",
    "            re.sub(r'\\s+', ' ', event['event_type'].split()[0].lower()), []\n",
    "        )\n",
    "        interest_match = len(set(user_interests) & set(event_categories))\n",
    "        \n",
    "        # Generate interaction probability\n",
    "        interaction_prob = (\n",
    "            0.4 * interest_match +\n",
    "            0.3 * np.exp(-distance/50) +  # 50km distance decay\n",
    "            0.2 * event_popularity[event.name] +\n",
    "            0.1 * random.random()\n",
    "        )\n",
    "        \n",
    "        if random.random() < interaction_prob:\n",
    "            interactions.append({\n",
    "                'interaction_id': generic.person.identifier(mask='@@###@'),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': random.choices(\n",
    "                    ['view', 'save', 'attend'],\n",
    "                    weights=[0.7, 0.2, 0.1]\n",
    "                )[0],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "# ====================== #\n",
    "# 5. Execution #\n",
    "# ====================== #\n",
    "if __name__ == \"__main__\":\n",
    "    users_df = generate_users(5000)\n",
    "    events_df = generate_events(2000)\n",
    "    interactions_df = generate_interactions(users_df, events_df, 10000)\n",
    "    \n",
    "    # # Save datasets\n",
    "    # users_df.to_csv('synthetic_users.csv', index=False)\n",
    "    # events_df.to_csv('synthetic_events.csv', index=False)\n",
    "    # interactions_df.to_csv('synthetic_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5d08150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lon</th>\n",
       "      <th>user_city</th>\n",
       "      <th>user_weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>core_interest</th>\n",
       "      <th>secondary_interests</th>\n",
       "      <th>social_connectedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OM335X</td>\n",
       "      <td>Male</td>\n",
       "      <td>middle</td>\n",
       "      <td>48.946743</td>\n",
       "      <td>2.398599</td>\n",
       "      <td>Paris</td>\n",
       "      <td>indoor</td>\n",
       "      <td>40</td>\n",
       "      <td>music</td>\n",
       "      <td>art,literature,music</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID403V</td>\n",
       "      <td>Female</td>\n",
       "      <td>middle</td>\n",
       "      <td>40.806782</td>\n",
       "      <td>-73.939511</td>\n",
       "      <td>New York</td>\n",
       "      <td>indoor</td>\n",
       "      <td>35</td>\n",
       "      <td>sports</td>\n",
       "      <td>fashion,tech,sports</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG972A</td>\n",
       "      <td>Other</td>\n",
       "      <td>middle</td>\n",
       "      <td>48.814846</td>\n",
       "      <td>2.374571</td>\n",
       "      <td>Paris</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>33</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BF417Q</td>\n",
       "      <td>Other</td>\n",
       "      <td>young</td>\n",
       "      <td>40.715647</td>\n",
       "      <td>-73.987517</td>\n",
       "      <td>New York</td>\n",
       "      <td>indoor</td>\n",
       "      <td>25</td>\n",
       "      <td>fashion</td>\n",
       "      <td>fashion</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GV450O</td>\n",
       "      <td>Female</td>\n",
       "      <td>middle</td>\n",
       "      <td>52.508030</td>\n",
       "      <td>13.329408</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>any</td>\n",
       "      <td>32</td>\n",
       "      <td>fitness</td>\n",
       "      <td>food,travel</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  gender age_group   user_lat   user_lon user_city  \\\n",
       "0  OM335X    Male    middle  48.946743   2.398599     Paris   \n",
       "1  ID403V  Female    middle  40.806782 -73.939511  New York   \n",
       "2  HG972A   Other    middle  48.814846   2.374571     Paris   \n",
       "3  BF417Q   Other     young  40.715647 -73.987517  New York   \n",
       "4  GV450O  Female    middle  52.508030  13.329408    Berlin   \n",
       "\n",
       "  user_weather_preference  age core_interest   secondary_interests  \\\n",
       "0                  indoor   40         music  art,literature,music   \n",
       "1                  indoor   35        sports   fashion,tech,sports   \n",
       "2                 outdoor   33         music                 music   \n",
       "3                  indoor   25       fashion               fashion   \n",
       "4                     any   32       fitness           food,travel   \n",
       "\n",
       "   social_connectedness  \n",
       "0                    16  \n",
       "1                    13  \n",
       "2                    14  \n",
       "3                    13  \n",
       "4                    12  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15177f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df[\"user_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217aaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd34903a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_lat</th>\n",
       "      <th>event_lon</th>\n",
       "      <th>event_city</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>temperature</th>\n",
       "      <th>attendance_rate</th>\n",
       "      <th>event_indoor_capability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RY823V</td>\n",
       "      <td>Innovative incremental circuit Seasonal &amp; Fest...</td>\n",
       "      <td>Seasonal &amp; Festivals</td>\n",
       "      <td>38.986291</td>\n",
       "      <td>-75.106163</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-04-08 13:10:17.349674</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>18.4</td>\n",
       "      <td>20.656472</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN297A</td>\n",
       "      <td>Re-engineered explicit knowledge user Seasonal...</td>\n",
       "      <td>Seasonal &amp; Festivals</td>\n",
       "      <td>-23.617500</td>\n",
       "      <td>-46.709916</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>2025-07-15 09:25:44.908898</td>\n",
       "      <td>240</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>24.3</td>\n",
       "      <td>14.775524</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VV502T</td>\n",
       "      <td>Profit-focused context-sensitive infrastructur...</td>\n",
       "      <td>Sports &amp; Fitness</td>\n",
       "      <td>37.317136</td>\n",
       "      <td>138.850860</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>2025-07-02 19:11:25.074699</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>22.8</td>\n",
       "      <td>40.198146</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QB605T</td>\n",
       "      <td>Secured empowering success Education &amp; Learnin...</td>\n",
       "      <td>Education &amp; Learning</td>\n",
       "      <td>51.414209</td>\n",
       "      <td>-0.110879</td>\n",
       "      <td>London</td>\n",
       "      <td>2025-05-18 18:20:35.709724</td>\n",
       "      <td>180</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.595591</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XN991I</td>\n",
       "      <td>Centralized bandwidth-monitored initiative Art...</td>\n",
       "      <td>Arts &amp; Culture</td>\n",
       "      <td>20.370323</td>\n",
       "      <td>74.818798</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2025-04-12 10:27:30.465703</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>32.5</td>\n",
       "      <td>30.103833</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_id                                              title  \\\n",
       "0   RY823V  Innovative incremental circuit Seasonal & Fest...   \n",
       "1   CN297A  Re-engineered explicit knowledge user Seasonal...   \n",
       "2   VV502T  Profit-focused context-sensitive infrastructur...   \n",
       "3   QB605T  Secured empowering success Education & Learnin...   \n",
       "4   XN991I  Centralized bandwidth-monitored initiative Art...   \n",
       "\n",
       "             event_type  event_lat   event_lon event_city  \\\n",
       "0  Seasonal & Festivals  38.986291  -75.106163   New York   \n",
       "1  Seasonal & Festivals -23.617500  -46.709916  São Paulo   \n",
       "2      Sports & Fitness  37.317136  138.850860      Tokyo   \n",
       "3  Education & Learning  51.414209   -0.110879     London   \n",
       "4        Arts & Culture  20.370323   74.818798     Mumbai   \n",
       "\n",
       "                  start_time  duration weather_condition  temperature  \\\n",
       "0 2025-04-08 13:10:17.349674       120             Clear         18.4   \n",
       "1 2025-07-15 09:25:44.908898       240            Cloudy         24.3   \n",
       "2 2025-07-02 19:11:25.074699       180             Clear         22.8   \n",
       "3 2025-05-18 18:20:35.709724       180            Cloudy         13.5   \n",
       "4 2025-04-12 10:27:30.465703       120             Clear         32.5   \n",
       "\n",
       "   attendance_rate  event_indoor_capability  \n",
       "0        20.656472                    False  \n",
       "1        14.775524                    False  \n",
       "2        40.198146                    False  \n",
       "3        21.595591                     True  \n",
       "4        30.103833                     True  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a40d2a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Re-engineered explicit knowledge user Seasonal & Festivals in São Paulo',\n",
       "       'Profit-focused context-sensitive infrastructure Sports & Fitness in Tokyo',\n",
       "       'Secured empowering success Education & Learning in London',\n",
       "       'Centralized bandwidth-monitored initiative Arts & Culture in Mumbai'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df[\"title\"][1:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ea8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>distance_to_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QP412E</td>\n",
       "      <td>HF620X</td>\n",
       "      <td>JV517H</td>\n",
       "      <td>view</td>\n",
       "      <td>658.259646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AW641F</td>\n",
       "      <td>TI615N</td>\n",
       "      <td>XE760J</td>\n",
       "      <td>save</td>\n",
       "      <td>10.173327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HQ068A</td>\n",
       "      <td>LC341M</td>\n",
       "      <td>NT411W</td>\n",
       "      <td>save</td>\n",
       "      <td>6293.823670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UN837C</td>\n",
       "      <td>AI471V</td>\n",
       "      <td>KW121S</td>\n",
       "      <td>view</td>\n",
       "      <td>339.571753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SJ872I</td>\n",
       "      <td>EF057Z</td>\n",
       "      <td>LO190F</td>\n",
       "      <td>view</td>\n",
       "      <td>5690.197951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interaction_id user_id event_id interaction_type  distance_to_event\n",
       "0         QP412E  HF620X   JV517H             view         658.259646\n",
       "1         AW641F  TI615N   XE760J             save          10.173327\n",
       "2         HQ068A  LC341M   NT411W             save        6293.823670\n",
       "3         UN837C  AI471V   KW121S             view         339.571753\n",
       "4         SJ872I  EF057Z   LO190F             view        5690.197951"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2a20d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id  event_id\n",
       "AA384G   VJ390I      1\n",
       "AA420W   UW503I      1\n",
       "AA485N   QE637J      1\n",
       "AA558Q   TT639R      1\n",
       "AA888P   OI625M      1\n",
       "                    ..\n",
       "ZZ109E   JB399Y      1\n",
       "ZZ603L   GC412F      1\n",
       "ZZ604X   EZ480V      1\n",
       "ZZ644X   CI397P      1\n",
       "ZZ902E   SC888L      1\n",
       "Name: count, Length: 1682, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[[\"user_id\",\"event_id\"]].groupby(\"user_id\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf0edcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "from mimesis import Generic\n",
    "import hopsworks\n",
    "import re\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "#generic = Person()\n",
    "\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        # Ensure at least one interest is selected\n",
    "        user_interests = random.sample(interests, k=random.randint(1, min(4, len(interests))))\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'user_weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'user_interests': ','.join(user_interests),  # Join interests into a comma-separated string\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', 'Arts & Culture', \n",
    "        'Entertainment', 'Sports & Fitness', 'Business & Networking', 'Health & Wellness', \n",
    "        'Music & Concerts', 'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    current_date = datetime(2025, 3, 27, 11, 48)  # Current date and time\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        if event_type in ['Sports & Fitness', 'Seasonal & Festivals']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['Education & Learning', 'Technology', 'Business & Networking']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        \n",
    "        base_temp = {\n",
    "            'New York': 15, 'London': 12, 'Paris': 16, 'Tokyo': 20, \n",
    "            'Sydney': 22, 'Berlin': 14, 'Mumbai': 28, 'São Paulo': 24, \n",
    "            'Toronto': 10, 'Dubai': 32\n",
    "        }[city]\n",
    "        \n",
    "        temp_adjustment = {\n",
    "            'Clear': np.random.uniform(2, 5),\n",
    "            'Rain': np.random.uniform(-3, 0),\n",
    "            'Snow': np.random.uniform(-8, -3),\n",
    "            'Cloudy': np.random.uniform(-1, 2),\n",
    "            'Windy': np.random.uniform(-2, 1)\n",
    "        }[weather_condition]\n",
    "        \n",
    "        temperature = round(base_temp + temp_adjustment, 1)\n",
    "        \n",
    "        start_time = fake.date_time_between(start_date=current_date, end_date=current_date + timedelta(days=180))\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'temperature': temperature,\n",
    "            'attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'event_indoor_capability': event_type in ['Education & Learning', 'Technology', 'Business & Networking', \n",
    "                                               'Arts & Culture', 'Entertainment', 'Immersive Experiences']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def calculate_time_weight(interaction_time, current_time, half_life=30):\n",
    "    time_diff = (current_time - interaction_time).days\n",
    "    return np.exp(np.log(0.5) * time_diff / half_life)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    interaction_types = ['maybe', 'invited & maybe', 'no', 'yes', 'invited & yes', 'invited & no', 'invited']\n",
    "    attempts = n_interactions * 5\n",
    "    current_time = datetime(2025, 3, 27, 11, 48)\n",
    "    \n",
    "    for _ in range(attempts):\n",
    "        if len(interactions) >= n_interactions:\n",
    "            break\n",
    "            \n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        distance = geodesic((user['user_lat'], user['user_lon']), \n",
    "                           (event['event_lat'], event['event_lon'])).km\n",
    "        \n",
    "        distance_score = np.exp(-distance/10)\n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and \n",
    "                               user['user_weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        interaction_prob = 0.85*distance_score + 0.1*weather_score + 0.05*social_score\n",
    "\n",
    "        max_distance = 50 if random.random() < 0.7 else 300\n",
    "        \n",
    "        if distance < max_distance and (random.random() < interaction_prob):\n",
    "            interaction_time = fake.date_time_between(\n",
    "                start_date=event['start_time'] - timedelta(days=30), \n",
    "                end_date=event['start_time']\n",
    "            )\n",
    "            time_weight = calculate_time_weight(interaction_time, current_time)\n",
    "            interaction_prob *= time_weight\n",
    "            \n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.15, 0.20, 0.05, 0.25, 0.20, 0.05, 0.10]\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.20, 0.15, 0.10, 0.20, 0.15, 0.10, 0.10]\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.25, 0.10, 0.15, 0.15, 0.10, 0.15, 0.10]\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.20, 0.05, 0.25, 0.10, 0.05, 0.20, 0.15]\n",
    "            else:\n",
    "                interaction_type_probs = [0.15, 0.05, 0.30, 0.05, 0.05, 0.25, 0.15]\n",
    "                \n",
    "            interactions.append({\n",
    "                'interaction_id': generic.person.identifier(mask='@@###@'),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(interaction_types, p=interaction_type_probs),\n",
    "                'distance_to_event': distance,\n",
    "                \"interaction_label\": interactions_df['interaction_type'].apply(\n",
    "                    lambda x: 1 if x in ['maybe', 'invited & maybe', 'yes', 'invited & yes'] else 0\n",
    "                )\n",
    "                            })\n",
    "    \n",
    "    return pd.DataFrame(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58ccca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating user data...\n",
      "Generating event data...\n",
      "Generating interaction data...\n"
     ]
    }
   ],
   "source": [
    "#Generate data\n",
    "print(\"Generating user data...\")\n",
    "users_df = generate_users(10000)\n",
    "\n",
    "print(\"Generating event data...\")\n",
    "events_df = generate_events(10000)\n",
    "\n",
    "print(\"Generating interaction data...\")\n",
    "interactions_df = generate_interactions(users_df, events_df, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84c079ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_per_user = interactions_df.groupby(\"user_id\").size()\n",
    "max(events_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d37223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205 attended 1 events\n",
      "206 attended 2 events\n",
      "19 attended 3 events\n",
      "2 attended 4 events\n",
      "0 attended 5 events\n",
      "0 attended 6 events\n",
      "0 attended 7 events\n",
      "0 attended 8 events\n",
      "0 attended 9 events\n",
      "0 attended 10 events\n",
      "0 attended 11 events\n",
      "0 attended 12 events\n",
      "0 attended 13 events\n"
     ]
    }
   ],
   "source": [
    "events_per_user = interactions_df.groupby(\"user_id\").size()\n",
    "max(events_per_user)\n",
    "\n",
    "for i in range(1, 14):\n",
    "    print(f\"{events_per_user[events_per_user == i].size} attended {i} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10ce8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df[\"interaction_label\"] = interactions_df['interaction_type'].apply(\n",
    "    lambda x: 1 if x in ['maybe', 'invited & maybe', 'yes', 'invited & yes'] else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96165753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interaction_label\n",
       "1    12497\n",
       "0     5081\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[\"interaction_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a922e68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating user data...\n",
      "Generating event data...\n",
      "Generating interactions data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "from mimesis import Generic\n",
    "import re\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City configurations\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', \n",
    "          'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    k: (float(v[0]), float(v[1])) for k,v in {\n",
    "        'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278),\n",
    "        'Paris': (48.8566, 2.3522), 'Tokyo': (35.6762, 139.6503),\n",
    "        'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "        'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333),\n",
    "        'Toronto': (43.6532, -79.3832), 'Dubai': (25.2048, 55.2708)\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', \n",
    "                'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        # Generate interests with power-law distribution\n",
    "        num_interests = min(np.random.zipf(1.2), 5)\n",
    "        user_interests = random.sample(interests, k=num_interests)\n",
    "        user_interests = list(set(user_interests))[:4]  # Max 4 unique interests\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'user_weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'user_interests': ','.join(user_interests),\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', \n",
    "        'Arts & Culture', 'Entertainment', 'Sports & Fitness', \n",
    "        'Business & Networking', 'Health & Wellness', 'Music & Concerts', \n",
    "        'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    current_date = datetime(2025, 3, 27, 11, 48)\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        event_type = random.choice(event_types)\n",
    "        start_time = fake.date_time_between(\n",
    "            start_date=current_date, \n",
    "            end_date=current_date + timedelta(days=180)\n",
    "        )\n",
    "        \n",
    "        # Weather and temperature logic\n",
    "        if event_type in ['Sports & Fitness', 'Seasonal & Festivals']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else random.choice(['Rain', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = random.choice(['Clear', 'Cloudy', 'Rain', 'Snow', 'Windy'])\n",
    "        \n",
    "        base_temp = {\n",
    "            'New York': 15, 'London': 12, 'Paris': 16, 'Tokyo': 20, \n",
    "            'Sydney': 22, 'Berlin': 14, 'Mumbai': 28, 'São Paulo': 24, \n",
    "            'Toronto': 10, 'Dubai': 32\n",
    "        }[city]\n",
    "        \n",
    "        temp_adjustment = {\n",
    "            'Clear': np.random.uniform(2, 5),\n",
    "            'Rain': np.random.uniform(-3, 0),\n",
    "            'Snow': np.random.uniform(-8, -3),\n",
    "            'Cloudy': np.random.uniform(-1, 2),\n",
    "            'Windy': np.random.uniform(-2, 1)\n",
    "        }[weather_condition]\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'temperature': round(base_temp + temp_adjustment, 1),\n",
    "            'attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'event_indoor_capability': event_type in ['Education & Learning', 'Technology', \n",
    "                                                    'Business & Networking', 'Arts & Culture',\n",
    "                                                    'Entertainment', 'Immersive Experiences']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    zipf_param = 1.8  # More skewed distribution\n",
    "    max_events_per_user = 25\n",
    "    current_time = datetime(2025, 3, 27, 11, 48)\n",
    "    \n",
    "    def safe_sample(df, n):\n",
    "        return df.sample(n, replace=True) if len(df) > 0 else pd.DataFrame()\n",
    "\n",
    "    # Phase 1: Core interactions with power-law distribution\n",
    "    for user in users.itertuples():\n",
    "        user_interests = set(user.user_interests.split(','))\n",
    "        base_count = np.random.zipf(zipf_param)\n",
    "        events_to_attend = min(base_count, max_events_per_user)\n",
    "        \n",
    "        # Get interest-matching events with case-insensitive check\n",
    "        candidate_events = events[\n",
    "            events['event_type'].apply(\n",
    "                lambda x: any(i.lower() in x.lower() for i in user_interests)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        if len(candidate_events) == 0:\n",
    "            continue  # Skip users with no matching events\n",
    "            \n",
    "        candidate_sample = safe_sample(candidate_events, 50)\n",
    "        if len(candidate_sample) == 0:\n",
    "            continue\n",
    "\n",
    "        for event in candidate_sample.itertuples():\n",
    "            if events_to_attend <= 0:\n",
    "                break\n",
    "            \n",
    "            distance = geodesic(\n",
    "                (user.user_lat, user.user_lon),\n",
    "                (event.event_lat, event.event_lon)\n",
    "            ).km\n",
    "            \n",
    "            if distance < 50 and random.random() < 0.7:\n",
    "                interactions.append({\n",
    "                    'user_id': user.user_id,\n",
    "                    'event_id': event.event_id,\n",
    "                    'distance_to_event': distance,\n",
    "                    'event_type_in_user_interests': 1,\n",
    "                    'interaction_label': 1\n",
    "                })\n",
    "                events_to_attend -= 1\n",
    "\n",
    "    # Phase 2: Power users with many interactions\n",
    "    power_users = users.nlargest(500, 'social_connectedness')\n",
    "    for user in power_users.itertuples():\n",
    "        additional_events = min(np.random.zipf(zipf_param*1.5), max_events_per_user)\n",
    "        candidate_events = events.sample(100)\n",
    "        \n",
    "        for event in candidate_events.itertuples():\n",
    "            if additional_events <= 0:\n",
    "                break\n",
    "            \n",
    "            distance = geodesic(\n",
    "                (user.user_lat, user.user_lon),\n",
    "                (event.event_lat, event.event_lon)\n",
    "            ).km\n",
    "            \n",
    "            if distance < 100 and random.random() < 0.8:\n",
    "                interactions.append({\n",
    "                    'user_id': user.user_id,\n",
    "                    'event_id': event.event_id,\n",
    "                    'distance_to_event': distance,\n",
    "                    'event_type_in_user_interests': int(\n",
    "                        any(i in event.event_type for i in user.user_interests.split(','))\n",
    "                    ),\n",
    "                    'interaction_label': 1\n",
    "                })\n",
    "                additional_events -= 1\n",
    "\n",
    "    # Phase 3: Fill remaining interactions\n",
    "    while len(interactions) < n_interactions:\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        distance = geodesic(\n",
    "            (user['user_lat'], user['user_lon']),\n",
    "            (event['event_lat'], event['event_lon'])\n",
    "        ).km\n",
    "        \n",
    "        interactions.append({\n",
    "            'user_id': user['user_id'],\n",
    "            'event_id': event['event_id'],\n",
    "            'distance_to_event': distance,\n",
    "            'event_type_in_user_interests': int(\n",
    "                any(i in event['event_type'] for i in user['user_interests'].split(','))\n",
    "            ),\n",
    "            'interaction_label': 1 if random.random() < 0.3 else 0\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(interactions[:n_interactions])\n",
    "\n",
    "# Generate datasets\n",
    "# users_df = generate_users(10000)\n",
    "# events_df = generate_events(5000)\n",
    "# interactions_df = generate_interactions(users_df, events_df, 150000)\n",
    "\n",
    "# # Save to CSV\n",
    "# users_df.to_csv('synthetic_users.csv', index=False)\n",
    "# events_df.to_csv('synthetic_events.csv', index=False)\n",
    "# interactions_df.to_csv('synthetic_interactions.csv', index=False)\n",
    "\n",
    "    \n",
    "    # # Save datasets\n",
    "    # users_df.to_csv('synthetic_users.csv', index=False)\n",
    "    # events_df.to_csv('synthetic_events.csv', index=False)\n",
    "    # interactions_df.to_csv('synthetic_interactions.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating user data...\")\n",
    "users_df = generate_users(10000)\n",
    "print(\"Generating event data...\")\n",
    "events_df = generate_events(5000)\n",
    "print(\"Generating interactions data...\")\n",
    "interactions_df = generate_interactions(users_df, events_df, 150000)\n",
    "\n",
    "# # Save to CSV\n",
    "# users_df.to_csv('users.csv', index=False)\n",
    "# events_df.to_csv('events.csv', index=False)\n",
    "# interactions_df.to_csv('interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1f37908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 attended 1 events\n",
      "1 attended 2 events\n",
      "1 attended 3 events\n",
      "9 attended 4 events\n",
      "17 attended 5 events\n",
      "60 attended 6 events\n",
      "115 attended 7 events\n",
      "210 attended 8 events\n",
      "346 attended 9 events\n",
      "534 attended 10 events\n",
      "649 attended 11 events\n",
      "822 attended 12 events\n",
      "875 attended 13 events\n"
     ]
    }
   ],
   "source": [
    "events_per_user = interactions_df.groupby(\"user_id\").size()\n",
    "max(events_per_user)\n",
    "\n",
    "for i in range(1, 14):\n",
    "    print(f\"{events_per_user[events_per_user == i].size} attended {i} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddb1fddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interaction_label\n",
       "0    95645\n",
       "1    54355\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[\"interaction_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7589d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
