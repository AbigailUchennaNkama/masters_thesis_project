{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import joblib\n",
    "import hopsworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 19:25:08,033 INFO: Initializing external client\n",
      "2025-04-23 19:25:08,036 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-04-23 19:25:08,558 WARNING: UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 19:25:09,300 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_fg = fs.get_feature_group(\n",
    "    name=\"users\",\n",
    "    version=1\n",
    ")\n",
    "\n",
    "events_fg = fs.get_feature_group(\n",
    "    name=\"events\",\n",
    "    version=1\n",
    ")\n",
    "\n",
    "weather_rank_fg = fs.get_feature_group(\n",
    "    name=\"weather_ranking\",\n",
    "    version=1\n",
    ")\n",
    "\n",
    "no_weather_rank_fg = fs.get_feature_group(\n",
    "    name=\"no_weather_ranking\",\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">⚙️ Feature View Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hsfs.feature_view.FeatureView at 0x7f36f8550df0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "selected_features_customers = users_fg.select_all()\n",
    "\n",
    "fs.get_or_create_feature_view( \n",
    "    name='users',\n",
    "    query=selected_features_customers,\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hsfs.feature_view.FeatureView at 0x7f36f8434a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "selected_features_articles = events_fg.select_all()\n",
    "\n",
    "fs.get_or_create_feature_view(\n",
    "    name='events',\n",
    "    query=selected_features_articles,\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_WEATHER_SELECTED_FEATURES =['interaction_distance_to_event', 'title',\n",
    "       'event_type','event_city', 'duration',\n",
    "       'attendance_rate', 'event_indoor_capability', 'user_city',\n",
    "       'age', 'user_interests','label']\n",
    "\n",
    "WEATHER_SELECTED_FEATURES =['interaction_distance_to_event', 'title',\n",
    "       'event_type','event_city', 'duration','weather_condition', 'temperature',\n",
    "       'attendance_rate', 'event_indoor_capability', 'user_city',\n",
    "       'user_weather_preference', 'age', 'user_interests','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1220788/fs/1208418/fv/weather_ranking_2/version/1\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "features_weather_ranking = weather_rank_fg.select(WEATHER_SELECTED_FEATURES)\n",
    "\n",
    "feature_view_ranking_weather = fs.get_or_create_feature_view(\n",
    "    name='weather_ranking_2',\n",
    "    query=features_weather_ranking,\n",
    "    labels=[\"label\"],\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1220788/fs/1208418/fv/no_weather_ranking_2/version/1\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "features_no_weather_ranking = no_weather_rank_fg.select(NO_WEATHER_SELECTED_FEATURES)\n",
    "\n",
    "feature_view_ranking_no_weather = fs.get_or_create_feature_view(\n",
    "    name='no_weather_ranking_2',\n",
    "    query=features_no_weather_ranking,\n",
    "    labels=[\"label\"],\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature views\n",
    "feature_view_ranking_weather = fs.get_feature_view(name='weather_ranking_2', version=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature views\n",
    "feature_view_ranking_weather = fs.get_feature_view(name='weather_ranking_2', version=1)\n",
    "\n",
    "feature_view_ranking_no_weather = fs.get_feature_view(name='no_weather_ranking_2', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (6.67s) \n",
      "2025-04-23 19:56:07,750 WARNING: VersionWarning: Incremented version to `1`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get feature views\n",
    "feature_view_ranking_weather = fs.get_feature_view(name='weather_ranking_2', version=1)\n",
    "\n",
    "feature_view_ranking_no_weather = fs.get_feature_view(name='no_weather_ranking_2', version=1)\n",
    "# Get training and validation data directly from feature views\n",
    "weather_X_train, weather_X_val, weather_y_train, weather_y_val = \\\n",
    "    feature_view_ranking_weather.train_test_split(\n",
    "    test_size=0.1,\n",
    "    description='Weather ranking training dataset',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_city</th>\n",
       "      <th>duration</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>temperature</th>\n",
       "      <th>attendance_rate</th>\n",
       "      <th>event_indoor_capability</th>\n",
       "      <th>user_city</th>\n",
       "      <th>user_weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>user_interests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16921.000000</td>\n",
       "      <td>synchronized solutionoriented migration commun...</td>\n",
       "      <td>Community &amp; Causes</td>\n",
       "      <td>Paris</td>\n",
       "      <td>240</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>16.9</td>\n",
       "      <td>79.345494</td>\n",
       "      <td>False</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>46</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.584158</td>\n",
       "      <td>cloned valueadded graphical user interface hea...</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>Paris</td>\n",
       "      <td>240</td>\n",
       "      <td>Clear</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12.692646</td>\n",
       "      <td>False</td>\n",
       "      <td>Paris</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>49</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>449.000000</td>\n",
       "      <td>reverseengineered tertiary neuralnet entertain...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>240</td>\n",
       "      <td>Rain</td>\n",
       "      <td>9.1</td>\n",
       "      <td>83.238042</td>\n",
       "      <td>True</td>\n",
       "      <td>New York</td>\n",
       "      <td>any</td>\n",
       "      <td>32</td>\n",
       "      <td>cinema tech music sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>objectbased local support technology in new york</td>\n",
       "      <td>Technology</td>\n",
       "      <td>New York</td>\n",
       "      <td>240</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81.857555</td>\n",
       "      <td>True</td>\n",
       "      <td>New York</td>\n",
       "      <td>indoor</td>\n",
       "      <td>34</td>\n",
       "      <td>tech literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6581.000000</td>\n",
       "      <td>publickey intermediate budgetary management sp...</td>\n",
       "      <td>Sports &amp; Fitness</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>180</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>13.3</td>\n",
       "      <td>76.052787</td>\n",
       "      <td>False</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>34</td>\n",
       "      <td>food fashion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   interaction_distance_to_event  \\\n",
       "1                   16921.000000   \n",
       "2                      10.584158   \n",
       "3                     449.000000   \n",
       "4                       7.000000   \n",
       "5                    6581.000000   \n",
       "\n",
       "                                               title          event_type  \\\n",
       "1  synchronized solutionoriented migration commun...  Community & Causes   \n",
       "2  cloned valueadded graphical user interface hea...   Health & Wellness   \n",
       "3  reverseengineered tertiary neuralnet entertain...       Entertainment   \n",
       "4   objectbased local support technology in new york          Technology   \n",
       "5  publickey intermediate budgetary management sp...    Sports & Fitness   \n",
       "\n",
       "  event_city  duration weather_condition  temperature  attendance_rate  \\\n",
       "1      Paris       240            Cloudy         16.9        79.345494   \n",
       "2      Paris       240             Clear         18.1        12.692646   \n",
       "3    Toronto       240              Rain          9.1        83.238042   \n",
       "4   New York       240            Cloudy         16.0        81.857555   \n",
       "5     Berlin       180            Cloudy         13.3        76.052787   \n",
       "\n",
       "   event_indoor_capability user_city user_weather_preference  age  \\\n",
       "1                    False    Sydney                 outdoor   46   \n",
       "2                    False     Paris                 outdoor   49   \n",
       "3                     True  New York                     any   32   \n",
       "4                     True  New York                  indoor   34   \n",
       "5                    False   Toronto                 outdoor   34   \n",
       "\n",
       "             user_interests  \n",
       "1                    travel  \n",
       "2                      food  \n",
       "3  cinema tech music sports  \n",
       "4           tech literature  \n",
       "5              food fashion  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (5.86s) \n",
      "2025-04-23 19:57:36,700 WARNING: VersionWarning: Incremented version to `1`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_weather_X_train, no_weather_X_val, no_weather_y_train, no_weather_y_val = \\\n",
    "    feature_view_ranking_no_weather.train_test_split(\n",
    "    test_size=0.1,\n",
    "    description='No-weather ranking training dataset',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['interaction_distance_to_event', 'title', 'event_type', 'event_city',\n",
       "       'duration', 'weather_condition', 'temperature', 'attendance_rate',\n",
       "       'event_indoor_capability', 'user_city', 'user_weather_preference',\n",
       "       'age', 'user_interests'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_X_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_WEATHER_SELECTED_FEATURES =['interaction_distance_to_event', 'title',\n",
    "       'event_type','event_city', 'duration',\n",
    "       'attendance_rate', 'event_indoor_capability', 'user_city',\n",
    "       'age', 'user_interests']\n",
    "\n",
    "WEATHER_SELECTED_FEATURES =['interaction_distance_to_event', 'title',\n",
    "       'event_type','event_city', 'duration','weather_condition', 'temperature',\n",
    "       'attendance_rate', 'event_indoor_capability', 'user_city',\n",
    "       'user_weather_preference', 'age', 'user_interests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/data/main_data/users.csv')\n",
    "events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/partially_synthetic/data/main_data/events.csv\")\n",
    "interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/data/main_data/interactions.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Merge user/event features into interactions\n",
    "interactions_df = interactions_df.merge(users_df, on=\"user_id\")\n",
    "interactions_df = interactions_df.merge(events_df, on=\"event_id\", suffixes=('_user', '_event'))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NO_WEATHER_SELECTED_FEATURES =['interaction_type',\n",
    "       'distance_to_event', 'interaction_label',\n",
    "        'gender', 'joinedAt', 'location', 'age',\n",
    "      'indoor_outdoor_preference', 'user_interests', \n",
    "       'start_time', 'city', 'yes_count',\n",
    "       'maybe_count', 'invited_count', 'no_count', 'total_users', 'category', \n",
    "       'title', 'event_type','event_indoor_capability']\n",
    "\n",
    "WEATHER_SELECTED_FEATURES =['interaction_type',\n",
    "       'distance_to_event', 'interaction_label',\n",
    "        'gender', 'joinedAt', 'location', 'age',\n",
    "      'indoor_outdoor_preference',\n",
    "       'start_time', 'city', 'yes_count',\n",
    "       'maybe_count', 'invited_count', 'no_count', 'total_users',\n",
    "       'weather_description', 'category','event_type',\n",
    "       'event_indoor_capability', 'temperature_2m_mean', 'precipitation_sum']\n",
    "\n",
    "# )\n",
    "# Splitting the dataset into features and labels\n",
    "weather_X = interactions_df[WEATHER_SELECTED_FEATURES]  # Features\n",
    "weather_y = interactions_df['interaction_label']   \n",
    "\n",
    "no_weather_X = interactions_df[NO_WEATHER_SELECTED_FEATURES]  # Features\n",
    "no_weather_y = interactions_df['interaction_label']                   # Labels\n",
    "\n",
    "# Splitting the dataset into training and evaluation sets\n",
    "weather_X_train, weather_X_val, weather_y_train, weather_y_val = \\\n",
    "    train_test_split(weather_X, weather_y, test_size=0.2, random_state=42)\n",
    "\n",
    "no_weather_X_train, no_weather_X_val, no_weather_y_train, no_weather_y_val = \\\n",
    "    train_test_split(no_weather_X, no_weather_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Updated CatBoost training function to exclude title and user_interests for memory safety.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "# Get feature views\n",
    "# feature_view_ranking_weather = fs.get_feature_view(name='weather_ranking_2', version=1)\n",
    "\n",
    "# feature_view_ranking_no_weather = fs.get_feature_view(name='no_weather_ranking_2', version=1)\n",
    "# # Get training and validation data directly from feature views\n",
    "# weather_X_train, weather_X_val, weather_y_train, weather_y_val = \\\n",
    "#     feature_view_ranking_weather.train_test_split(\n",
    "#     test_size=0.1,\n",
    "#     description='Weather ranking training dataset',\n",
    "# )\n",
    "\n",
    "# NO_WEATHER_SELECTED_FEATURES =['interaction_distance_to_event', 'title',\n",
    "#        'event_type','event_city', 'duration',\n",
    "#        'attendance_rate', 'event_indoor_capability', 'user_city',\n",
    "#        'age', 'user_interests']\n",
    "\n",
    "# WEATHER_SELECTED_FEATURES =['interaction_distance_to_event', 'title',\n",
    "#        'event_type','event_city', 'duration','weather_condition', 'temperature',\n",
    "#        'attendance_rate', 'event_indoor_capability', 'user_city',\n",
    "#        'user_weather_preference', 'age', 'user_interests']\n",
    "\n",
    "# Final version without text fields (title, user_interests)\n",
    "def train_catboost_without_text_fields(\n",
    "    train_df, val_df, train_y, val_y\n",
    "):\n",
    "    # Drop the text fields if present\n",
    "    text_columns = [\"title\", \"user_interests\"]\n",
    "    train_df = train_df.drop(columns=[col for col in text_columns if col in train_df.columns])\n",
    "    val_df = val_df.drop(columns=[col for col in text_columns if col in val_df.columns])\n",
    "\n",
    "    # Identify categorical features\n",
    "    cat_features = train_df.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "    # Create CatBoost Pools\n",
    "    train_pool = Pool(train_df, train_y, cat_features=cat_features)\n",
    "    val_pool = Pool(val_df, val_y, cat_features=cat_features)\n",
    "\n",
    "    # Train the model\n",
    "    model = CatBoostClassifier(\n",
    "        learning_rate=0.2,\n",
    "        iterations=100,\n",
    "        depth=10,\n",
    "        early_stopping_rounds=5,\n",
    "        use_best_model=True,\n",
    "        scale_pos_weight=None, \n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "    # Evaluation\n",
    "    preds = model.predict(val_pool)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(val_y, preds, average=\"binary\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_y, preds))\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"fscore\": fscore,\n",
    "    }\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    preds = model.scores = model.predict_proba(val_pool)[:, 1] \n",
    "    print(\"Predicted Class Distribution:\", np.unique(preds, return_counts=True))\n",
    "\n",
    "    # print(\"\\nConfusion Matrix:\")\n",
    "    # print(confusion_matrix(val_y, preds))\n",
    "\n",
    "    return model, metrics, val_pool\n",
    "\n",
    "\"✅ Updated CatBoost training function to exclude title and user_interests for memory safety.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interaction_distance_to_event', 'event_lat', 'event_lon', 'duration', 'temperature', 'attendance_rate', 'user_lat', 'user_lon', 'age']\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "numeric_columns = weather_X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "print(numeric_columns)\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'event_type', 'event_city', 'user_city', 'user_interests']\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "categorical_columns = no_weather_X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(categorical_columns)\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_catboost_with_text_fields(train_df, val_df, train_y, val_y):\n",
    "    # Step 1: Process text fields with TF-IDF\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    \n",
    "    # Fit and transform text features\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.fit_transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.fit_transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "\n",
    "    # Step 2: Process numeric features\n",
    "    numeric_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n",
    "    train_df[numeric_cols] = train_df[numeric_cols].fillna(0)\n",
    "    val_df[numeric_cols] = val_df[numeric_cols].fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "\n",
    "    # Step 3: Process categorical features\n",
    "    categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categorical_cols = [col for col in categorical_cols if col not in [\"title\", \"user_interests\"]]\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in categorical_cols:\n",
    "        train_df[col] = train_df[col].fillna('unknown')\n",
    "        val_df[col] = val_df[col].fillna('unknown')\n",
    "    \n",
    "    # Extract categorical features as separate DataFrames\n",
    "    X_train_cat = train_df[categorical_cols].reset_index(drop=True)\n",
    "    X_val_cat = val_df[categorical_cols].reset_index(drop=True)\n",
    "    \n",
    "    # Convert TF-IDF and numeric features to numpy arrays\n",
    "    X_train_text_numeric = hstack([X_train_text, X_train_numeric]).toarray()\n",
    "    X_val_text_numeric = hstack([X_val_text, X_val_numeric]).toarray()\n",
    "    \n",
    "    # Create DataFrames for the text and numeric features\n",
    "    X_train_text_numeric_df = pd.DataFrame(\n",
    "        X_train_text_numeric, \n",
    "        columns=[f'tfidf_feature_{i}' for i in range(X_train_text_numeric.shape[1])]\n",
    "    )\n",
    "    X_val_text_numeric_df = pd.DataFrame(\n",
    "        X_val_text_numeric, \n",
    "        columns=[f'tfidf_feature_{i}' for i in range(X_val_text_numeric.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Combine all features into DataFrames\n",
    "    X_train_all = pd.concat([X_train_text_numeric_df, X_train_cat], axis=1)\n",
    "    X_val_all = pd.concat([X_val_text_numeric_df, X_val_cat], axis=1)\n",
    "    \n",
    "    # Define categorical feature indices (positions in the final DataFrame)\n",
    "    cat_features = list(range(X_train_text_numeric.shape[1], X_train_all.shape[1]))\n",
    "    \n",
    "    # Create Pool objects with proper categorical features\n",
    "    train_pool = Pool(data=X_train_all, label=train_y, cat_features=cat_features)\n",
    "    val_pool = Pool(data=X_val_all, label=val_y, cat_features=cat_features)\n",
    "\n",
    "    # Train the model with CatBoost-specific parameters\n",
    "    model = CatBoostClassifier(\n",
    "        learning_rate=0.2,\n",
    "        iterations=100,\n",
    "        depth=10,\n",
    "        early_stopping_rounds=5,\n",
    "        use_best_model=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "    # Evaluation\n",
    "    preds = model.predict(val_pool)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(val_y, preds, average=\"binary\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_y, preds))\n",
    "\n",
    "    # Store probabilities for later use\n",
    "    model.scores = model.predict_proba(val_pool)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"fscore\": fscore,\n",
    "    }\n",
    "    \n",
    "    return model, metrics, val_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       325\n",
      "         1.0       1.00      1.00      1.00      5325\n",
      "\n",
      "    accuracy                           1.00      5650\n",
      "   macro avg       1.00      1.00      1.00      5650\n",
      "weighted avg       1.00      1.00      1.00      5650\n",
      "\n",
      "Predicted Class Distribution: (array([2.41121643e-04, 2.49152161e-04, 2.49167986e-04, ...,\n",
      "       9.99983900e-01, 9.99983901e-01, 9.99983902e-01]), array([1, 1, 1, ..., 1, 1, 1]))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m weather_model, weather_metrics, weather_val_pool \u001b[38;5;241m=\u001b[39m train_catboost_without_text_fields(\n\u001b[1;32m      3\u001b[0m     train_df\u001b[38;5;241m=\u001b[39mweather_X_train,\n\u001b[1;32m      4\u001b[0m     val_df\u001b[38;5;241m=\u001b[39mweather_X_val,\n\u001b[1;32m      5\u001b[0m     train_y\u001b[38;5;241m=\u001b[39mweather_y_train,\n\u001b[1;32m      6\u001b[0m     val_y\u001b[38;5;241m=\u001b[39mweather_y_val\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save the models using Joblib\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241m.\u001b[39mdump(weather_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather_ranking_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModels saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Use this function to train on your weather / no-weather datasets\n",
    "weather_model, weather_metrics, weather_val_pool = train_catboost_without_text_fields(\n",
    "    train_df=weather_X_train,\n",
    "    val_df=weather_X_val,\n",
    "    train_y=weather_y_train,\n",
    "    val_y=weather_y_val\n",
    ")\n",
    "\n",
    "# Save the models using Joblib\n",
    "joblib.dump(weather_model, 'weather_ranking_model.pkl')\n",
    "print(\"\\nModels saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interaction_type': 76.5778113641695,\n",
       " 'weather_description': 4.30674008101765,\n",
       " 'age': 3.237467817179253,\n",
       " 'category': 2.9370056032853573,\n",
       " 'maybe_count': 2.7196625311286935,\n",
       " 'event_indoor_capability': 2.2254651728353663,\n",
       " 'indoor_outdoor_preference': 2.1986309453821233,\n",
       " 'temperature_2m_mean': 1.9819653541549123,\n",
       " 'distance_to_event': 1.712327496290426,\n",
       " 'total_users': 0.8689565646688637,\n",
       " 'yes_count': 0.7116664731004836,\n",
       " 'invited_count': 0.3244445822618073,\n",
       " 'precipitation_sum': 0.14332783183238257,\n",
       " 'interaction_label': 0.05452766519541397,\n",
       " 'no_count': 2.939750863428235e-07,\n",
       " 'gender': 2.2352268228832082e-07,\n",
       " 'joinedAt': 0.0,\n",
       " 'location': 0.0,\n",
       " 'start_time': 0.0,\n",
       " 'city': 0.0,\n",
       " 'event_type': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_to_score = {\n",
    "    feature: score \n",
    "    for feature, score \n",
    "    in zip(\n",
    "        weather_X_train.columns, \n",
    "        weather_model.feature_importances_,\n",
    "    )\n",
    "}\n",
    "\n",
    "feat_to_score = dict(\n",
    "    sorted(\n",
    "        feat_to_score.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True,\n",
    "    )\n",
    ")\n",
    "feat_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       325\n",
      "         1.0       1.00      1.00      1.00      5325\n",
      "\n",
      "    accuracy                           1.00      5650\n",
      "   macro avg       1.00      1.00      1.00      5650\n",
      "weighted avg       1.00      1.00      1.00      5650\n",
      "\n",
      "Predicted Class Distribution: (array([4.23551057e-04, 4.30403995e-04, 4.39265302e-04, ...,\n",
      "       9.99971194e-01, 9.99971230e-01, 9.99971461e-01]), array([1, 1, 1, ..., 1, 1, 2]))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use this function to train on your weather / no-weather datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m no_weather_model, no_weather_metrics, no_weather_val_pool \u001b[38;5;241m=\u001b[39m train_catboost_without_text_fields(\n\u001b[1;32m      3\u001b[0m     train_df\u001b[38;5;241m=\u001b[39mno_weather_X_train,\n\u001b[1;32m      4\u001b[0m     val_df\u001b[38;5;241m=\u001b[39mno_weather_X_val,\n\u001b[1;32m      5\u001b[0m     train_y\u001b[38;5;241m=\u001b[39mno_weather_y_train,\n\u001b[1;32m      6\u001b[0m     val_y\u001b[38;5;241m=\u001b[39mno_weather_y_val\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241m.\u001b[39mdump(no_weather_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_weather_ranking_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModels saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Use this function to train on your weather / no-weather datasets\n",
    "no_weather_model, no_weather_metrics, no_weather_val_pool = train_catboost_without_text_fields(\n",
    "    train_df=no_weather_X_train,\n",
    "    val_df=no_weather_X_val,\n",
    "    train_y=no_weather_y_train,\n",
    "    val_y=no_weather_y_val\n",
    ")\n",
    "\n",
    "joblib.dump(no_weather_model, 'no_weather_ranking_model.pkl')\n",
    "print(\"\\nModels saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75      8951\n",
      "           1       0.72      0.58      0.64      7414\n",
      "\n",
      "    accuracy                           0.71     16365\n",
      "   macro avg       0.71      0.70      0.70     16365\n",
      "weighted avg       0.71      0.71      0.70     16365\n",
      "\n",
      "Predicted Class Distribution: (array([0.25162631, 0.25221187, 0.25478915, ..., 0.83534972, 0.83585689,\n",
      "       0.83656814]), array([1, 1, 1, ..., 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# Use this function to train on your weather / no-weather datasets\n",
    "no_weather_model, no_weather_metrics, pool_no_weather_val = train_catboost_without_text_fields(\n",
    "    train_df=no_weather_X_train,\n",
    "    val_df=no_weather_X_val,\n",
    "    train_y=no_weather_y_train,\n",
    "    val_y=no_weather_y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interaction_type': 54.72122052353688,\n",
       " 'interaction_label': 30.8715625371324,\n",
       " 'maybe_count': 2.3459777055818587,\n",
       " 'city': 2.2449991247582415,\n",
       " 'no_count': 1.6503214458250421,\n",
       " 'age': 1.5090039137423563,\n",
       " 'total_users': 1.4746309724073807,\n",
       " 'distance_to_event': 1.1914178305045156,\n",
       " 'yes_count': 1.1667068796907545,\n",
       " 'indoor_outdoor_preference': 1.1514013475146947,\n",
       " 'category': 0.7191426447743281,\n",
       " 'title': 0.48369153655103775,\n",
       " 'invited_count': 0.3897249471402347,\n",
       " 'start_time': 0.08019859084027804,\n",
       " 'gender': 0.0,\n",
       " 'joinedAt': 0.0,\n",
       " 'location': 0.0,\n",
       " 'user_interests': 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_to_score = {\n",
    "    feature: score \n",
    "    for feature, score \n",
    "    in zip(\n",
    "        no_weather_X_train.columns, \n",
    "        no_weather_model.feature_importances_,\n",
    "    )\n",
    "}\n",
    "\n",
    "feat_to_score = dict(\n",
    "    sorted(\n",
    "        feat_to_score.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True,\n",
    "    )\n",
    ")\n",
    "feat_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Evaluation function ready: scores ranking model using AUC, MAP, Precision@K, Recall@K, and NDCG@K.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, ndcg_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_ranking_model_proba(model, val_pool, val_y, k_list=[5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate a CatBoost ranking model using predicted probabilities, not binary class outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict class probabilities (not class labels)\n",
    "    proba = model.predict_proba(val_pool)[:, 1]  # Probability for class 1\n",
    "\n",
    "    results = {\n",
    "        \"AUC\": roc_auc_score(val_y, proba),\n",
    "        \"Average Precision (MAP)\": average_precision_score(val_y, proba),\n",
    "    }\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    true_labels = np.array(val_y)\n",
    "    predicted_scores = np.array(proba)\n",
    "\n",
    "    # Sort by predicted score\n",
    "    sorted_indices = np.argsort(predicted_scores)[::-1]\n",
    "    sorted_true = true_labels[sorted_indices]\n",
    "\n",
    "    for k in k_list:\n",
    "        top_k = sorted_true[:k]\n",
    "        precision_at_k = np.mean(top_k)\n",
    "        recall_at_k = np.sum(top_k) / np.sum(true_labels)\n",
    "        ndcg_at_k = ndcg_score(\n",
    "            y_true=true_labels.reshape(1, -1),\n",
    "            y_score=predicted_scores.reshape(1, -1),\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        results[f\"Precision@{k}\"] = precision_at_k\n",
    "        results[f\"Recall@{k}\"] = recall_at_k\n",
    "        results[f\"NDCG@{k}\"] = ndcg_at_k\n",
    "\n",
    "    return results\n",
    "\n",
    "\"✅ Evaluation function ready: scores ranking model using AUC, MAP, Precision@K, Recall@K, and NDCG@K.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Model Scores:\n",
      "AUC: 1.0000\n",
      "Average Precision (MAP): 1.0000\n",
      "Precision@5: 1.0000\n",
      "Recall@5: 0.0009\n",
      "NDCG@5: 1.0000\n",
      "Precision@10: 1.0000\n",
      "Recall@10: 0.0019\n",
      "NDCG@10: 1.0000\n",
      "\n",
      "No-Weather Model Scores:\n",
      "AUC: 1.0000\n",
      "Average Precision (MAP): 1.0000\n",
      "Precision@5: 1.0000\n",
      "Recall@5: 0.0009\n",
      "NDCG@5: 1.0000\n",
      "Precision@10: 1.0000\n",
      "Recall@10: 0.0019\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate weather-aware model\n",
    "weather_scores = evaluate_ranking_model_proba(\n",
    "    model=weather_model,\n",
    "    val_pool=weather_val_pool,\n",
    "    val_y=weather_y_val\n",
    ")\n",
    "\n",
    "# Evaluate no-weather model\n",
    "no_weather_scores = evaluate_ranking_model_proba(\n",
    "    model=no_weather_model,\n",
    "    val_pool=no_weather_val_pool,\n",
    "    val_y=no_weather_y_val\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"Weather Model Scores:\")\n",
    "for k, v in weather_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nNo-Weather Model Scores:\")\n",
    "for k, v in no_weather_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "def evaluate_and_save_model(model, val_pool, val_y, model_name=\"catboost_model\", ranking_dir=\"ranking_models\"):\n",
    "    \"\"\"\n",
    "    Evaluate the CatBoost model and save it for future use\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : CatBoostClassifier\n",
    "        Trained CatBoost model\n",
    "    val_pool : Pool\n",
    "        Validation data pool\n",
    "    val_y : array-like\n",
    "        Validation labels\n",
    "    model_name : str\n",
    "        Name for the saved model\n",
    "    save_dir : str\n",
    "        Directory to save the model and evaluation results\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(ranking_dir):\n",
    "        os.makedirs(ranking_dir)\n",
    "    \n",
    "    # Generate timestamp for model versioning\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"{model_name}_{timestamp}\"\n",
    "    \n",
    "    # 1. Basic evaluation metrics\n",
    "    y_pred = model.predict(val_pool)\n",
    "    y_pred_proba = model.predict_proba(val_pool)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(val_y, y_pred)\n",
    "    \n",
    "    # 2. ROC Curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(val_y, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f\"{ranking_dir}/{model_filename}_roc_curve.png\")\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(val_y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f\"{ranking_dir}/{model_filename}_confusion_matrix.png\")\n",
    "    \n",
    "    # 4. Feature Importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    feature_importances = model.get_feature_importance(val_pool)\n",
    "    feature_names = model.feature_names_\n",
    "    \n",
    "    # Sort feature importances\n",
    "    indices = np.argsort(feature_importances)[-20:]  # top 20 features\n",
    "    plt.barh(range(len(indices)), feature_importances[indices])\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{ranking_dir}/{model_filename}_feature_importance.png\")\n",
    "    \n",
    "    # 5. Save model and performance metrics\n",
    "    # Save the trained model\n",
    "    with open(f\"{ranking_dir}/{model_filename}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"feature_importance\": dict(zip(feature_names, feature_importances)),\n",
    "        \"model_path\": f\"{ranking_dir}/{model_filename}.pkl\"\n",
    "    }\n",
    "    \n",
    "    # Save metrics to JSON file\n",
    "    import json\n",
    "    with open(f\"{ranking_dir}/{model_filename}_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"Model saved to {ranking_dir}/{model_filename}.pkl\")\n",
    "    print(f\"Evaluation metrics saved to {ranking_dir}/{model_filename}_metrics.json\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming weather_model is your trained model from previous code\n",
    "metrics = evaluate_and_save_model(\n",
    "    model=weather_model,\n",
    "    val_pool=weather_val_pool,  # This should be the validation Pool object created earlier\n",
    "    val_y=weather_y_val,\n",
    "    model_name=\"weather_classifier\",\n",
    "    save_dir=\"weather_models\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you need to load the model later\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load a saved CatBoost model\"\"\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Example of using the saved model for prediction with new data\n",
    "def predict_with_saved_model(model_path, new_data):\n",
    "    \"\"\"\n",
    "    Make predictions using a saved model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_path : str\n",
    "        Path to the saved model file\n",
    "    new_data : Pool or DataFrame\n",
    "        New data to make predictions on\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        Predicted probabilities\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # If new_data is not already a Pool object, you'll need to convert it\n",
    "    # following the same preprocessing steps as during training\n",
    "    \n",
    "    # Return predictions\n",
    "    return model.predict_proba(new_data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks Model Registry\n",
    "model_registry = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'✅ Model upload function is ready for Hopsworks. Pass your CatBoost model, train data, metrics, and registry reference.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "import joblib\n",
    "\n",
    "def upload_catboost_model_to_hopsworks(\n",
    "    model, model_name: str, model_description: str,\n",
    "    model_registry, X_train, y_train, metrics: dict,\n",
    "    local_path\n",
    "):\n",
    "    # Save the model locally\n",
    "    joblib.dump(model, local_path)\n",
    "\n",
    "    # Create input/output schema\n",
    "    input_schema = Schema(X_train)\n",
    "    output_schema = Schema(y_train)\n",
    "    model_schema = ModelSchema(input_schema, output_schema)\n",
    "\n",
    "    # Sample input for UI representation\n",
    "    input_example = X_train.sample().to_dict(\"records\")\n",
    "\n",
    "    # Create and save model in Hopsworks\n",
    "    hopsworks_model = model_registry.python.create_model(\n",
    "        name=model_name,\n",
    "        metrics=metrics,\n",
    "        model_schema=model_schema,\n",
    "        input_example=input_example,\n",
    "        description=model_description,\n",
    "    )\n",
    "\n",
    "    hopsworks_model.save(local_path)\n",
    "\n",
    "    print(f\"✅ Model '{model_name}' successfully uploaded to Hopsworks.\")\n",
    "    return hopsworks_model\n",
    "\n",
    "\"✅ Model upload function is ready for Hopsworks. Pass your CatBoost model, train data, metrics, and registry reference.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1140cc76a90441e0beb44726928ef79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55259681dc4b410194fa682711214d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/262968 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c1ff13402c415794a1d73f7f0e77d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/598 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e0da89d3b74e3b943b8949a55261df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/1586 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1220788/models/weather_ranking_model/2\n",
      "✅ Model 'weather_ranking_model' successfully uploaded to Hopsworks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'weather_ranking_model', version: 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_catboost_model_to_hopsworks(\n",
    "    model=weather_model,\n",
    "    model_name=\"weather_ranking_model\",\n",
    "    model_description=\"Ranking model trained with weather features\",\n",
    "    model_registry=project.get_model_registry(),\n",
    "    X_train=weather_X_train, \n",
    "    y_train=weather_y_train,\n",
    "    metrics=weather_metrics,\n",
    "    local_path=\"weather_ranking_model.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c978f1313ceb418aa945b5feb19100e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff138e7610dc4d4db3dcb91b64b00a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/901551 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818c3179dcaf41f993ece7b25a78f0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/505 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1945185a09b14a7a8cf475d467e2cc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/1348 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1220788/models/no_weather_ranking_model/2\n",
      "✅ Model 'no_weather_ranking_model' successfully uploaded to Hopsworks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'no_weather_ranking_model', version: 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_catboost_model_to_hopsworks(\n",
    "    model=no_weather_model,\n",
    "    model_name=\"no_weather_ranking_model\",\n",
    "    model_description=\"Ranking model trained without weather features\",\n",
    "    model_registry=project.get_model_registry(),\n",
    "    X_train=no_weather_X_train, \n",
    "    y_train=no_weather_y_train,\n",
    "    metrics=no_weather_metrics,\n",
    "    local_path=\"no_weather_ranking_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction distribution by user:\n",
      "count\n",
      "40    1367\n",
      "1      707\n",
      "12     629\n",
      "11     614\n",
      "5      429\n",
      "41     292\n",
      "13     234\n",
      "2      224\n",
      "45     208\n",
      "16     158\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, zipf\n",
    "from mimesis import Generic\n",
    "import re\n",
    "\n",
    "# Initialize components\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ====================== #\n",
    "# 1. Core Configuration #\n",
    "# ====================== #\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', \n",
    "          'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    k: (float(v[0]), float(v[1])) for k,v in {\n",
    "        'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278),\n",
    "        'Paris': (48.8566, 2.3522), 'Tokyo': (35.6762, 139.6503),\n",
    "        'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "        'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333),\n",
    "        'Toronto': (43.6532, -79.3832), 'Dubai': (25.2048, 55.2708)\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "# ====================== #\n",
    "# 2. Helper Functions #\n",
    "# ====================== #\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    return (\n",
    "        base_lat + np.random.uniform(-0.1, 0.1),\n",
    "        base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    )\n",
    "\n",
    "def get_relevant_events(user, events):\n",
    "    # Access user_interests as attribute, not dictionary key\n",
    "    user_interests = set(user.user_interests.split(','))\n",
    "    return events[\n",
    "        events.event_type.apply(\n",
    "            lambda x: any(i.lower() in x.lower() for i in user_interests)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def safe_sample(df, n):\n",
    "    return df.sample(n, replace=True) if len(df) > 0 else pd.DataFrame()\n",
    "\n",
    "# ====================== #\n",
    "# 3. User Generation #\n",
    "# ====================== #\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', \n",
    "                'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Generate interests with power-law distribution\n",
    "        num_interests = min(zipf.rvs(1.2), 4)\n",
    "        user_interests = random.sample(interests, k=num_interests)\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'age': age,\n",
    "            'user_interests': ','.join(user_interests),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "# ====================== #\n",
    "# 4. Event Generation #\n",
    "# ====================== #\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', \n",
    "        'Arts & Culture', 'Entertainment', 'Sports & Fitness', \n",
    "        'Business & Networking', 'Health & Wellness', 'Music & Concerts', \n",
    "        'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        start_time = fake.date_time_between(\n",
    "            start_date=datetime(2025, 3, 27), \n",
    "            end_date=datetime(2025, 12, 31)\n",
    "        )\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} in {city}\",\n",
    "            'event_type': random.choice(event_types),\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': random.choice([120, 180, 240]),\n",
    "            'expected_attendance': int(np.random.lognormal(mean=6, sigma=0.5))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "# ====================== #\n",
    "# 5. Interaction Generation #\n",
    "# ====================== #\n",
    "def generate_interactions(users, events, n_interactions=150000):\n",
    "    interactions = []\n",
    "    \n",
    "    # Phase 1: Base users (attend few events)\n",
    "    base_users = users.sample(frac=0.2)\n",
    "    for user in base_users.itertuples():\n",
    "        num_events = min(zipf.rvs(2.5), 2)  # ⬅ lower number of events\n",
    "        candidate_events = get_relevant_events(user, events)\n",
    "        selected_events = safe_sample(candidate_events, num_events)\n",
    "        for event in selected_events.itertuples():\n",
    "            interactions.append(create_interaction(user, event))\n",
    "\n",
    "    # Phase 2: Regular users\n",
    "    regular_users = users.drop(base_users.index).sample(frac=0.3)\n",
    "    for user in regular_users.itertuples():\n",
    "        num_events = min(zipf.rvs(1.5) + 4, 12)  # ⬅ still modest\n",
    "        candidate_events = get_relevant_events(user, events)\n",
    "        selected_events = safe_sample(candidate_events, num_events)\n",
    "        for event in selected_events.itertuples():\n",
    "            interactions.append(create_interaction(user, event))\n",
    "\n",
    "    # Phase 3: Power users\n",
    "    power_users = users.nlargest(int(len(users)*0.5), 'social_connectedness')\n",
    "    for user in power_users.itertuples():\n",
    "        num_events = min(zipf.rvs(1.2) + 10, 40)  # ⬅ significantly higher\n",
    "        candidate_events = events.sample(250)\n",
    "        selected_events = safe_sample(candidate_events, num_events)\n",
    "        for event in selected_events.itertuples():\n",
    "            interactions.append(create_interaction(user, event))\n",
    "\n",
    "    # Final interaction DataFrame\n",
    "    df = pd.DataFrame(interactions)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(\"Interaction distribution by user:\")\n",
    "    print(df['user_id'].value_counts().value_counts().head(10))  # Debug: see how many users attend how many events\n",
    "    \n",
    "    return df\n",
    "\n",
    "    #return balance_labels(df)\n",
    "\n",
    "def create_interaction(user, event):\n",
    "    distance = geodesic(\n",
    "        (user.user_lat, user.user_lon),  # Changed from ['user_lat']\n",
    "        (event.event_lat, event.event_lon)\n",
    "    ).km\n",
    "    \n",
    "    interest_match = len(\n",
    "        set(user.user_interests.split(',')) &  # Changed from ['user_interests']\n",
    "        set(event.event_type.split(' & '))\n",
    "    ) > 0\n",
    "    \n",
    "    return {\n",
    "        'user_id': user.user_id,  # Changed from ['user_id']\n",
    "        'event_id': event.event_id,\n",
    "        'distance_to_event': distance,\n",
    "        'event_type_in_user_interests': int(interest_match),\n",
    "        'interest_match': interest_match,\n",
    "        'interaction_label': 1 if random.random() < (0.6 if interest_match else 0.4) else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def balance_labels(df):\n",
    "    pos = df[df.interaction_label == 1]\n",
    "    neg = df[df.interaction_label == 0]\n",
    "    min_count = min(len(pos), len(neg))\n",
    "    \n",
    "    return pd.concat([\n",
    "        pos.sample(min_count, replace=True),\n",
    "        neg.sample(min_count, replace=True)\n",
    "    ]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# ====================== #\n",
    "# 6. Execution #\n",
    "# ====================== #\n",
    "if __name__ == \"__main__\":\n",
    "    users_df = generate_users(10000)\n",
    "    events_df = generate_events(5000)\n",
    "    interactions_df = generate_interactions(users_df, events_df)\n",
    "    \n",
    "    # # Save datasets\n",
    "    # users_df.to_csv('synthetic_users.csv', index=False)\n",
    "    # events_df.to_csv('synthetic_events.csv', index=False)\n",
    "    # interactions_df.to_csv('synthetic_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107309"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707 attended 1 events\n",
      "224 attended 2 events\n",
      "0 attended 3 events\n",
      "0 attended 4 events\n",
      "429 attended 5 events\n",
      "153 attended 6 events\n",
      "65 attended 7 events\n",
      "55 attended 8 events\n",
      "30 attended 9 events\n",
      "20 attended 10 events\n",
      "614 attended 11 events\n",
      "629 attended 12 events\n",
      "234 attended 13 events\n",
      "146 attended 14 events\n",
      "102 attended 15 events\n",
      "158 attended 16 events\n",
      "129 attended 17 events\n",
      "121 attended 18 events\n",
      "71 attended 19 events\n",
      "79 attended 20 events\n",
      "68 attended 21 events\n",
      "54 attended 22 events\n",
      "106 attended 23 events\n",
      "74 attended 24 events\n",
      "56 attended 25 events\n",
      "50 attended 26 events\n",
      "49 attended 27 events\n",
      "31 attended 28 events\n",
      "29 attended 29 events\n",
      "31 attended 30 events\n",
      "24 attended 31 events\n",
      "23 attended 32 events\n",
      "24 attended 33 events\n",
      "21 attended 34 events\n",
      "22 attended 35 events\n",
      "17 attended 36 events\n",
      "31 attended 37 events\n",
      "26 attended 38 events\n",
      "12 attended 39 events\n",
      "1367 attended 40 events\n",
      "292 attended 41 events\n",
      "109 attended 42 events\n",
      "0 attended 43 events\n",
      "6 attended 44 events\n",
      "208 attended 45 events\n",
      "66 attended 46 events\n",
      "34 attended 47 events\n",
      "32 attended 48 events\n",
      "27 attended 49 events\n"
     ]
    }
   ],
   "source": [
    "events_per_user = interactions_df.groupby(\"user_id\").size()\n",
    "max(events_per_user)\n",
    "\n",
    "for i in range(1, 50):\n",
    "    print(f\"{events_per_user[events_per_user == i].size} attended {i} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interaction_label\n",
       "0    91978\n",
       "1    61130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "interactions_df[\"interaction_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'user_lat', 'user_lon', 'user_city', 'age', 'user_interests',\n",
       "       'social_connectedness'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'title', 'event_type', 'event_lat', 'event_lon',\n",
       "       'event_city', 'start_time', 'duration', 'expected_attendance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'event_id', 'distance_to_event',\n",
       "       'event_type_in_user_interests', 'interest_match', 'interaction_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>distance_to_event</th>\n",
       "      <th>event_type_in_user_interests</th>\n",
       "      <th>interest_match</th>\n",
       "      <th>interaction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JE120K</td>\n",
       "      <td>AK136W</td>\n",
       "      <td>9601.453066</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DV609A</td>\n",
       "      <td>MH837R</td>\n",
       "      <td>6289.333916</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EF903S</td>\n",
       "      <td>UL464S</td>\n",
       "      <td>540.419648</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UF178B</td>\n",
       "      <td>LD259A</td>\n",
       "      <td>11088.821115</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF768O</td>\n",
       "      <td>TS601C</td>\n",
       "      <td>8.373094</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id event_id  distance_to_event  event_type_in_user_interests  \\\n",
       "0  JE120K   AK136W        9601.453066                             0   \n",
       "1  DV609A   MH837R        6289.333916                             0   \n",
       "2  EF903S   UL464S         540.419648                             0   \n",
       "3  UF178B   LD259A       11088.821115                             0   \n",
       "4  DF768O   TS601C           8.373094                             0   \n",
       "\n",
       "   interest_match  interaction_label  \n",
       "0           False                  0  \n",
       "1           False                  0  \n",
       "2           False                  0  \n",
       "3           False                  0  \n",
       "4           False                  0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "from mimesis import Generic\n",
    "import re\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City configurations\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', \n",
    "          'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    k: (float(v[0]), float(v[1])) for k,v in {\n",
    "        'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278),\n",
    "        'Paris': (48.8566, 2.3522), 'Tokyo': (35.6762, 139.6503),\n",
    "        'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "        'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333),\n",
    "        'Toronto': (43.6532, -79.3832), 'Dubai': (25.2048, 55.2708)\n",
    "    }.items()\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', \n",
    "                'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        # Generate interests with power-law distribution\n",
    "        num_interests = min(np.random.zipf(1.2), 5)\n",
    "        user_interests = random.sample(interests, k=num_interests)\n",
    "        user_interests = list(set(user_interests))[:4]  # Max 4 unique interests\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'user_weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'user_interests': ','.join(user_interests),\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', \n",
    "        'Arts & Culture', 'Entertainment', 'Sports & Fitness', \n",
    "        'Business & Networking', 'Health & Wellness', 'Music & Concerts', \n",
    "        'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    current_date = datetime(2025, 3, 27, 11, 48)\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        event_type = random.choice(event_types)\n",
    "        start_time = fake.date_time_between(\n",
    "            start_date=current_date, \n",
    "            end_date=current_date + timedelta(days=180)\n",
    "        )\n",
    "        \n",
    "        # Weather and temperature logic\n",
    "        if event_type in ['Sports & Fitness', 'Seasonal & Festivals']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else random.choice(['Rain', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = random.choice(['Clear', 'Cloudy', 'Rain', 'Snow', 'Windy'])\n",
    "        \n",
    "        base_temp = {\n",
    "            'New York': 15, 'London': 12, 'Paris': 16, 'Tokyo': 20, \n",
    "            'Sydney': 22, 'Berlin': 14, 'Mumbai': 28, 'São Paulo': 24, \n",
    "            'Toronto': 10, 'Dubai': 32\n",
    "        }[city]\n",
    "        \n",
    "        temp_adjustment = {\n",
    "            'Clear': np.random.uniform(2, 5),\n",
    "            'Rain': np.random.uniform(-3, 0),\n",
    "            'Snow': np.random.uniform(-8, -3),\n",
    "            'Cloudy': np.random.uniform(-1, 2),\n",
    "            'Windy': np.random.uniform(-2, 1)\n",
    "        }[weather_condition]\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'temperature': round(base_temp + temp_adjustment, 1),\n",
    "            'attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'event_indoor_capability': event_type in ['Education & Learning', 'Technology', \n",
    "                                                    'Business & Networking', 'Arts & Culture',\n",
    "                                                    'Entertainment', 'Immersive Experiences']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    zipf_param = 1.8  # More skewed distribution\n",
    "    max_events_per_user = 25\n",
    "    current_time = datetime(2025, 3, 27, 11, 48)\n",
    "    \n",
    "    def safe_sample(df, n):\n",
    "        return df.sample(n, replace=True) if len(df) > 0 else pd.DataFrame()\n",
    "\n",
    "    # Phase 1: Core interactions with power-law distribution\n",
    "    for user in users.itertuples():\n",
    "        user_interests = set(user.user_interests.split(','))\n",
    "        base_count = np.random.zipf(zipf_param)\n",
    "        events_to_attend = min(base_count, max_events_per_user)\n",
    "        \n",
    "        # Get interest-matching events with case-insensitive check\n",
    "        candidate_events = events[\n",
    "            events['event_type'].apply(\n",
    "                lambda x: any(i.lower() in x.lower() for i in user_interests)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        if len(candidate_events) == 0:\n",
    "            continue  # Skip users with no matching events\n",
    "            \n",
    "        candidate_sample = safe_sample(candidate_events, 50)\n",
    "        if len(candidate_sample) == 0:\n",
    "            continue\n",
    "\n",
    "        for event in candidate_sample.itertuples():\n",
    "            if events_to_attend <= 0:\n",
    "                break\n",
    "            \n",
    "            distance = geodesic(\n",
    "                (user.user_lat, user.user_lon),\n",
    "                (event.event_lat, event.event_lon)\n",
    "            ).km\n",
    "            \n",
    "            if distance < 50 and random.random() < 0.7:\n",
    "                interactions.append({\n",
    "                    'user_id': user.user_id,\n",
    "                    'event_id': event.event_id,\n",
    "                    'distance_to_event': distance,\n",
    "                    'event_type_in_user_interests': 1,\n",
    "                    'interaction_label': 1\n",
    "                })\n",
    "                events_to_attend -= 1\n",
    "\n",
    "    # Phase 2: Power users with many interactions\n",
    "    power_users = users.nlargest(500, 'social_connectedness')\n",
    "    for user in power_users.itertuples():\n",
    "        additional_events = min(np.random.zipf(zipf_param*1.5), max_events_per_user)\n",
    "        candidate_events = events.sample(100)\n",
    "        \n",
    "        for event in candidate_events.itertuples():\n",
    "            if additional_events <= 0:\n",
    "                break\n",
    "            \n",
    "            distance = geodesic(\n",
    "                (user.user_lat, user.user_lon),\n",
    "                (event.event_lat, event.event_lon)\n",
    "            ).km\n",
    "            \n",
    "            if distance < 100 and random.random() < 0.8:\n",
    "                interactions.append({\n",
    "                    'user_id': user.user_id,\n",
    "                    'event_id': event.event_id,\n",
    "                    'distance_to_event': distance,\n",
    "                    'event_type_in_user_interests': int(\n",
    "                        any(i in event.event_type for i in user.user_interests.split(','))\n",
    "                    ),\n",
    "                    'interaction_label': 1\n",
    "                })\n",
    "                additional_events -= 1\n",
    "\n",
    "    # Phase 3: Fill remaining interactions\n",
    "    while len(interactions) < n_interactions:\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        distance = geodesic(\n",
    "            (user['user_lat'], user['user_lon']),\n",
    "            (event['event_lat'], event['event_lon'])\n",
    "        ).km\n",
    "        \n",
    "        interactions.append({\n",
    "            'user_id': user['user_id'],\n",
    "            'event_id': event['event_id'],\n",
    "            'distance_to_event': distance,\n",
    "            'event_type_in_user_interests': int(\n",
    "                any(i in event['event_type'] for i in user['user_interests'].split(','))\n",
    "            ),\n",
    "            'interaction_label': 1 if random.random() < 0.3 else 0\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(interactions[:n_interactions])\n",
    "\n",
    "# Generate datasets\n",
    "users_df = generate_users(10000)\n",
    "events_df = generate_events(5000)\n",
    "interactions_df = generate_interactions(users_df, events_df, 150000)\n",
    "\n",
    "# # Save to CSV\n",
    "# users_df.to_csv('synthetic_users.csv', index=False)\n",
    "# events_df.to_csv('synthetic_events.csv', index=False)\n",
    "# interactions_df.to_csv('synthetic_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 attended 1 events\n",
      "0 attended 2 events\n",
      "0 attended 3 events\n",
      "0 attended 4 events\n",
      "0 attended 5 events\n",
      "0 attended 6 events\n",
      "0 attended 7 events\n",
      "0 attended 8 events\n",
      "0 attended 9 events\n",
      "618 attended 10 events\n",
      "653 attended 11 events\n",
      "672 attended 12 events\n",
      "690 attended 13 events\n",
      "663 attended 14 events\n",
      "671 attended 15 events\n",
      "628 attended 16 events\n",
      "692 attended 17 events\n",
      "619 attended 18 events\n",
      "710 attended 19 events\n",
      "626 attended 20 events\n",
      "685 attended 21 events\n",
      "696 attended 22 events\n",
      "688 attended 23 events\n",
      "688 attended 24 events\n",
      "0 attended 25 events\n",
      "0 attended 26 events\n",
      "0 attended 27 events\n",
      "0 attended 28 events\n",
      "0 attended 29 events\n",
      "0 attended 30 events\n",
      "0 attended 31 events\n",
      "0 attended 32 events\n",
      "0 attended 33 events\n",
      "0 attended 34 events\n",
      "0 attended 35 events\n",
      "0 attended 36 events\n",
      "0 attended 37 events\n",
      "0 attended 38 events\n",
      "0 attended 39 events\n",
      "0 attended 40 events\n",
      "0 attended 41 events\n",
      "0 attended 42 events\n",
      "0 attended 43 events\n",
      "0 attended 44 events\n",
      "0 attended 45 events\n",
      "0 attended 46 events\n",
      "0 attended 47 events\n",
      "0 attended 48 events\n",
      "0 attended 49 events\n"
     ]
    }
   ],
   "source": [
    "events_per_user = interactions_df.groupby(\"user_id\").size()\n",
    "max(events_per_user)\n",
    "\n",
    "for i in range(1, 50):\n",
    "    print(f\"{events_per_user[events_per_user == i].size} attended {i} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>distance_to_event</th>\n",
       "      <th>event_type_in_user_interests</th>\n",
       "      <th>interaction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZN105D</td>\n",
       "      <td>BJ353P</td>\n",
       "      <td>16.615926</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VS446V</td>\n",
       "      <td>YN703P</td>\n",
       "      <td>15.843688</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VS446V</td>\n",
       "      <td>WF293K</td>\n",
       "      <td>10.646002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VS446V</td>\n",
       "      <td>TV389J</td>\n",
       "      <td>3.041735</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VS446V</td>\n",
       "      <td>WF293K</td>\n",
       "      <td>10.646002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id event_id  distance_to_event  event_type_in_user_interests  \\\n",
       "0  ZN105D   BJ353P          16.615926                             1   \n",
       "1  VS446V   YN703P          15.843688                             1   \n",
       "2  VS446V   WF293K          10.646002                             1   \n",
       "3  VS446V   TV389J           3.041735                             1   \n",
       "4  VS446V   WF293K          10.646002                             1   \n",
       "\n",
       "   interaction_label  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12936"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions_df[interactions_df[\"event_type_in_user_interests\"]==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interaction_label\n",
       "0    95360\n",
       "1    54640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[\"interaction_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of events attended per user:\n",
      "2210 attended 1 events\n",
      "1811 attended 2 events\n",
      "1736 attended 3 events\n",
      "1816 attended 4 events\n",
      "1763 attended 5 events\n",
      "1753 attended 6 events\n",
      "1514 attended 7 events\n",
      "1252 attended 8 events\n",
      "952 attended 9 events\n",
      "648 attended 10 events\n",
      "435 attended 11 events\n",
      "276 attended 12 events\n",
      "152 attended 13 events\n",
      "69 attended 14 events\n",
      "50 attended 15 events\n",
      "22 attended 16 events\n",
      "14 attended 17 events\n",
      "8 attended 18 events\n",
      "1 attended 19 events\n",
      "1 attended 20 events\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "from mimesis import Generic\n",
    "import hopsworks\n",
    "import re\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        # Ensure at least one interest is selected\n",
    "        user_interests = random.sample(interests, k=random.randint(1, min(4, len(interests))))\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'user_weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'user_interests': ','.join(user_interests),  # Join interests into a comma-separated string\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', 'Arts & Culture', \n",
    "        'Entertainment', 'Sports & Fitness', 'Business & Networking', 'Health & Wellness', \n",
    "        'Music & Concerts', 'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    current_date = datetime(2025, 3, 27, 11, 48)  # Current date and time\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        if event_type in ['Sports & Fitness', 'Seasonal & Festivals']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['Education & Learning', 'Technology', 'Business & Networking']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        \n",
    "        base_temp = {\n",
    "            'New York': 15, 'London': 12, 'Paris': 16, 'Tokyo': 20, \n",
    "            'Sydney': 22, 'Berlin': 14, 'Mumbai': 28, 'São Paulo': 24, \n",
    "            'Toronto': 10, 'Dubai': 32\n",
    "        }[city]\n",
    "        \n",
    "        temp_adjustment = {\n",
    "            'Clear': np.random.uniform(2, 5),\n",
    "            'Rain': np.random.uniform(-3, 0),\n",
    "            'Snow': np.random.uniform(-8, -3),\n",
    "            'Cloudy': np.random.uniform(-1, 2),\n",
    "            'Windy': np.random.uniform(-2, 1)\n",
    "        }[weather_condition]\n",
    "        \n",
    "        temperature = round(base_temp + temp_adjustment, 1)\n",
    "        \n",
    "        start_time = fake.date_time_between(start_date=current_date, end_date=current_date + timedelta(days=180))\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'temperature': temperature,\n",
    "            'attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'event_indoor_capability': event_type in ['Education & Learning', 'Technology', 'Business & Networking', \n",
    "                                               'Arts & Culture', 'Entertainment', 'Immersive Experiences']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def calculate_time_weight(interaction_time, current_time, half_life=30):\n",
    "    time_diff = (current_time - interaction_time).days\n",
    "    return np.exp(np.log(0.5) * time_diff / half_life)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    interaction_types = ['maybe', 'invited & maybe', 'no', 'yes', 'invited & yes', 'invited & no', 'invited']\n",
    "    attempts = n_interactions * 5\n",
    "    current_time = datetime(2025, 3, 27, 11, 48)\n",
    "    \n",
    "    # Create a user participation bias - some users are much more active than others\n",
    "    # Using a reversed power law distribution to favor higher event attendance\n",
    "    participation_factor = 1 - np.random.power(0.5, size=len(users))\n",
    "    users['participation_factor'] = participation_factor\n",
    "    \n",
    "    # Sort users by participation factor (higher values first)\n",
    "    users = users.sort_values('participation_factor', ascending=False)\n",
    "    \n",
    "    # Group events by city to increase the probability of users attending events in their city\n",
    "    events_by_city = {city: events[events['event_city'] == city] for city in cities}\n",
    "    \n",
    "    # Create a dictionary to track user event counts\n",
    "    user_event_count = {user_id: 0 for user_id in users['user_id']}\n",
    "    \n",
    "    for _ in range(attempts):\n",
    "        if len(interactions) >= n_interactions:\n",
    "            break\n",
    "            \n",
    "        # Sample users with probability proportional to their participation factor\n",
    "        # This ensures users with higher participation factors get selected more often\n",
    "        user_probs = users['participation_factor'] / users['participation_factor'].sum()\n",
    "        user_idx = np.random.choice(range(len(users)), p=user_probs)\n",
    "        user = users.iloc[user_idx]\n",
    "        \n",
    "        # Prioritize events in the user's city with higher probability\n",
    "        user_city_events = events_by_city.get(user['user_city'], pd.DataFrame())\n",
    "        if len(user_city_events) > 0 and random.random() < 0.85:\n",
    "            event = user_city_events.sample(1).iloc[0]\n",
    "        else:\n",
    "            event = events.sample(1).iloc[0]\n",
    "        \n",
    "        distance = geodesic((user['user_lat'], user['user_lon']), \n",
    "                           (event['event_lat'], event['event_lon'])).km\n",
    "        \n",
    "        # Adjust distance calculation to favor closer events\n",
    "        distance_score = np.exp(-distance/50)  # More forgiving distance score (changed from 10 to 50)\n",
    "        \n",
    "        weather_score = 1.5 if (event['weather_condition'] == 'Clear' and \n",
    "                               user['user_weather_preference'] in ['outdoor', 'any']) else 0.8\n",
    "        \n",
    "        # Increase the impact of social connectedness\n",
    "        social_score = np.log1p(user['social_connectedness']) / 5  # Changed from 10 to 5\n",
    "        \n",
    "        # Add participation factor to interaction probability\n",
    "        participation_boost = user['participation_factor'] * 0.5\n",
    "        \n",
    "        # Combine all factors with emphasis on participation\n",
    "        interaction_prob = 0.5*distance_score + 0.1*weather_score + 0.1*social_score + 0.3*participation_boost\n",
    "        \n",
    "        # Increase the maximum distance threshold\n",
    "        max_distance = 100 if random.random() < 0.8 else 500  # Increased from 50/300 to 100/500\n",
    "        \n",
    "        if distance < max_distance and (random.random() < interaction_prob):\n",
    "            interaction_time = fake.date_time_between(\n",
    "                start_date=event['start_time'] - timedelta(days=30), \n",
    "                end_date=event['start_time']\n",
    "            )\n",
    "            time_weight = calculate_time_weight(interaction_time, current_time)\n",
    "            interaction_prob *= time_weight\n",
    "            \n",
    "            # Adjust interaction type probabilities to favor positive responses\n",
    "            if distance <= 10:\n",
    "                interaction_type_probs = [0.15, 0.25, 0.05, 0.35, 0.15, 0.03, 0.02]  # More yes and invited & yes\n",
    "            elif distance <= 40:\n",
    "                interaction_type_probs = [0.20, 0.20, 0.05, 0.30, 0.15, 0.05, 0.05]\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.25, 0.15, 0.10, 0.20, 0.15, 0.10, 0.05]\n",
    "            elif distance <= 200:\n",
    "                interaction_type_probs = [0.20, 0.10, 0.15, 0.15, 0.15, 0.15, 0.10]\n",
    "            else:\n",
    "                interaction_type_probs = [0.15, 0.10, 0.20, 0.15, 0.10, 0.20, 0.10]\n",
    "            \n",
    "            # Select interaction type\n",
    "            interaction_type = np.random.choice(interaction_types, p=interaction_type_probs)\n",
    "            \n",
    "            # Determine if this counts as attendance (maybe or yes)\n",
    "            is_attending = interaction_type in ['maybe', 'invited & maybe', 'yes', 'invited & yes']\n",
    "            \n",
    "            # Add to user's event count if attending\n",
    "            if is_attending:\n",
    "                user_event_count[user['user_id']] += 1\n",
    "                \n",
    "            # Create the interaction record\n",
    "            interactions.append({\n",
    "                'interaction_id': generic.person.identifier(mask='@@###@'),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': interaction_type,\n",
    "                'distance_to_event': distance,\n",
    "                'interaction_label': 1 if is_attending else 0\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    interactions_df = pd.DataFrame(interactions)\n",
    "    \n",
    "    return interactions_df\n",
    "\n",
    "# Generate data\n",
    "users_df = generate_users(n_users=20000)\n",
    "events_df = generate_events(n_events=5000)\n",
    "interactions_df = generate_interactions(users_df, events_df, n_interactions=100000)\n",
    "\n",
    "# Calculate and display the distribution of events attended per user\n",
    "attendance_counts = interactions_df[interactions_df['interaction_label'] == 1].groupby('user_id').size()\n",
    "attendance_distribution = attendance_counts.value_counts().sort_index()\n",
    "\n",
    "print(\"Distribution of events attended per user:\")\n",
    "for count, num_users in attendance_distribution.items():\n",
    "    print(f\"{num_users} attended {count} events\")\n",
    "\n",
    "# # Optional: Save data to CSV files\n",
    "# users_df.to_csv('users.csv', index=False)\n",
    "# events_df.to_csv('events.csv', index=False)\n",
    "# interactions_df.to_csv('interactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interaction_label\n",
       "1    85861\n",
       "0    14139\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df['interaction_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>distance_to_event</th>\n",
       "      <th>interaction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XN454Q</td>\n",
       "      <td>FI641K</td>\n",
       "      <td>TJ649N</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>10.159621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TD269S</td>\n",
       "      <td>ZT737H</td>\n",
       "      <td>ZJ827M</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>5.877954</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RU745A</td>\n",
       "      <td>JR639E</td>\n",
       "      <td>JU394E</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>5.391734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UN286X</td>\n",
       "      <td>MS715T</td>\n",
       "      <td>QQ190V</td>\n",
       "      <td>invited &amp; no</td>\n",
       "      <td>10.824373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OW716Q</td>\n",
       "      <td>TP904C</td>\n",
       "      <td>GE665R</td>\n",
       "      <td>yes</td>\n",
       "      <td>10.946627</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interaction_id user_id event_id interaction_type  distance_to_event  \\\n",
       "0         XN454Q  FI641K   TJ649N  invited & maybe          10.159621   \n",
       "1         TD269S  ZT737H   ZJ827M  invited & maybe           5.877954   \n",
       "2         RU745A  JR639E   JU394E  invited & maybe           5.391734   \n",
       "3         UN286X  MS715T   QQ190V     invited & no          10.824373   \n",
       "4         OW716Q  TP904C   GE665R              yes          10.946627   \n",
       "\n",
       "   interaction_label  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  0  \n",
       "4                  1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skewnorm, dirichlet, zipf\n",
    "from mimesis import Generic\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "generic = Generic('en')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "event_category = [\n",
    "    'Education',          # Lectures, seminars, or academic events (e.g., university talks)\n",
    "    'Technology',        # Tech meetups, hackathons, or product launches\n",
    "    'Arts',              # Art gallery openings, visual arts exhibitions\n",
    "    'Festival',          # Cultural, music, or seasonal festivals (e.g., summer street fairs)\n",
    "    'Entertainment',     # General entertainment like comedy shows or magic performances\n",
    "    'Sports',            # Sporting events (e.g., soccer matches, marathons)\n",
    "    'Business',          # Networking events, startup pitches, or corporate events\n",
    "    'Conference',        # Professional or industry conferences (e.g., tech or medical summits)\n",
    "    'Exhibition',        # Trade shows, museum exhibits, or product showcases\n",
    "    'Workshop',          # Hands-on learning sessions (e.g., coding or painting workshops)\n",
    "    'Health',            # Wellness events like yoga classes or mental health talks\n",
    "    'Music Concerts',    # Live music performances (e.g., rock, jazz, classical)\n",
    "    'Food Fair',         # Culinary events like food truck festivals or wine tastings\n",
    "    'Theater',           # Plays, musicals, or performance art\n",
    "    'Cinema',            # Movie screenings, film festivals, or premieres\n",
    "    'Dance',             # Dance performances or classes (e.g., ballet, salsa)\n",
    "    'Literature',        # Book readings, author talks, or poetry slams\n",
    "    'Charity',           # Fundraising events, charity runs, or volunteer drives\n",
    "    'Fashion',           # Fashion shows, pop-up boutiques, or designer showcases\n",
    "    'Gaming',            # Esports tournaments, board game nights, or VR experiences\n",
    "    'Outdoor Adventure', # Hiking, camping, or adventure sports events\n",
    "    'Family',            # Kid-friendly events like puppet shows or science fairs\n",
    "    'Cultural',          # Heritage celebrations, cultural expos, or religious festivals\n",
    "    'Nightlife',         # Club events, DJ nights, or bar crawls\n",
    "    'Fitness',           # Group fitness classes, fun runs, or obstacle courses\n",
    "    'Science',           # Science fairs, planetarium shows, or innovation demos\n",
    "    'Craft',             # Craft fairs, DIY workshops, or maker markets\n",
    "    'Photography',       # Photo walks, exhibitions, or photography classes\n",
    "    'History',           # Historical reenactments, museum tours, or heritage walks\n",
    "    'Virtual',           # Online events, webinars, or virtual reality meetups\n",
    "]\n",
    "\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=10000):\n",
    "    users = []\n",
    "    interests = [\n",
    "    'music',              # Interest in music-related events (e.g., concerts, music festivals)\n",
    "    'sports',            # Interest in sports events (e.g., matches, marathons)\n",
    "    'technology',              # Interest in technology (e.g., hackathons, tech talks)\n",
    "    'food',              # Interest in culinary events (e.g., food fairs, wine tastings)\n",
    "    'art',               # Interest in visual arts (e.g., gallery exhibitions, art workshops)\n",
    "    'literature',        # Interest in books and writing (e.g., author talks, poetry slams)\n",
    "    'cinema',            # Interest in movies (e.g., film screenings, festivals)\n",
    "    'travel',            # Interest in travel-related events (e.g., travel expos, adventure talks)\n",
    "    'fitness',           # Interest in physical activities (e.g., yoga, fun runs)\n",
    "    'fashion',           # Interest in fashion events (e.g., fashion shows, pop-up boutiques)\n",
    "    'festival',          # Interest in festivals (e.g., cultural, seasonal, or music festivals)\n",
    "    'health',            # Interest in wellness (e.g., mental health talks, wellness workshops)\n",
    "    'business',          # Interest in professional events (e.g., networking, startup pitches)\n",
    "    'theater',           # Interest in performing arts (e.g., plays, musicals)\n",
    "    'dance',             # Interest in dance (e.g., ballet performances, salsa classes)\n",
    "    'gaming',            # Interest in gaming (e.g., esports, board game nights)\n",
    "    'outdoor',           # Interest in outdoor activities (e.g., hiking, camping)\n",
    "    'family',            # Interest in family-friendly events (e.g., puppet shows, science fairs)\n",
    "    'cultural',          # Interest in cultural events (e.g., heritage celebrations)\n",
    "    'nightlife',         # Interest in nightlife (e.g., club events, DJ nights)\n",
    "    'science',           # Interest in science (e.g., planetarium shows, science fairs)\n",
    "    'craft',             # Interest in crafts (e.g., DIY workshops, craft fairs)\n",
    "    'photography',       # Interest in photography (e.g., photo walks, exhibitions)\n",
    "    'gardening',         # Interest in gardening (e.g., plant swaps, garden tours)\n",
    "    'history',           # Interest in historical events (e.g., reenactments, heritage walks)\n",
    "    'education',         # Interest in learning (e.g., lectures, educational workshops)\n",
    "    'charity',           # Interest in charitable causes (e.g., fundraisers, volunteer events)\n",
    "]\n",
    "    genders = ['male', 'female', 'other']\n",
    "    gender_probs = [0.48, 0.48, 0.04]\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        gender = np.random.choice(genders, p=gender_probs)\n",
    "        # Power-law: more users with more interests\n",
    "        num_interests = min(max(zipf.rvs(2.0), 1), 5)\n",
    "        user_interests = random.sample(interests, k=num_interests)\n",
    "        users.append({\n",
    "            'user_id': generic.person.identifier(mask='@@###@'),\n",
    "            'gender': gender,\n",
    "            'user_lat': lat,\n",
    "            'user_lon': lon,\n",
    "            'user_city': city,\n",
    "            'indoor_outdoor_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'user_interests': ','.join(user_interests),\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now')\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=3000):\n",
    "    events = []\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    current_date = datetime(2025, 3, 27, 11, 48)\n",
    "    for _ in range(n_events):\n",
    "        cat = np.random.choice(event_category)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        if cat in ['Sports', 'Festival']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif cat in ['Education', 'Technology', 'Business', 'Conference', 'Exhibition', 'Workshop']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        base_temp = {\n",
    "            'New York': 15, 'London': 12, 'Paris': 16, 'Tokyo': 20, \n",
    "            'Sydney': 22, 'Berlin': 14, 'Mumbai': 28, 'São Paulo': 24, \n",
    "            'Toronto': 10, 'Dubai': 32\n",
    "        }[city]\n",
    "        temp_adjustment = {\n",
    "            'Clear': np.random.uniform(2, 5),\n",
    "            'Rain': np.random.uniform(-3, 0),\n",
    "            'Snow': np.random.uniform(-8, -3),\n",
    "            'Cloudy': np.random.uniform(-1, 2),\n",
    "            'Windy': np.random.uniform(-2, 1)\n",
    "        }[weather_condition]\n",
    "        temperature = round(base_temp + temp_adjustment, 1)\n",
    "        start_time = fake.date_time_between(start_date=current_date, end_date=current_date + timedelta(days=180))\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        events.append({\n",
    "            'event_id': generic.person.identifier(mask='@@###@'),\n",
    "            'title': f\"{fake.catch_phrase()} {cat} in {city}\",\n",
    "            'event_category': cat,\n",
    "            'event_lat': lat,\n",
    "            'event_lon': lon,\n",
    "            'event_city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'temperature': temperature,\n",
    "            'attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'event_indoor_capability': cat in [\n",
    "                'Education', 'Technology', 'Business', 'Conference', 'Exhibition', 'Workshop', \n",
    "                'Arts', 'Entertainment', 'Music Concerts', 'Theater', 'Cinema'\n",
    "            ]\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def calculate_time_weight(interaction_time, current_time, half_life=30):\n",
    "    time_diff = (current_time - interaction_time).days\n",
    "    return np.exp(np.log(0.5) * time_diff / half_life)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=120000):\n",
    "    interactions = []\n",
    "    interaction_types_positive = ['yes', 'invited & yes']\n",
    "    interaction_types_other = ['maybe', 'invited & maybe', 'no', 'invited & no', 'invited']\n",
    "    current_time = datetime(2025, 3, 27, 11, 48)\n",
    "    min_positive_interactions = 10\n",
    "    \n",
    "    # Assign total interactions per user (10–24, ensuring at least 10 positive)\n",
    "    user_interaction_counts = {user_id: np.random.randint(10, 25) for user_id in users['user_id']}\n",
    "    user_positive_counts = {user_id: 0 for user_id in users['user_id']}\n",
    "    \n",
    "    # Generate interactions for each user\n",
    "    for user_id, total_interactions in user_interaction_counts.items():\n",
    "        user = users[users['user_id'] == user_id].iloc[0]\n",
    "        user_interests = set(user['user_interests'].split(','))\n",
    "        \n",
    "        # Sample events, prioritizing interest matches\n",
    "        interest_events = events[events['event_category'].apply(lambda x: any(i.lower() in x.lower() for i in user_interests))]\n",
    "        sample_size = max(total_interactions, len(interest_events))\n",
    "        chosen_events = interest_events.sample(n=sample_size, replace=True) if len(interest_events) > 0 else events.sample(n=sample_size, replace=True)\n",
    "        \n",
    "        for _, event in chosen_events.head(total_interactions).iterrows():\n",
    "            distance = geodesic((user['user_lat'], user['user_lon']), (event['event_lat'], event['event_lon'])).km\n",
    "            interaction_time = fake.date_time_between(start_date=event['start_time'] - timedelta(days=30), end_date=event['start_time'])\n",
    "            interest_match = int(any(i.lower() in event['event_category'].lower() for i in user_interests))\n",
    "            \n",
    "            # Weather penalty for outdoor events in bad weather\n",
    "            weather_penalty = 0.5 if (not event['event_indoor_capability'] and event['weather_condition'] in ['Rain', 'Snow'] and user['indoor_outdoor_preference'] == 'indoor') else 0.2 if (not event['event_indoor_capability'] and event['weather_condition'] in ['Rain', 'Snow'] and user['indoor_outdoor_preference'] == 'any') else 0\n",
    "            \n",
    "            # Prioritize positive interactions until minimum is met\n",
    "            needs_positive = user_positive_counts[user_id] < min_positive_interactions\n",
    "            is_positive = needs_positive or (random.random() > 0.5 and random.random() > weather_penalty)\n",
    "            interaction_type = random.choice(interaction_types_positive) if is_positive else random.choice(interaction_types_other)\n",
    "            interaction_label = 1 if is_positive else 0\n",
    "            \n",
    "            interactions.append({\n",
    "                'interaction_id': generic.person.identifier(mask='@@###@'),\n",
    "                'user_id': user_id,\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': interaction_type,\n",
    "                'distance_to_event': distance,\n",
    "                'interaction_label': interaction_label,\n",
    "                'interest_match': interest_match\n",
    "            })\n",
    "            \n",
    "            if is_positive:\n",
    "                user_positive_counts[user_id] += 1\n",
    "    \n",
    "    # Fill remaining interactions to reach n_interactions\n",
    "    while len(interactions) < n_interactions:\n",
    "        user_id = random.choice(users['user_id'].tolist())\n",
    "        user = users[users['user_id'] == user_id].iloc[0]\n",
    "        user_interests = set(user['user_interests'].split(','))\n",
    "        event = events.sample(n=1).iloc[0]\n",
    "        \n",
    "        distance = geodesic((user['user_lat'], user['user_lon']), (event['event_lat'], event['event_lon'])).km\n",
    "        interaction_time = fake.date_time_between(start_date=event['start_time'] - timedelta(days=30), end_date=event['start_time'])\n",
    "        interest_match = int(any(i.lower() in event['event_category'].lower() for i in user_interests))\n",
    "        \n",
    "        weather_penalty = 0.5 if (not event['event_indoor_capability'] and event['weather_condition'] in ['Rain', 'Snow'] and user['indoor_outdoor_preference'] == 'indoor') else 0.2 if (not event['event_indoor_capability'] and event['weather_condition'] in ['Rain', 'Snow'] and user['indoor_outdoor_preference'] == 'any') else 0\n",
    "        is_positive = random.random() > 0.5 and random.random() > weather_penalty\n",
    "        interaction_type = random.choice(interaction_types_positive) if is_positive else random.choice(interaction_types_other)\n",
    "        interaction_label = 1 if is_positive else 0\n",
    "        \n",
    "        interactions.append({\n",
    "            'interaction_id': generic.person.identifier(mask='@@###@'),\n",
    "            'user_id': user_id,\n",
    "            'event_id': event['event_id'],\n",
    "            'interaction_type': interaction_type,\n",
    "            'distance_to_event': distance,\n",
    "            'interaction_label': interaction_label,\n",
    "            'interest_match': interest_match\n",
    "        })\n",
    "        if is_positive:\n",
    "            user_positive_counts[user_id] += 1\n",
    "    \n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "\n",
    "# Generate datasets\n",
    "users_df = generate_users(10000)\n",
    "events_df = generate_events(10000)\n",
    "interactions_df = generate_interactions(users_df, events_df, n_interactions=100000)\n",
    "\n",
    "# # Save to CSV\n",
    "# users_df.to_csv('synthetic_users.csv', index=False)\n",
    "# events_df.to_csv('synthetic_events.csv', index=False)\n",
    "# interactions_df.to_csv('synthetic_interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of events attended per user:\n",
      "618 attended 10 events\n",
      "653 attended 11 events\n",
      "672 attended 12 events\n",
      "690 attended 13 events\n",
      "663 attended 14 events\n",
      "671 attended 15 events\n",
      "628 attended 16 events\n",
      "692 attended 17 events\n",
      "619 attended 18 events\n",
      "710 attended 19 events\n",
      "626 attended 20 events\n",
      "685 attended 21 events\n",
      "696 attended 22 events\n",
      "688 attended 23 events\n",
      "688 attended 24 events\n",
      "Number of interactions for users who attended 10 or more events: 9381\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display the distribution of events attended per user\n",
    "attendance_counts = interactions_df.groupby('user_id').size()\n",
    "attendance_distribution = attendance_counts.value_counts().sort_index()\n",
    "\n",
    "print(\"Distribution of events attended per user:\")\n",
    "for count, num_users in attendance_distribution.items():\n",
    "    print(f\"{num_users} attended {count} events\")\n",
    "\n",
    "# Calculate the number of interactions for users who attended 10 or more events\n",
    "users_with_10_or_more_events = attendance_distribution[attendance_distribution.index >= 11].sum()\n",
    "print(f\"Number of interactions for users who attended 10 or more events: {users_with_10_or_more_events}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to CSV\n",
    "users_df.to_csv('synthetic_users.csv', index=False)\n",
    "events_df.to_csv('synthetic_events.csv', index=False)\n",
    "interactions_df.to_csv('synthetic_interactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170741"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interaction_label\n",
       "1    134067\n",
       "0     36674\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df['interaction_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>distance_to_event</th>\n",
       "      <th>interaction_label</th>\n",
       "      <th>interest_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DY865T</td>\n",
       "      <td>LH639X</td>\n",
       "      <td>QH593N</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>19.390390</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JK377A</td>\n",
       "      <td>LH639X</td>\n",
       "      <td>PJ960I</td>\n",
       "      <td>invited</td>\n",
       "      <td>9718.955250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IU133S</td>\n",
       "      <td>LH639X</td>\n",
       "      <td>CC961P</td>\n",
       "      <td>no</td>\n",
       "      <td>7254.504840</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HU050G</td>\n",
       "      <td>LH639X</td>\n",
       "      <td>IU130U</td>\n",
       "      <td>no</td>\n",
       "      <td>9249.921518</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BL008W</td>\n",
       "      <td>LH639X</td>\n",
       "      <td>UY375V</td>\n",
       "      <td>invited &amp; no</td>\n",
       "      <td>325.393142</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interaction_id user_id event_id interaction_type  distance_to_event  \\\n",
       "0         DY865T  LH639X   QH593N    invited & yes          19.390390   \n",
       "1         JK377A  LH639X   PJ960I          invited        9718.955250   \n",
       "2         IU133S  LH639X   CC961P               no        7254.504840   \n",
       "3         HU050G  LH639X   IU130U               no        9249.921518   \n",
       "4         BL008W  LH639X   UY375V     invited & no         325.393142   \n",
       "\n",
       "   interaction_label  interest_match  \n",
       "0                  1               1  \n",
       "1                  1               1  \n",
       "2                  1               1  \n",
       "3                  0               1  \n",
       "4                  1               1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23652"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(interactions_df[interactions_df['interest_match'] == 1]))\n",
    "len(interactions_df[interactions_df['interest_match'] == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130251"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170764\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lon</th>\n",
       "      <th>user_city</th>\n",
       "      <th>indoor_outdoor_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>user_interests</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>...</th>\n",
       "      <th>event_category</th>\n",
       "      <th>event_lat</th>\n",
       "      <th>event_lon</th>\n",
       "      <th>event_city</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>temperature</th>\n",
       "      <th>attendance_rate</th>\n",
       "      <th>event_indoor_capability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QY831O</td>\n",
       "      <td>male</td>\n",
       "      <td>48.946743</td>\n",
       "      <td>2.398599</td>\n",
       "      <td>Paris</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>40</td>\n",
       "      <td>music</td>\n",
       "      <td>2024-04-24 10:30:13.682443</td>\n",
       "      <td>RX851G</td>\n",
       "      <td>...</td>\n",
       "      <td>Music Concerts</td>\n",
       "      <td>52.610034</td>\n",
       "      <td>13.3988</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-05-27 18:04:34.907373</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.344309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZX781T</td>\n",
       "      <td>male</td>\n",
       "      <td>35.649461</td>\n",
       "      <td>139.679388</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>29</td>\n",
       "      <td>music</td>\n",
       "      <td>2024-04-14 20:27:55.801585</td>\n",
       "      <td>IJ828R</td>\n",
       "      <td>...</td>\n",
       "      <td>Music Concerts</td>\n",
       "      <td>52.610034</td>\n",
       "      <td>13.3988</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-05-27 18:04:34.907373</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.344309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VM571H</td>\n",
       "      <td>male</td>\n",
       "      <td>51.552374</td>\n",
       "      <td>-0.105399</td>\n",
       "      <td>London</td>\n",
       "      <td>indoor</td>\n",
       "      <td>42</td>\n",
       "      <td>music,festival,science,food,craft</td>\n",
       "      <td>2023-11-12 23:22:33.194089</td>\n",
       "      <td>AZ647H</td>\n",
       "      <td>...</td>\n",
       "      <td>Music Concerts</td>\n",
       "      <td>52.610034</td>\n",
       "      <td>13.3988</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-05-27 18:04:34.907373</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.344309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WZ425F</td>\n",
       "      <td>other</td>\n",
       "      <td>19.150239</td>\n",
       "      <td>72.811710</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>indoor</td>\n",
       "      <td>23</td>\n",
       "      <td>music,festival</td>\n",
       "      <td>2024-03-06 17:28:51.046101</td>\n",
       "      <td>JR744U</td>\n",
       "      <td>...</td>\n",
       "      <td>Music Concerts</td>\n",
       "      <td>52.610034</td>\n",
       "      <td>13.3988</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-05-27 18:04:34.907373</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.344309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XA863B</td>\n",
       "      <td>male</td>\n",
       "      <td>41.541536</td>\n",
       "      <td>-72.126511</td>\n",
       "      <td>New York</td>\n",
       "      <td>indoor</td>\n",
       "      <td>29</td>\n",
       "      <td>music</td>\n",
       "      <td>2024-01-07 19:41:16.405027</td>\n",
       "      <td>ON454C</td>\n",
       "      <td>...</td>\n",
       "      <td>Music Concerts</td>\n",
       "      <td>52.610034</td>\n",
       "      <td>13.3988</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-05-27 18:04:34.907373</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.344309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id gender   user_lat    user_lon user_city indoor_outdoor_preference  \\\n",
       "0  QY831O   male  48.946743    2.398599     Paris                   outdoor   \n",
       "1  ZX781T   male  35.649461  139.679388     Tokyo                   outdoor   \n",
       "2  VM571H   male  51.552374   -0.105399    London                    indoor   \n",
       "3  WZ425F  other  19.150239   72.811710    Mumbai                    indoor   \n",
       "4  XA863B   male  41.541536  -72.126511  New York                    indoor   \n",
       "\n",
       "   age                     user_interests                signup_date  \\\n",
       "0   40                              music 2024-04-24 10:30:13.682443   \n",
       "1   29                              music 2024-04-14 20:27:55.801585   \n",
       "2   42  music,festival,science,food,craft 2023-11-12 23:22:33.194089   \n",
       "3   23                     music,festival 2024-03-06 17:28:51.046101   \n",
       "4   29                              music 2024-01-07 19:41:16.405027   \n",
       "\n",
       "  interaction_id  ...  event_category  event_lat  event_lon  event_city  \\\n",
       "0         RX851G  ...  Music Concerts  52.610034    13.3988      Berlin   \n",
       "1         IJ828R  ...  Music Concerts  52.610034    13.3988      Berlin   \n",
       "2         AZ647H  ...  Music Concerts  52.610034    13.3988      Berlin   \n",
       "3         JR744U  ...  Music Concerts  52.610034    13.3988      Berlin   \n",
       "4         ON454C  ...  Music Concerts  52.610034    13.3988      Berlin   \n",
       "\n",
       "                  start_time duration weather_condition  temperature  \\\n",
       "0 2025-05-27 18:04:34.907373      180             Clear         16.2   \n",
       "1 2025-05-27 18:04:34.907373      180             Clear         16.2   \n",
       "2 2025-05-27 18:04:34.907373      180             Clear         16.2   \n",
       "3 2025-05-27 18:04:34.907373      180             Clear         16.2   \n",
       "4 2025-05-27 18:04:34.907373      180             Clear         16.2   \n",
       "\n",
       "   attendance_rate event_indoor_capability  \n",
       "0        33.344309                    True  \n",
       "1        33.344309                    True  \n",
       "2        33.344309                    True  \n",
       "3        33.344309                    True  \n",
       "4        33.344309                    True  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = users_df.merge(interactions_df, on=\"user_id\",how=\"inner\")\\\n",
    "    .merge(events_df,on=\"event_id\",how=\"inner\")\n",
    "\n",
    "print(len(merged_df))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36680"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df[merged_df[\"interaction_label\"]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"test_merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 00:49:09,330 INFO: Use pytorch device_name: cpu\n",
      "2025-05-02 00:49:09,344 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b469124d5d52419798c9c9e784754446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 00:49:15,034 INFO: Use pytorch device_name: cpu\n",
      "2025-05-02 00:49:15,036 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(combined)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 5. Two-Tower Model\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTwoTowerModel\u001b[39;00m(\u001b[43mtfrs\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfrs' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.layers import TextVectorization, Normalization\n",
    "\n",
    "# 1. Initialize Sentence Transformers\n",
    "user_text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "event_text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Feature Processing Layers\n",
    "geo_normalizer = Normalization(axis=-1)\n",
    "temp_normalizer = Normalization(axis=-1)\n",
    "age_normalizer = Normalization(axis=-1)\n",
    "\n",
    "# 3. User Tower\n",
    "class UserTower(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_encoder = tf.keras.layers.Lambda(\n",
    "            lambda x: user_text_encoder.encode(x.numpy().astype(str), convert_to_tensor=True)\n",
    "        )\n",
    "        self.geo_norm = geo_normalizer\n",
    "        self.age_norm = age_normalizer\n",
    "        self.dense = tf.keras.layers.Dense(256)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Process interests\n",
    "        interest_emb = self.text_encoder(inputs[\"user_interests\"])\n",
    "        \n",
    "        # Process demographics\n",
    "        geo = self.geo_norm(tf.stack([inputs[\"user_lat\"], inputs[\"user_lon\"]], axis=1))\n",
    "        age = self.age_norm(tf.reshape(inputs[\"age\"], (-1, 1)))\n",
    "        \n",
    "        # Combine features\n",
    "        combined = tf.concat([\n",
    "            tf.cast(interest_emb, tf.float32),\n",
    "            geo,\n",
    "            age\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self.dense(combined)\n",
    "\n",
    "# 4. Event Tower  \n",
    "class EventTower(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_encoder = tf.keras.layers.Lambda(\n",
    "            lambda x: event_text_encoder.encode(x.numpy().astype(str), convert_to_tensor=True)\n",
    "        )\n",
    "        self.geo_norm = geo_normalizer\n",
    "        self.temp_norm = temp_normalizer\n",
    "        self.dense = tf.keras.layers.Dense(256)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Process event context\n",
    "        context = inputs[\"event_title\"] + \" \" + inputs[\"event_category\"]\n",
    "        context_emb = self.text_encoder(context)\n",
    "        \n",
    "        # Process environmental factors\n",
    "        geo = self.geo_norm(tf.stack([inputs[\"event_lat\"], inputs[\"event_lon\"]], axis=1))\n",
    "        temp = self.temp_norm(tf.reshape(inputs[\"temperature\"], (-1, 1)))\n",
    "        \n",
    "        # Combine features\n",
    "        combined = tf.concat([\n",
    "            tf.cast(context_emb, tf.float32),\n",
    "            geo,\n",
    "            temp\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self.dense(combined)\n",
    "\n",
    "# 5. Two-Tower Model\n",
    "class TwoTowerModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_tower = UserTower()\n",
    "        self.event_tower = EventTower()\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=events.batch(128).map(self.event_tower)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            user_embeddings = self.user_tower({\n",
    "                \"user_interests\": features[\"user_interests\"],\n",
    "                \"user_lat\": features[\"user_lat\"],\n",
    "                \"user_lon\": features[\"user_lon\"],\n",
    "                \"age\": features[\"age\"]\n",
    "            })\n",
    "            \n",
    "            event_embeddings = self.event_tower({\n",
    "                \"event_title\": features[\"event_title\"],\n",
    "                \"event_category\": features[\"event_category\"],\n",
    "                \"event_lat\": features[\"event_lat\"],\n",
    "                \"event_lon\": features[\"event_lon\"],\n",
    "                \"temperature\": features[\"temperature\"]\n",
    "            })\n",
    "\n",
    "            loss = self.task(user_embeddings, event_embeddings)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "# 6. Data Preparation\n",
    "def prepare_dataset(interactions_df):\n",
    "    return tf.data.Dataset.from_tensor_slices({\n",
    "        \"user_interests\": interactions_df['user_interests'],\n",
    "        \"user_lat\": interactions_df['user_lat'],\n",
    "        \"user_lon\": interactions_df['user_lon'],\n",
    "        \"age\": interactions_df['age'],\n",
    "        \"event_title\": interactions_df['event_title'],\n",
    "        \"event_category\": interactions_df['event_category'],\n",
    "        \"event_lat\": interactions_df['event_lat'],\n",
    "        \"event_lon\": interactions_df['event_lon'],\n",
    "        \"temperature\": interactions_df['temperature']\n",
    "    }).batch(512).cache()\n",
    "\n",
    "# 7. Training\n",
    "def train_model(users, events, interactions):\n",
    "    # Prepare data\n",
    "    train_ds = prepare_dataset(interactions)\n",
    "    event_ds = prepare_dataset(events)\n",
    "    \n",
    "    # Initialize towers\n",
    "    model = TwoTowerModel()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    \n",
    "    # Adapt normalizers\n",
    "    geo_data = tf.concat([users[['user_lat', 'user_lon']], events[['event_lat', 'event_lon']]], axis=0)\n",
    "    model.user_tower.geo_norm.adapt(geo_data)\n",
    "    model.event_tower.geo_norm.adapt(geo_data)\n",
    "    \n",
    "    model.user_tower.age_norm.adapt(users['age'])\n",
    "    model.event_tower.temp_norm.adapt(events['temperature'])\n",
    "    \n",
    "    # Fine-tune\n",
    "    model.fit(train_ds, epochs=10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 8. Usage\n",
    "model = train_model(users_df, events_df, interactions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_recommenders as tfrs\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# Load the generated datasets\n",
    "users_df = pd.read_csv('synthetic_users.csv')\n",
    "events_df = pd.read_csv('synthetic_events.csv')\n",
    "interactions_df = pd.read_csv('synthetic_interactions.csv')\n",
    "\n",
    "print(f\"Loaded {len(users_df)} users, {len(events_df)} events, and {len(interactions_df)} interactions\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "users_df['signup_date'] = pd.to_datetime(users_df['signup_date'])\n",
    "events_df['start_time'] = pd.to_datetime(events_df['start_time'])\n",
    "interactions_df['interaction_time'] = pd.to_datetime(interactions_df['interaction_time'])\n",
    "\n",
    "# Data overview\n",
    "print(\"\\nUsers data sample:\")\n",
    "print(users_df.head())\n",
    "print(\"\\nEvents data sample:\")\n",
    "print(events_df.head())\n",
    "print(\"\\nInteractions data sample:\")\n",
    "print(interactions_df.head())\n",
    "\n",
    "# Let's analyze the data to understand it better\n",
    "def analyze_data():\n",
    "    print(\"\\n---- Data Analysis ----\")\n",
    "    \n",
    "    # User weather preferences distribution\n",
    "    print(\"\\nUser weather preferences:\")\n",
    "    print(users_df['user_weather_preference'].value_counts(normalize=True))\n",
    "    \n",
    "    # Event categories distribution\n",
    "    print(\"\\nEvent categories:\")\n",
    "    print(events_df['event_category'].value_counts())\n",
    "    \n",
    "    # Weather conditions distribution\n",
    "    print(\"\\nWeather conditions:\")\n",
    "    print(events_df['weather_condition'].value_counts())\n",
    "    \n",
    "    # Interaction types distribution\n",
    "    print(\"\\nInteraction types:\")\n",
    "    print(interactions_df['interaction_type'].value_counts())\n",
    "    \n",
    "    # Interaction label distribution\n",
    "    print(\"\\nInteraction labels:\")\n",
    "    print(interactions_df['interaction_label'].value_counts(normalize=True))\n",
    "    \n",
    "    # Plot the distribution of ages\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(users_df['age'], bins=20)\n",
    "    plt.title('Distribution of User Ages')\n",
    "    plt.savefig('user_age_distribution.png')\n",
    "    \n",
    "    # Plot the distribution of temperatures at events\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(events_df['temperature'], bins=20)\n",
    "    plt.title('Distribution of Event Temperatures')\n",
    "    plt.savefig('event_temperature_distribution.png')\n",
    "    \n",
    "    # Plot interaction distances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(interactions_df['interaction_distance_to_event'].clip(0, 500), bins=50)\n",
    "    plt.title('Distribution of Interaction Distances (km, capped at 500km)')\n",
    "    plt.savefig('interaction_distance_distribution.png')\n",
    "\n",
    "analyze_data()\n",
    "\n",
    "# Feature Engineering for Weather-Based Recommendation\n",
    "\n",
    "def engineer_features():\n",
    "    print(\"\\n---- Feature Engineering ----\")\n",
    "    \n",
    "    # Merge interactions with user and event data\n",
    "    merged_df = interactions_df.merge(users_df, on='user_id', how='left')\n",
    "    merged_df = merged_df.merge(events_df, on='event_id', how='left')\n",
    "    \n",
    "    # Create weather preference match feature\n",
    "    def weather_preference_match(row):\n",
    "        if row['user_weather_preference'] == 'any':\n",
    "            return 1.0\n",
    "        elif row['user_weather_preference'] == 'indoor' and row['event_indoor_capability']:\n",
    "            return 1.0\n",
    "        elif row['user_weather_preference'] == 'outdoor' and not row['event_indoor_capability']:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.5\n",
    "    \n",
    "    merged_df['weather_preference_match'] = merged_df.apply(weather_preference_match, axis=1)\n",
    "    \n",
    "    # Create temperature comfort feature (closer to preferred temperature = higher score)\n",
    "    # Assume comfort zone is between 18-24°C\n",
    "    def temp_comfort_score(temp):\n",
    "        if 18 <= temp <= 24:\n",
    "            return 1.0\n",
    "        elif 15 <= temp < 18 or 24 < temp <= 27:\n",
    "            return 0.8\n",
    "        elif 10 <= temp < 15 or 27 < temp <= 30:\n",
    "            return 0.6\n",
    "        elif 5 <= temp < 10 or 30 < temp <= 35:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return 0.2\n",
    "    \n",
    "    merged_df['temperature_comfort'] = merged_df['temperature'].apply(temp_comfort_score)\n",
    "    \n",
    "    # Create weather condition score\n",
    "    weather_scores = {\n",
    "        'Clear': 1.0,\n",
    "        'Cloudy': 0.8,\n",
    "        'Windy': 0.6,\n",
    "        'Rain': 0.4,\n",
    "        'Snow': 0.2\n",
    "    }\n",
    "    merged_df['weather_condition_score'] = merged_df['weather_condition'].map(weather_scores)\n",
    "    \n",
    "    # Create recency feature\n",
    "    current_time = datetime.datetime(2025, 3, 27, 11, 48)\n",
    "    merged_df['days_since_interaction'] = (current_time - merged_df['interaction_time']).dt.total_seconds() / (24 * 3600)\n",
    "    merged_df['interaction_recency'] = np.exp(-0.05 * merged_df['days_since_interaction'])  # Exponential decay\n",
    "    \n",
    "    # Create time-to-event feature\n",
    "    merged_df['days_to_event'] = (merged_df['start_time'] - merged_df['interaction_time']).dt.total_seconds() / (24 * 3600)\n",
    "    merged_df['days_to_event_scaled'] = np.clip(merged_df['days_to_event'] / 30, 0, 1)  # Normalize to [0,1]\n",
    "    \n",
    "    # Create weekend flag\n",
    "    merged_df['is_weekend'] = merged_df['start_time'].dt.dayofweek >= 5\n",
    "    \n",
    "    # Create distance feature (exponential decay based on distance)\n",
    "    merged_df['distance_score'] = np.exp(-0.01 * merged_df['interaction_distance_to_event'])\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    categorical_cols = ['gender', 'user_city', 'user_weather_preference', \n",
    "                        'event_category', 'event_city', 'weather_condition',\n",
    "                        'interaction_type']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        dummies = pd.get_dummies(merged_df[col], prefix=col)\n",
    "        merged_df = pd.concat([merged_df, dummies], axis=1)\n",
    "    \n",
    "    # Extract user interests as features\n",
    "    all_interests = set()\n",
    "    for interests in users_df['user_interests'].str.split(','):\n",
    "        all_interests.update(interests)\n",
    "    \n",
    "    for interest in all_interests:\n",
    "        merged_df[f'interest_{interest}'] = merged_df['user_interests'].str.contains(interest).astype(int)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = ['age', 'social_connectedness', 'temperature', 'attendance_rate',\n",
    "                      'interaction_distance_to_event', 'days_to_event']\n",
    "    \n",
    "    merged_df[numerical_cols] = scaler.fit_transform(merged_df[numerical_cols])\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "merged_df = engineer_features()\n",
    "print(\"\\nFeature-engineered data sample:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# PART 1: RETRIEVAL MODEL USING TENSORFLOW RECOMMENDERS\n",
    "\n",
    "def build_retrieval_model():\n",
    "    print(\"\\n---- Building Retrieval Model ----\")\n",
    "    \n",
    "    # Prepare data for TF-Recommenders\n",
    "    # Convert to TensorFlow datasets\n",
    "    user_ids = tf.constant(train_df['user_id'].unique())\n",
    "    event_ids = tf.constant(events_df['event_id'].unique())\n",
    "    \n",
    "    # Create mappings\n",
    "    user_to_features = {}\n",
    "    for _, user in tqdm(users_df.iterrows(), total=len(users_df), desc=\"Processing users\"):\n",
    "        user_id = user['user_id']\n",
    "        user_to_features[user_id] = {\n",
    "            'gender': user['gender'],\n",
    "            'city': user['user_city'],\n",
    "            'weather_preference': user['user_weather_preference'],\n",
    "            'age': float(user['age']),\n",
    "            'social_connectedness': float(user['social_connectedness']),\n",
    "            'interests': user['user_interests']\n",
    "        }\n",
    "    \n",
    "    event_to_features = {}\n",
    "    for _, event in tqdm(events_df.iterrows(), total=len(events_df), desc=\"Processing events\"):\n",
    "        event_id = event['event_id']\n",
    "        event_to_features[event_id] = {\n",
    "            'category': event['event_category'],\n",
    "            'city': event['event_city'],\n",
    "            'weather': event['weather_condition'],\n",
    "            'temperature': float(event['temperature']),\n",
    "            'indoor_capability': bool(event['event_indoor_capability']),\n",
    "            'is_weekend': bool(event['start_time'].dayofweek >= 5)\n",
    "        }\n",
    "    \n",
    "    # Create training data\n",
    "    train_interactions = tf.data.Dataset.from_tensor_slices({\n",
    "        'user_id': tf.constant(train_df['user_id'].values),\n",
    "        'event_id': tf.constant(train_df['event_id'].values),\n",
    "        'label': tf.constant(train_df['interaction_label'].values, dtype=tf.float32)\n",
    "    })\n",
    "    \n",
    "    # Create test data\n",
    "    test_interactions = tf.data.Dataset.from_tensor_slices({\n",
    "        'user_id': tf.constant(test_df['user_id'].values),\n",
    "        'event_id': tf.constant(test_df['event_id'].values),\n",
    "        'label': tf.constant(test_df['interaction_label'].values, dtype=tf.float32)\n",
    "    })\n",
    "    \n",
    "    # Define vocabulary sizes\n",
    "    user_vocab_size = len(user_ids)\n",
    "    event_vocab_size = len(event_ids)\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Define model architecture\n",
    "    class WeatherEventRetrievalModel(tfrs.Model):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            # User embeddings\n",
    "            self.user_embedding_layer = tf.keras.Sequential([\n",
    "                tf.keras.layers.StringLookup(vocabulary=user_ids, mask_token=None),\n",
    "                tf.keras.layers.Embedding(user_vocab_size + 1, embedding_dim)\n",
    "            ])\n",
    "            \n",
    "            # Event embeddings\n",
    "            self.event_embedding_layer = tf.keras.Sequential([\n",
    "                tf.keras.layers.StringLookup(vocabulary=event_ids, mask_token=None),\n",
    "                tf.keras.layers.Embedding(event_vocab_size + 1, embedding_dim)\n",
    "            ])\n",
    "            \n",
    "            # Task: retrieval task that optimizes for user-event matches\n",
    "            self.task = tfrs.tasks.Retrieval(\n",
    "                metrics=tfrs.metrics.FactorizedTopK(\n",
    "                    candidates=tf.data.Dataset.from_tensor_slices(event_ids).map(\n",
    "                        lambda event_id: (event_id, self.event_embedding_layer(event_id))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        def compute_loss(self, features, training=False):\n",
    "            user_embeddings = self.user_embedding_layer(features[\"user_id\"])\n",
    "            event_embeddings = self.event_embedding_layer(features[\"event_id\"])\n",
    "            \n",
    "            return self.task(user_embeddings, event_embeddings, compute_metrics=not training)\n",
    "        \n",
    "        def call(self, features):\n",
    "            return {\n",
    "                \"user_embeddings\": self.user_embedding_layer(features[\"user_id\"]),\n",
    "                \"event_embeddings\": self.event_embedding_layer(features[\"event_id\"])\n",
    "            }\n",
    "    \n",
    "    # Build and compile model\n",
    "    retrieval_model = WeatherEventRetrievalModel()\n",
    "    retrieval_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "    \n",
    "    # Shuffle and batch the data\n",
    "    cached_train = train_interactions.shuffle(10000).batch(8192).cache()\n",
    "    cached_test = test_interactions.batch(4096).cache()\n",
    "    \n",
    "    # Train the model\n",
    "    history = retrieval_model.fit(cached_train, epochs=5, validation_data=cached_test)\n",
    "    \n",
    "    # Create the retrieval index for serving\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(retrieval_model.user_embedding_layer)\n",
    "    index.index_from_dataset(\n",
    "        tf.data.Dataset.from_tensor_slices((event_ids)).map(lambda event_id: (\n",
    "            event_id, \n",
    "            retrieval_model.event_embedding_layer(event_id)\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    tf.saved_model.save(index, \"weather_event_retrieval_model\")\n",
    "    \n",
    "    return index, retrieval_model, history\n",
    "\n",
    "retrieval_index, retrieval_model, retrieval_history = build_retrieval_model()\n",
    "\n",
    "# PART 2: RANKING MODEL USING CATBOOST\n",
    "\n",
    "def build_ranking_model():\n",
    "    print(\"\\n---- Building Ranking Model ----\")\n",
    "    \n",
    "    # Prepare features for CatBoost\n",
    "    # Select features for ranking\n",
    "    feature_cols = [\n",
    "        # User features\n",
    "        'age', 'social_connectedness',\n",
    "        # Event features\n",
    "        'temperature', 'attendance_rate', 'event_indoor_capability',\n",
    "        # Interaction features\n",
    "        'interaction_distance_to_event', 'days_to_event',\n",
    "        # Engineered features\n",
    "        'weather_preference_match', 'temperature_comfort', 'weather_condition_score',\n",
    "        'interaction_recency', 'distance_score', 'is_weekend',\n",
    "        'interest_match'\n",
    "    ]\n",
    "    \n",
    "    # Add one-hot encoded columns\n",
    "    for col in merged_df.columns:\n",
    "        if col.startswith(('gender_', 'user_city_', 'user_weather_preference_', \n",
    "                           'event_category_', 'event_city_', 'weather_condition_',\n",
    "                           'interaction_type_', 'interest_')):\n",
    "            feature_cols.append(col)\n",
    "    \n",
    "    # Define categorical features for CatBoost\n",
    "    categorical_feature_indices = []\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col.startswith(('gender_', 'user_city_', 'user_weather_preference_', \n",
    "                          'event_category_', 'event_city_', 'weather_condition_',\n",
    "                          'interaction_type_', 'interest_', 'is_weekend', \n",
    "                          'event_indoor_capability')):\n",
    "            categorical_feature_indices.append(i)\n",
    "    \n",
    "    # Prepare train and test sets\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['interaction_label']\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df['interaction_label']\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Configure and train CatBoost model\n",
    "    catboost_params = {\n",
    "        'iterations': 500,\n",
    "        'learning_rate': 0.03,\n",
    "        'depth': 6,\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 100,\n",
    "        'cat_features': categorical_feature_indices\n",
    "    }\n",
    "    \n",
    "    ranker = CatBoostClassifier(**catboost_params)\n",
    "    ranker.fit(X_train, y_train, eval_set=(X_test, y_test), plot=True)\n",
    "    \n",
    "    # Evaluate the ranking model\n",
    "    y_pred_proba = ranker.predict_proba(X_test)[:, 1]\n",
    "    y_pred = ranker.predict(X_test)\n",
    "    \n",
    "    print(\"\\nRanking Model Evaluation:\")\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': ranker.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
    "    plt.title('Top 20 Features by Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    \n",
    "    # Save the model\n",
    "    ranker.save_model('weather_event_ranking_model.cbm')\n",
    "    \n",
    "    return ranker, feature_importance\n",
    "\n",
    "ranker, feature_importance = build_ranking_model()\n",
    "\n",
    "# PART 3: FULL RECOMMENDATION PIPELINE\n",
    "\n",
    "def create_recommendation_pipeline():\n",
    "    print(\"\\n---- Creating Recommendation Pipeline ----\")\n",
    "    \n",
    "    def recommend_events_for_user(user_id, top_k=20, final_k=10):\n",
    "        \"\"\"\n",
    "        End-to-end recommendation function using both retrieval and ranking models\n",
    "        \n",
    "        Args:\n",
    "            user_id: The ID of the user to recommend events for\n",
    "            top_k: Number of candidates to retrieve from retrieval model\n",
    "            final_k: Final number of recommendations after ranking\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with ranked event recommendations\n",
    "        \"\"\"\n",
    "        # Step 1: Get user data\n",
    "        user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "        \n",
    "        # Step 2: Retrieve candidate events using the retrieval model\n",
    "        _, candidates = retrieval_index(tf.constant([user_id]), k=top_k)\n",
    "        candidate_event_ids = [event_id.numpy().decode('utf-8') for event_id in candidates[0]]\n",
    "        \n",
    "        # Get candidate events data\n",
    "        candidate_events = events_df[events_df['event_id'].isin(candidate_event_ids)]\n",
    "        \n",
    "        # Step 3: Prepare features for each user-event pair for ranking\n",
    "        ranking_features = []\n",
    "        \n",
    "        for _, event in candidate_events.iterrows():\n",
    "            # Create a synthetic interaction\n",
    "            synthetic_interaction = {\n",
    "                'user_id': user_id,\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_time': datetime.datetime(2025, 3, 27, 11, 48),  # Current time\n",
    "                'interaction_distance_to_event': geodesic(\n",
    "                    (user['user_lat'], user['user_lon']),\n",
    "                    (event['event_lat'], event['event_lon'])\n",
    "                ).km\n",
    "            }\n",
    "            \n",
    "            # Create a DataFrame with this single interaction\n",
    "            interaction_df = pd.DataFrame([synthetic_interaction])\n",
    "            \n",
    "            # Merge with user and event data\n",
    "            merged = pd.DataFrame([synthetic_interaction]).merge(\n",
    "                pd.DataFrame([user]), on='user_id', how='left'\n",
    "            ).merge(\n",
    "                pd.DataFrame([event]), on='event_id', how='left'\n",
    "            )\n",
    "            \n",
    "            # Apply the same feature engineering as in training\n",
    "            # Weather preference match\n",
    "            if user['user_weather_preference'] == 'any':\n",
    "                merged['weather_preference_match'] = 1.0\n",
    "            elif user['user_weather_preference'] == 'indoor' and event['event_indoor_capability']:\n",
    "                merged['weather_preference_match'] = 1.0\n",
    "            elif user['user_weather_preference'] == 'outdoor' and not event['event_indoor_capability']:\n",
    "                merged['weather_preference_match'] = 1.0\n",
    "            else:\n",
    "                merged['weather_preference_match'] = 0.5\n",
    "            \n",
    "            # Temperature comfort\n",
    "            def temp_comfort_score(temp):\n",
    "                if 18 <= temp <= 24:\n",
    "                    return 1.0\n",
    "                elif 15 <= temp < 18 or 24 < temp <= 27:\n",
    "                    return 0.8\n",
    "                elif 10 <= temp < 15 or 27 < temp <= 30:\n",
    "                    return 0.6\n",
    "                elif 5 <= temp < 10 or 30 < temp <= 35:\n",
    "                    return 0.4\n",
    "                else:\n",
    "                    return 0.2\n",
    "            \n",
    "            merged['temperature_comfort'] = temp_comfort_score(event['temperature'])\n",
    "            \n",
    "            # Weather condition score\n",
    "            weather_scores = {\n",
    "                'Clear': 1.0,\n",
    "                'Cloudy': 0.8,\n",
    "                'Windy': 0.6,\n",
    "                'Rain': 0.4,\n",
    "                'Snow': 0.2\n",
    "            }\n",
    "            merged['weather_condition_score'] = weather_scores.get(event['weather_condition'], 0.5)\n",
    "            \n",
    "            # Distance score\n",
    "            merged['distance_score'] = np.exp(-0.01 * merged['interaction_distance_to_event'])\n",
    "            \n",
    "            # Time features\n",
    "            current_time = datetime.datetime(2025, 3, 27, 11, 48)\n",
    "            merged['days_to_event'] = (merged['start_time'] - current_time).dt.total_seconds() / (24 * 3600)\n",
    "            merged['interaction_recency'] = 1.0  # This is a new interaction\n",
    "            merged['is_weekend'] = merged['start_time'].dt.dayofweek >= 5\n",
    "            \n",
    "            # Interest match\n",
    "            user_interests = set(user['user_interests'].split(','))\n",
    "            merged['interest_match'] = int(any(i.lower() in event['event_category'].lower() for i in user_interests))\n",
    "            \n",
    "            # One-hot encoding of categorical variables\n",
    "            for col, col_value in [\n",
    "                ('gender', user['gender']),\n",
    "                ('user_city', user['user_city']),\n",
    "                ('user_weather_preference', user['user_weather_preference']),\n",
    "                ('event_category', event['event_category']),\n",
    "                ('event_city', event['event_city']),\n",
    "                ('weather_condition', event['weather_condition']),\n",
    "                ('interaction_type', 'maybe')  # Default value for new interactions\n",
    "            ]:\n",
    "                for unique_val in merged_df[col].unique():\n",
    "                    merged[f'{col}_{unique_val}'] = 1 if col_value == unique_val else 0\n",
    "            \n",
    "            # Interest features\n",
    "            for interest in all_interests:\n",
    "                merged[f'interest_{interest}'] = 1 if interest in user_interests else 0\n",
    "            \n",
    "            ranking_features.append(merged)\n",
    "        \n",
    "        if not ranking_features:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        ranking_df = pd.concat(ranking_features, ignore_index=True)\n",
    "        \n",
    "        # Use the same feature columns as in training\n",
    "        feature_cols = [col for col in ranker.feature_names_]\n",
    "        for col in feature_cols:\n",
    "            if col not in ranking_df.columns:\n",
    "                ranking_df[col] = 0  # Fill missing columns with default value\n",
    "        \n",
    "        # Step 4: Predict scores using the ranking model\n",
    "        ranking_scores = ranker.predict_proba(ranking_df[feature_cols])[:, 1]\n",
    "        \n",
    "        # Step 5: Sort and return recommendations\n",
    "        recommendations = candidate_events.copy()\n",
    "        recommendations['score'] = ranking_scores\n",
    "        recommendations = recommendations.sort_values('score', ascending=False).head(final_k)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    # Test the recommendation pipeline\n",
    "    test_user_id = users_df['user_id'].iloc[0]\n",
    "    recommendations = recommend_events_for_user(test_user_id)\n",
    "    \n",
    "    print(f\"\\nTop recommendations for user {test_user_id}:\")\n",
    "    print(recommendations[['title', 'event_category', 'event_city', 'weather_condition', 'temperature', 'score']])\n",
    "    \n",
    "    return recommend_events_for_user\n",
    "\n",
    "recommendation_function = create_recommendation_pipeline()\n",
    "\n",
    "# Function to explain recommendations based on weather and other factors\n",
    "def explain_recommendations(user_id, recommendations):\n",
    "    user = users_df[users_df['user_id'] == user_id].iloc[0]\n",
    "    \n",
    "    print(f\"\\nExplanation of recommendations for user {user_id}:\")\n",
    "    print(f\"User Profile: {user['gender']}, {user['age']} years old, from {user['user_city']}\")\n",
    "    print(f\"Weather Preference: {user['user_weather_preference']}\")\n",
    "    print(f\"Interests: {user['user_interests']}\")\n",
    "    \n",
    "    for i, (_, rec) in enumerate(recommendations.iterrows(), 1):\n",
    "        print(f\"\\n{i}. {rec['title']}\")\n",
    "        print(f\"   Category: {rec['event_category']}\")\n",
    "        print(f\"   Weather: {rec['weather_condition']} at {rec['temperature']}°C\")\n",
    "        print(f\"   Location: {rec['event_city']}\")\n",
    "        \n",
    "        # Explain why this was recommended\n",
    "        reasons = []\n",
    "        \n",
    "        # Check if location matches\n",
    "        if rec['event_city'] == user['user_city']:\n",
    "            reasons.append(\"📍 This event is in your city\")\n",
    "        \n",
    "        # Check if category matches interests\n",
    "        user_interests = user['user_interests'].split(',')\n",
    "        for interest in user_interests:\n",
    "            if interest.lower() in rec['event_category'].lower():\n",
    "                reasons.append(f\"🎯 Matches your interest in {interest}\")\n",
    "                break\n",
    "        \n",
    "        # Check weather preference match\n",
    "        if user['user_weather_preference'] == 'indoor' and rec['event_indoor_capability']:\n",
    "            reasons.append(\"🏠 This is an indoor event matching your preference\")\n",
    "        elif user['user_weather_preference'] == 'outdoor' and not rec['event_indoor_capability']:\n",
    "            reasons.append(\"🌳 This is an outdoor event matching your preference\")\n",
    "        elif rec['weather_condition'] == 'Clear':\n",
    "            reasons.append(\"☀️ The weather is expected to be clear\")\n",
    "        \n",
    "        # Check temperature\n",
    "        if 18 <= rec['temperature'] <= 24:\n",
    "            reasons.append(\"🌡️ The temperature is in the comfortable range\")\n",
    "        \n",
    "        print(\"   Why we recommended this:\", \" \".join(reasons))\n",
    "\n",
    "# Test explanation with a sample user\n",
    "test_user_id = users_df['user_id'].iloc[0]\n",
    "recommendations = recommend_events_for_user(test_user_id)\n",
    "explain_recommendations(test_user_id, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
