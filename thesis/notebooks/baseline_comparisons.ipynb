{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and comparing models...\n",
      "Training Content-Based model...\n",
      "Training SVD model...\n",
      "Training Hybrid (SVD + Content + Weather) model...\n",
      "\n",
      "Validation Results:\n",
      "                    AUC       MAP  MRR  Precision@5  Recall@5    NDCG@5  \\\n",
      "Content-Based  0.703533  0.670664  1.0          0.6  0.000197  0.654809   \n",
      "SVD            0.618879  0.574715  1.0          0.8  0.000263  0.868795   \n",
      "Hybrid         0.591918  0.572668  1.0          1.0  0.000329  1.000000   \n",
      "\n",
      "               Precision@10  Recall@10   NDCG@10  \n",
      "Content-Based           0.8   0.000526  0.775994  \n",
      "SVD                     0.8   0.000526  0.851236  \n",
      "Hybrid                  1.0   0.000657  1.000000  \n",
      "\n",
      "Evaluating models on real-world test data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- svd_score\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 576\u001b[0m\n\u001b[1;32m    574\u001b[0m content_test_metrics \u001b[38;5;241m=\u001b[39m evaluate_on_real_world_data(test_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    575\u001b[0m svd_test_metrics \u001b[38;5;241m=\u001b[39m evaluate_on_real_world_data(test_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 576\u001b[0m hybrid_test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_on_real_world_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhybrid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to display the results\u001b[39;00m\n\u001b[1;32m    579\u001b[0m test_results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Based\u001b[39m\u001b[38;5;124m\"\u001b[39m: content_test_metrics,\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m: svd_test_metrics,\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m: hybrid_test_metrics,\n\u001b[1;32m    583\u001b[0m })\n",
      "Cell \u001b[0;32mIn[23], line 475\u001b[0m, in \u001b[0;36mevaluate_on_real_world_data\u001b[0;34m(test_df, model_type)\u001b[0m\n\u001b[1;32m    472\u001b[0m overall_metrics \u001b[38;5;241m=\u001b[39m compute_all_metrics(y_true, y_score)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# Calculate per-user metrics\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m user_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_per_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_interests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverall_metrics, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39muser_metrics}\n",
      "Cell \u001b[0;32mIn[23], line 99\u001b[0m, in \u001b[0;36mevaluate_per_user\u001b[0;34m(df, model, tfidf_title, tfidf_interests, scaler, encoder, k)\u001b[0m\n\u001b[1;32m     97\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteraction_distance_to_event\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattendance_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     98\u001b[0m user_df[numeric_cols] \u001b[38;5;241m=\u001b[39m user_df[numeric_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m X_numeric \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Categorical features\u001b[39;00m\n\u001b[1;32m    102\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather_condition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_indoor_capability\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_weather_preference\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1003\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1005\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1006\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    517\u001b[0m ):\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/sklearn/base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- svd_score\n"
     ]
    }
   ],
   "source": [
    "# Enhanced benchmarking evaluation and training pipeline with complete metrics and weather features included\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create directory for saved models if it doesn't exist\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# --- Shared Evaluation ---\n",
    "def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "    # Sort indices by descending score\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    # Get top-k true labels\n",
    "    top_k = np.array(y_true)[sorted_indices][:k]\n",
    "    # Calculate precision\n",
    "    precision = np.mean(top_k)\n",
    "    # Calculate recall\n",
    "    recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "    # Calculate DCG\n",
    "    dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "    # Calculate ideal DCG\n",
    "    ideal_k = min(int(np.sum(y_true)), k)\n",
    "    idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "    # Calculate NDCG\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_score, k=10):\n",
    "    # Sort indices by descending score\n",
    "    sorted_indices = np.argsort(y_score)[::-1][:k]\n",
    "    top_k = np.array(y_true)[sorted_indices]\n",
    "    ranks = np.where(top_k == 1)[0]\n",
    "    if len(ranks) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 / (ranks[0] + 1)\n",
    "\n",
    "def compute_all_metrics(y_true, y_score):\n",
    "    # Ensure there are enough positive examples for AUC calculation\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        metrics = {\n",
    "            \"AUC\": np.nan,\n",
    "            \"MAP\": np.nan\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            \"AUC\": roc_auc_score(y_true, y_score),\n",
    "            \"MAP\": average_precision_score(y_true, y_score)\n",
    "        }\n",
    "    \n",
    "    # Add MRR metric\n",
    "    metrics[\"MRR\"] = mean_reciprocal_rank(y_true, y_score)\n",
    "    \n",
    "    for k in [5, 10]:\n",
    "        p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "        metrics[f\"Precision@{k}\"] = p\n",
    "        metrics[f\"Recall@{k}\"] = r\n",
    "        metrics[f\"NDCG@{k}\"] = n\n",
    "    return metrics\n",
    "\n",
    "def evaluate_per_user(df, model, tfidf_title, tfidf_interests, scaler, encoder, k=10):\n",
    "    \"\"\"Evaluate model performance per user and average the metrics\"\"\"\n",
    "    results = {\n",
    "        f\"Precision@{k}\": [], \n",
    "        f\"Recall@{k}\": [], \n",
    "        f\"NDCG@{k}\": [], \n",
    "        f\"MRR@{k}\": []\n",
    "    }\n",
    "    \n",
    "    users = df[\"user_id\"].unique()\n",
    "    for user in users:\n",
    "        user_df = df[df[\"user_id\"] == user]\n",
    "        if len(user_df) < 2 or user_df[\"interaction_label\"].sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Prepare features\n",
    "        user_df = user_df.copy()\n",
    "        user_df[\"title\"] = user_df[\"title\"].fillna(\"\")\n",
    "        user_df[\"user_interests\"] = user_df[\"user_interests\"].fillna(\"\")\n",
    "        \n",
    "        # TF-IDF features\n",
    "        tfidf_title_mat = tfidf_title.transform(user_df[\"title\"])\n",
    "        tfidf_interests_mat = tfidf_interests.transform(user_df[\"user_interests\"])\n",
    "        \n",
    "        # Numeric features\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "        user_df[numeric_cols] = user_df[numeric_cols].fillna(0)\n",
    "        X_numeric = scaler.transform(user_df[numeric_cols])\n",
    "        \n",
    "        # Categorical features\n",
    "        categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "        for col in categorical_cols:\n",
    "            user_df[col] = user_df[col].fillna('unknown')\n",
    "        X_cat = encoder.transform(user_df[categorical_cols])\n",
    "        \n",
    "        # Combine features\n",
    "        X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric, X_cat]).toarray()\n",
    "        \n",
    "        # Get predictions\n",
    "        y_true = user_df[\"interaction_label\"].values\n",
    "        y_score = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "        mrr = mean_reciprocal_rank(y_true, y_score, k)\n",
    "        \n",
    "        results[f\"Precision@{k}\"].append(p)\n",
    "        results[f\"Recall@{k}\"].append(r)\n",
    "        results[f\"NDCG@{k}\"].append(n)\n",
    "        results[f\"MRR@{k}\"].append(mrr)\n",
    "    \n",
    "    # Average metrics across users\n",
    "    return {metric: np.mean(values) for metric, values in results.items() if values}\n",
    "\n",
    "def train_content_model(train_df, val_df):\n",
    "    # Clean and prepare training data\n",
    "    train_clean = train_df.copy()\n",
    "    train_clean[\"title\"] = train_clean[\"title\"].fillna(\"\")\n",
    "    train_clean[\"user_interests\"] = train_clean[\"user_interests\"].fillna(\"\")\n",
    "    \n",
    "    # Clean and prepare validation data\n",
    "    val_clean = val_df.copy()\n",
    "    val_clean[\"title\"] = val_clean[\"title\"].fillna(\"\")\n",
    "    val_clean[\"user_interests\"] = val_clean[\"user_interests\"].fillna(\"\")\n",
    "\n",
    "    # TF-IDF features - fit only on training data\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "    tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    \n",
    "    # Transform validation data using fitted vectorizers\n",
    "    tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "    tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "\n",
    "    # Numeric features\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    train_clean[numeric_cols] = train_clean[numeric_cols].fillna(0)\n",
    "    val_clean[numeric_cols] = val_clean[numeric_cols].fillna(0)\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "    X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "    X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    \n",
    "    # Categorical features with OneHotEncoder\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    \n",
    "    # Fill missing values in categorical columns\n",
    "    for col in categorical_cols:\n",
    "        train_clean[col] = train_clean[col].fillna('unknown')\n",
    "        val_clean[col] = val_clean[col].fillna('unknown')\n",
    "    \n",
    "    # Create OneHotEncoder for categorical features\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "\n",
    "    # Combine all features\n",
    "    X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "    X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    \n",
    "    y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "    y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Train CatBoost on training data only\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model and preprocessors\n",
    "    model.save_model(\"saved_models/content_model.cbm\")\n",
    "    with open(\"saved_models/content_tfidf_title.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tfidf_title, f)\n",
    "    with open(\"saved_models/content_tfidf_interests.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tfidf_interests, f)\n",
    "    with open(\"saved_models/content_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(\"saved_models/content_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder\n",
    "\n",
    "def train_svd(train_df, val_df):\n",
    "    # Prepare training data for SVD\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    \n",
    "    # Convert all to strings to ensure compatibility\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    \n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    # Train SVD model on training data only\n",
    "    svd = SVD(n_epochs=50).fit(trainset)\n",
    "    \n",
    "    # Save the SVD model\n",
    "    with open(\"saved_models/svd_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(svd, f)\n",
    "    \n",
    "    # Generate predictions for validation data\n",
    "    val_copy = val_df.copy()\n",
    "    val_copy[\"svd_score\"] = val_copy.apply(\n",
    "        lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return svd, compute_all_metrics(val_copy[\"interaction_label\"].astype(int), val_copy[\"svd_score\"])\n",
    "\n",
    "def hybrid_model(train_df, val_df):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    # Step 1: Train SVD model on train_df\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "    trainset = train_svd_data.build_full_trainset()\n",
    "    svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "    \n",
    "    # Save SVD component of hybrid model\n",
    "    with open(\"saved_models/hybrid_svd_component.pkl\", \"wb\") as f:\n",
    "        pickle.dump(svd_model, f)\n",
    "\n",
    "    # Step 2: Compute SVD scores for both train and val\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    val_df[\"svd_score\"] = val_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "\n",
    "    # Step 3: TF-IDF + numeric features\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "\n",
    "    # Numeric features including SVD score\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    train_df[numeric_cols] = train_df[numeric_cols].fillna(0)\n",
    "    val_df[numeric_cols] = val_df[numeric_cols].fillna(0)\n",
    "\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    \n",
    "    # Categorical features with OneHotEncoder\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    \n",
    "    # Fill missing values in categorical columns\n",
    "    for col in categorical_cols:\n",
    "        train_df[col] = train_df[col].fillna('unknown')\n",
    "        val_df[col] = val_df[col].fillna('unknown')\n",
    "    \n",
    "    # Create OneHotEncoder for categorical features\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "\n",
    "    # Combine all features\n",
    "    X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    \n",
    "    y_train = train_df[\"interaction_label\"].astype(int)\n",
    "    y_val = val_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save hybrid model and preprocessors\n",
    "    model.save_model(\"saved_models/hybrid_model.cbm\")\n",
    "    with open(\"saved_models/hybrid_tfidf_title.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tfidf_title, f)\n",
    "    with open(\"saved_models/hybrid_tfidf_interests.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tfidf_interests, f)\n",
    "    with open(\"saved_models/hybrid_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(\"saved_models/hybrid_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, scores), tfidf_title, tfidf_interests, scaler, encoder\n",
    "\n",
    "def compare_all_models(train_df, val_df):\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "\n",
    "    # Ensure correct types and fill missing\n",
    "    for df in [train_df, val_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "        df[\"interaction_label\"] = df[\"interaction_label\"].astype(int)\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "        df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "        \n",
    "        # Ensure categorical columns exist\n",
    "        for col in ['weather_condition', 'event_indoor_capability', 'user_weather_preference']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"unknown\"\n",
    "\n",
    "    print(\"Training Content-Based model...\")\n",
    "    content_model, content_scores, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder = train_content_model(train_df, val_df)\n",
    "\n",
    "    print(\"Training SVD model...\")\n",
    "    svd_model, svd_scores = train_svd(train_df, val_df)\n",
    "\n",
    "    print(\"Training Hybrid (SVD + Content + Weather) model...\")\n",
    "    hybrid_model_obj, hybrid_scores, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder = hybrid_model(train_df, val_df)\n",
    "\n",
    "    # Assemble final results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Content-Based\": content_scores,\n",
    "        \"SVD\": svd_scores,\n",
    "        \"Hybrid\": hybrid_scores,\n",
    "    })\n",
    "\n",
    "    return results_df.T, content_model, svd_model, hybrid_model_obj, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder\n",
    "\n",
    "def evaluate_on_real_world_data(test_df, model_type=\"hybrid\"):\n",
    "    \"\"\"\n",
    "    Evaluate the specified model on real-world test data\n",
    "    \n",
    "    Args:\n",
    "        test_df: DataFrame containing real-world test data\n",
    "        model_type: Type of model to evaluate ('content', 'svd', or 'hybrid')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    if model_type == \"content\":\n",
    "        model_path = \"saved_models/content_model.cbm\"\n",
    "        tfidf_title_path = \"saved_models/content_tfidf_title.pkl\"\n",
    "        tfidf_interests_path = \"saved_models/content_tfidf_interests.pkl\"\n",
    "        scaler_path = \"saved_models/content_scaler.pkl\"\n",
    "        encoder_path = \"saved_models/content_encoder.pkl\"\n",
    "    elif model_type == \"hybrid\":\n",
    "        model_path = \"saved_models/hybrid_model.cbm\"\n",
    "        tfidf_title_path = \"saved_models/hybrid_tfidf_title.pkl\"\n",
    "        tfidf_interests_path = \"saved_models/hybrid_tfidf_interests.pkl\"\n",
    "        scaler_path = \"saved_models/hybrid_scaler.pkl\"\n",
    "        encoder_path = \"saved_models/hybrid_encoder.pkl\"\n",
    "    else:\n",
    "        # For SVD, use a different approach\n",
    "        with open(\"saved_models/svd_model.pkl\", \"rb\") as f:\n",
    "            svd_model = pickle.load(f)\n",
    "        \n",
    "        test_df = test_df.copy()\n",
    "        test_df[\"svd_score\"] = test_df.apply(\n",
    "            lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        overall_metrics = compute_all_metrics(test_df[\"interaction_label\"].astype(int), test_df[\"svd_score\"])\n",
    "        \n",
    "        # Calculate per-user metrics\n",
    "        user_metrics = {}\n",
    "        for k in [5, 10]:\n",
    "            user_results = {f\"Precision@{k}\": [], f\"Recall@{k}\": [], f\"NDCG@{k}\": [], f\"MRR@{k}\": []}\n",
    "            for user in test_df[\"user_id\"].unique():\n",
    "                user_data = test_df[test_df[\"user_id\"] == user]\n",
    "                if len(user_data) < 2 or user_data[\"interaction_label\"].sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                y_true = user_data[\"interaction_label\"].values\n",
    "                y_score = user_data[\"svd_score\"].values\n",
    "                \n",
    "                p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "                mrr = mean_reciprocal_rank(y_true, y_score, k)\n",
    "                \n",
    "                user_results[f\"Precision@{k}\"].append(p)\n",
    "                user_results[f\"Recall@{k}\"].append(r)\n",
    "                user_results[f\"NDCG@{k}\"].append(n)\n",
    "                user_results[f\"MRR@{k}\"].append(mrr)\n",
    "            \n",
    "            for metric, values in user_results.items():\n",
    "                if values:\n",
    "                    user_metrics[metric] = np.mean(values)\n",
    "        \n",
    "        return {**overall_metrics, **user_metrics}\n",
    "    \n",
    "    # Load model and preprocessors\n",
    "    model = CatBoostClassifier()\n",
    "    model.load_model(model_path)\n",
    "    with open(tfidf_title_path, \"rb\") as f:\n",
    "        tfidf_title = pickle.load(f)\n",
    "    with open(tfidf_interests_path, \"rb\") as f:\n",
    "        tfidf_interests = pickle.load(f)\n",
    "    with open(scaler_path, \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    with open(encoder_path, \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"title\"] = test_df[\"title\"].fillna(\"\")\n",
    "    test_df[\"user_interests\"] = test_df[\"user_interests\"].fillna(\"\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    # TF-IDF features\n",
    "    tfidf_title_mat = tfidf_title.transform(test_df[\"title\"])\n",
    "    tfidf_interests_mat = tfidf_interests.transform(test_df[\"user_interests\"])\n",
    "    \n",
    "    # Numeric features\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    if model_type == \"hybrid\":\n",
    "        # For hybrid model, we need to compute SVD scores\n",
    "        with open(\"saved_models/hybrid_svd_component.pkl\", \"rb\") as f:\n",
    "            svd_model = pickle.load(f)\n",
    "        \n",
    "        test_df[\"svd_score\"] = test_df.apply(\n",
    "            lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, \n",
    "            axis=1\n",
    "        )\n",
    "        numeric_cols.append(\"svd_score\")\n",
    "    \n",
    "    test_df[numeric_cols] = test_df[numeric_cols].fillna(0)\n",
    "    X_numeric = scaler.transform(test_df[numeric_cols])\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for col in categorical_cols:\n",
    "        test_df[col] = test_df[col].fillna('unknown')\n",
    "    X_cat = encoder.transform(test_df[categorical_cols])\n",
    "    \n",
    "    # Combine features\n",
    "    X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric, X_cat]).toarray()\n",
    "    \n",
    "    # Get predictions\n",
    "    y_true = test_df[\"interaction_label\"].values\n",
    "    y_score = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = compute_all_metrics(y_true, y_score)\n",
    "    \n",
    "    # Calculate per-user metrics\n",
    "    user_metrics = evaluate_per_user(test_df, model, tfidf_title, tfidf_interests, scaler, encoder)\n",
    "    \n",
    "    return {**overall_metrics, **user_metrics}\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    import pandas as pd\n",
    "    # Load data for training and validation\n",
    "    users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/users.csv')\n",
    "    events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/events.csv\")\n",
    "    interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/interactions.csv')\n",
    "    \n",
    "    # Load real-world test data\n",
    "    test_users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_users_data.csv')\n",
    "    test_events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_events_data.csv\")\n",
    "    test_interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_interactions_data.csv')\n",
    "    \n",
    "    # Rename columns in test data\n",
    "    test_users_df.rename(columns={\n",
    "        'lat': 'user_lat',\n",
    "        'lng': 'user_lon',\n",
    "        'location': 'user_city',\n",
    "        'indoor_outdoor_preference': 'user_weather_preference',\n",
    "        'joinedAt': 'signup_date'\n",
    "    }, inplace=True)\n",
    "\n",
    "    test_events_df.rename(columns={\n",
    "        'category': 'event_type',\n",
    "        'lat': 'event_lat',\n",
    "        'lng': 'event_lon',\n",
    "        'city': 'event_city',\n",
    "        'weather_description': 'weather_condition',\n",
    "        'temperature_2m_mean': 'temperature'\n",
    "    }, inplace=True)\n",
    "\n",
    "    test_interactions_df.rename(columns={\n",
    "        'distance_to_event': 'interaction_distance_to_event'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Preprocess data\n",
    "    def preprocess_common(interactions_df, users_df, events_df):\n",
    "        # Create copies of the input dataframes\n",
    "        interactions_df = interactions_df.copy()\n",
    "        users_df = users_df.copy()\n",
    "        events_df = events_df.copy()\n",
    "\n",
    "        # Drop rows with missing user_id or event_id\n",
    "        interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "        \n",
    "        # Convert distance_to_event to float\n",
    "        interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "        \n",
    "        # Ensure correct types\n",
    "        for df in [interactions_df, users_df]:\n",
    "            df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "        \n",
    "        for df in [interactions_df, events_df]:\n",
    "            df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "        interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "        # Convert to string type for TF-IDF fields\n",
    "        events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "        users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "        users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "        # Ensure all numeric fields are float\n",
    "        numeric_cols = [\"temperature\", \"attendance_rate\"]\n",
    "        for col in numeric_cols:\n",
    "            events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "        # Return all three dataframes\n",
    "        return interactions_df, users_df, events_df\n",
    "    \n",
    "    # Preprocess training/validation data\n",
    "    interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "    merged_df = interactions_df.merge(events_df, on=\"event_id\").merge(users_df, on=\"user_id\")\n",
    "    #(merged_df.columns)\n",
    "    # Split into train and validation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    \n",
    "    # Preprocess test data\n",
    "    test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "    test_df = test_interactions_df.merge(test_events_df, on=\"event_id\").merge(test_users_df, on=\"user_id\")\n",
    "    #print(test_df.columns)\n",
    "    # Train and compare models\n",
    "    print(\"Training and comparing models...\")\n",
    "    results_df, content_model, svd_model, hybrid_model_obj, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder = compare_all_models(train_df, val_df)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Evaluate on real-world test data\n",
    "    print(\"\\nEvaluating models on real-world test data...\")\n",
    "    \n",
    "    content_test_metrics = evaluate_on_real_world_data(test_df, \"content\")\n",
    "    svd_test_metrics = evaluate_on_real_world_data(test_df, \"svd\")\n",
    "    hybrid_test_metrics = evaluate_on_real_world_data(test_df, \"hybrid\")\n",
    "    \n",
    "    # Create a DataFrame to display the results\n",
    "    test_results_df = pd.DataFrame({\n",
    "        \"Content-Based\": content_test_metrics,\n",
    "        \"SVD\": svd_test_metrics,\n",
    "        \"Hybrid\": hybrid_test_metrics,\n",
    "    })\n",
    "    \n",
    "    print(\"\\nReal-World Test Results:\")\n",
    "    print(test_results_df.T)\n",
    "    \n",
    "    # # Save the test results\n",
    "    # test_results_df.to_csv(\"real_world_test_results.csv\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete. Models and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "Content-Based: {'AUC': 0.7035326174166626, 'MAP': 0.6706636721021786, 'Precision@5': 0.6, 'Recall@5': 0.00019712201852946975, 'NDCG@5': 0.6548086577531307, 'Precision@10': 0.8, 'Recall@10': 0.000525658716078586, 'NDCG@10': 0.7759944384848245, 'MRR@10': 1.0}\n",
      "SVD: {'AUC': 0.6229846172682247, 'MAP': 0.5791060550566649, 'Precision@5': 1.0, 'Recall@5': 0.0003285366975491162, 'NDCG@5': 1.0, 'Precision@10': 0.7, 'Recall@10': 0.0004599513765687627, 'NDCG@10': 0.8006937664098821, 'MRR@10': 1.0}\n",
      "Hybrid: {'AUC': 0.5943294731766849, 'MAP': 0.5725222554606872, 'Precision@5': 0.8, 'Recall@5': 0.000262829358039293, 'NDCG@5': 0.6608397947263839, 'Precision@10': 0.9, 'Recall@10': 0.0005913660555884093, 'NDCG@10': 0.77990823370192, 'MRR@10': 0.5}\n",
      "\n",
      "Test Metrics:\n",
      "Content-Based: {'AUC': 0.5210915237187295, 'MAP': 0.9460097660367329, 'Precision@5': 1.0, 'Recall@5': 0.00018755392175250386, 'NDCG@5': 1.0, 'Precision@10': 1.0, 'Recall@10': 0.0003751078435050077, 'NDCG@10': 1.0, 'MRR@10': 1.0}\n",
      "SVD: {'AUC': 0.5, 'MAP': 0.9437148217636022, 'Precision@5': 1.0, 'Recall@5': 0.00018755392175250386, 'NDCG@5': 1.0, 'Precision@10': 1.0, 'Recall@10': 0.0003751078435050077, 'NDCG@10': 1.0, 'MRR@10': 1.0}\n",
      "Hybrid: {'AUC': 0.5161100679652947, 'MAP': 0.9471027281054745, 'Precision@5': 0.8, 'Recall@5': 0.00015004313740200306, 'NDCG@5': 0.830419897363192, 'Precision@10': 0.9, 'Recall@10': 0.0003375970591545069, 'NDCG@10': 0.88995411685096, 'MRR@10': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# --- Metrics ---\n",
    "def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    top_k = np.array(y_true)[sorted_indices][:k]\n",
    "    precision = np.mean(top_k)\n",
    "    recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "    dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "    ideal_k = min(int(np.sum(y_true)), k)\n",
    "    idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_score, k=10):\n",
    "    sorted_indices = np.argsort(y_score)[::-1][:k]\n",
    "    top_k = np.array(y_true)[sorted_indices]\n",
    "    ranks = np.where(top_k == 1)[0]\n",
    "    if len(ranks) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 / (ranks[0] + 1)\n",
    "\n",
    "def compute_all_metrics(y_true, y_score):\n",
    "    metrics = {}\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        metrics[\"AUC\"] = np.nan\n",
    "        metrics[\"MAP\"] = np.nan\n",
    "    else:\n",
    "        metrics[\"AUC\"] = roc_auc_score(y_true, y_score)\n",
    "        metrics[\"MAP\"] = average_precision_score(y_true, y_score)\n",
    "    for k in [5, 10]:\n",
    "        p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "        metrics[f\"Precision@{k}\"] = p\n",
    "        metrics[f\"Recall@{k}\"] = r\n",
    "        metrics[f\"NDCG@{k}\"] = n\n",
    "    metrics[\"MRR@10\"] = mean_reciprocal_rank(y_true, y_score, k=10)\n",
    "    return metrics\n",
    "\n",
    "# --- Model Training ---\n",
    "def train_content_model(train_df, val_df):\n",
    "    train_clean = train_df.copy()\n",
    "    val_clean = val_df.copy()\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "        df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "    tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "    tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "    X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "    X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for df in [train_clean, val_clean]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "    X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "    X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "    y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder\n",
    "\n",
    "def train_svd(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    svd = SVD(n_epochs=50).fit(trainset)\n",
    "    val_copy = val_df.copy()\n",
    "    val_copy[\"svd_score\"] = val_copy.apply(\n",
    "        lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_val = val_copy[\"interaction_label\"].astype(int)\n",
    "    val_scores = val_copy[\"svd_score\"]\n",
    "    return svd, compute_all_metrics(y_val, val_scores)\n",
    "\n",
    "def train_hybrid_model(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "    trainset = train_svd_data.build_full_trainset()\n",
    "    svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    val_df[\"svd_score\"] = val_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    for df in [train_df, val_df]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for df in [train_df, val_df]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "    X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    y_train = train_df[\"interaction_label\"].astype(int)\n",
    "    y_val = val_df[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder, svd_model\n",
    "\n",
    "# --- Test Set Evaluation ---\n",
    "def evaluate_content_or_hybrid_on_test(test_df, model, tfidf_title, tfidf_interests, scaler, encoder, model_type=\"content\", svd_model=None):\n",
    "    df = test_df.copy()\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    if model_type == \"hybrid\":\n",
    "        df[\"svd_score\"] = df.apply(\n",
    "            lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "        )\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    else:\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    tfidf_title_mat = tfidf_title.transform(df[\"title\"])\n",
    "    tfidf_interests_mat = tfidf_interests.transform(df[\"user_interests\"])\n",
    "    X_numeric = scaler.transform(df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    X_cat = encoder.transform(df[categorical_cols])\n",
    "    X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric, X_cat]).toarray()\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = model.predict_proba(X)[:, 1]\n",
    "    return compute_all_metrics(y_true, y_score)\n",
    "\n",
    "def evaluate_svd_on_test(test_df, svd_model):\n",
    "    df = test_df.copy()\n",
    "    df[\"svd_score\"] = df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = df[\"svd_score\"]\n",
    "    return compute_all_metrics(y_true, y_score)\n",
    "\n",
    "import pandas as pd\n",
    "# Load data for training and validation\n",
    "users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/users.csv')\n",
    "events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/events.csv\")\n",
    "interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/interactions.csv')\n",
    "\n",
    "# Load real-world test data\n",
    "test_users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_users_data.csv')\n",
    "test_events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_events_data.csv\")\n",
    "test_interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_interactions_data.csv')\n",
    "\n",
    "# Rename columns in test data\n",
    "test_users_df.rename(columns={\n",
    "    'lat': 'user_lat',\n",
    "    'lng': 'user_lon',\n",
    "    'location': 'user_city',\n",
    "    'indoor_outdoor_preference': 'user_weather_preference',\n",
    "    'joinedAt': 'signup_date'\n",
    "}, inplace=True)\n",
    "\n",
    "test_events_df.rename(columns={\n",
    "    'category': 'event_type',\n",
    "    'lat': 'event_lat',\n",
    "    'lng': 'event_lon',\n",
    "    'city': 'event_city',\n",
    "    'weather_description': 'weather_condition',\n",
    "    'temperature_2m_mean': 'temperature'\n",
    "}, inplace=True)\n",
    "\n",
    "test_interactions_df.rename(columns={\n",
    "    'distance_to_event': 'interaction_distance_to_event'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_common(interactions_df, users_df, events_df):\n",
    "    # Create copies of the input dataframes\n",
    "    interactions_df = interactions_df.copy()\n",
    "    users_df = users_df.copy()\n",
    "    events_df = events_df.copy()\n",
    "\n",
    "    # Drop rows with missing user_id or event_id\n",
    "    interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "    \n",
    "    # Convert distance_to_event to float\n",
    "    interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "    \n",
    "    # Ensure correct types\n",
    "    for df in [interactions_df, users_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    \n",
    "    for df in [interactions_df, events_df]:\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "    interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Convert to string type for TF-IDF fields\n",
    "    events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "    users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "    users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "    # Ensure all numeric fields are float\n",
    "    numeric_cols = [\"temperature\", \"attendance_rate\"]\n",
    "    for col in numeric_cols:\n",
    "        events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    # Return all three dataframes\n",
    "    return interactions_df, users_df, events_df\n",
    "\n",
    "# Preprocess training/validation data\n",
    "interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "merged_df = interactions_df.merge(events_df, on=\"event_id\").merge(users_df, on=\"user_id\")\n",
    "#(merged_df.columns)\n",
    "# Split into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Preprocess test data\n",
    "test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "test_df = test_interactions_df.merge(test_events_df, on=\"event_id\").merge(test_users_df, on=\"user_id\")\n",
    "#print(test_df.columns)\n",
    "\n",
    "# Train models\n",
    "content_model, content_val_metrics, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder = train_content_model(train_df, val_df)\n",
    "svd_model, svd_val_metrics = train_svd(train_df, val_df)\n",
    "hybrid_model_obj, hybrid_val_metrics, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, hybrid_svd_model = train_hybrid_model(train_df, val_df)\n",
    "\n",
    "# Evaluate on test set\n",
    "content_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, content_model, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder, model_type=\"content\"\n",
    ")\n",
    "svd_test_metrics = evaluate_svd_on_test(test_df, svd_model)\n",
    "hybrid_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, hybrid_model_obj, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, model_type=\"hybrid\", svd_model=hybrid_svd_model\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"Content-Based:\", content_val_metrics)\n",
    "print(\"SVD:\", svd_val_metrics)\n",
    "print(\"Hybrid:\", hybrid_val_metrics)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Content-Based:\", content_test_metrics)\n",
    "print(\"SVD:\", svd_test_metrics)\n",
    "print(\"Hybrid:\", hybrid_test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Precision@1  Recall@1  NDCG@1  MRR@1  Precision@5  Recall@5  \\\n",
      "Content-Based          1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
      "SVD                    1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
      "Hybrid                 1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
      "\n",
      "               NDCG@5  MRR@5  Precision@10  Recall@10  NDCG@10  MRR@10  \\\n",
      "Content-Based     1.0    1.0           1.0   0.000375      1.0     1.0   \n",
      "SVD               1.0    1.0           1.0   0.000375      1.0     1.0   \n",
      "Hybrid            1.0    1.0           1.0   0.000375      1.0     1.0   \n",
      "\n",
      "               Precision@50  Recall@50   NDCG@50  MRR@50  Precision@100  \\\n",
      "Content-Based          0.98   0.001838  0.981407     1.0           0.93   \n",
      "SVD                    0.96   0.001801  0.966914     1.0           0.86   \n",
      "Hybrid                 0.88   0.001650  0.899723     1.0           0.91   \n",
      "\n",
      "               Recall@100  NDCG@100  MRR@100  \n",
      "Content-Based    0.003489  0.943707      1.0  \n",
      "SVD              0.003226  0.886381      1.0  \n",
      "Hybrid           0.003413  0.915821      1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# --- Metrics ---\n",
    "K_LIST = [1, 5, 10, 50, 100]\n",
    "\n",
    "def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    top_k = np.array(y_true)[sorted_indices][:k]\n",
    "    precision = np.mean(top_k)\n",
    "    recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "    dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "    ideal_k = min(int(np.sum(y_true)), k)\n",
    "    idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_score, k=100):\n",
    "    sorted_indices = np.argsort(y_score)[::-1][:k]\n",
    "    top_k = np.array(y_true)[sorted_indices]\n",
    "    ranks = np.where(top_k == 1)[0]\n",
    "    if len(ranks) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 / (ranks[0] + 1)\n",
    "\n",
    "def compute_all_metrics(y_true, y_score, k_list=K_LIST):\n",
    "    metrics = {}\n",
    "    for k in k_list:\n",
    "        p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "        metrics[f\"Precision@{k}\"] = p\n",
    "        metrics[f\"Recall@{k}\"] = r\n",
    "        metrics[f\"NDCG@{k}\"] = n\n",
    "        metrics[f\"MRR@{k}\"] = mean_reciprocal_rank(y_true, y_score, k)\n",
    "    return metrics\n",
    "\n",
    "# --- Model Training ---\n",
    "def train_content_model(train_df, val_df):\n",
    "    train_clean = train_df.copy()\n",
    "    val_clean = val_df.copy()\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "        df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "    tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "    tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "    X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "    X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for df in [train_clean, val_clean]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "    X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "    X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "    y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder\n",
    "\n",
    "def train_svd(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    svd = SVD(n_epochs=50).fit(trainset)\n",
    "    val_copy = val_df.copy()\n",
    "    val_copy[\"svd_score\"] = val_copy.apply(\n",
    "        lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_val = val_copy[\"interaction_label\"].astype(int)\n",
    "    val_scores = val_copy[\"svd_score\"]\n",
    "    return svd, compute_all_metrics(y_val, val_scores)\n",
    "\n",
    "def train_hybrid_model(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "    trainset = train_svd_data.build_full_trainset()\n",
    "    svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    val_df[\"svd_score\"] = val_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    for df in [train_df, val_df]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for df in [train_df, val_df]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "    X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    y_train = train_df[\"interaction_label\"].astype(int)\n",
    "    y_val = val_df[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder, svd_model\n",
    "\n",
    "# --- Test Set Evaluation ---\n",
    "def evaluate_content_or_hybrid_on_test(test_df, model, tfidf_title, tfidf_interests, scaler, encoder, model_type=\"content\", svd_model=None, k_list=K_LIST):\n",
    "    df = test_df.copy()\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    if model_type == \"hybrid\":\n",
    "        df[\"svd_score\"] = df.apply(\n",
    "            lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "        )\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    else:\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    tfidf_title_mat = tfidf_title.transform(df[\"title\"])\n",
    "    tfidf_interests_mat = tfidf_interests.transform(df[\"user_interests\"])\n",
    "    X_numeric = scaler.transform(df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'user_weather_preference']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    X_cat = encoder.transform(df[categorical_cols])\n",
    "    X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric, X_cat]).toarray()\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = model.predict_proba(X)[:, 1]\n",
    "    return compute_all_metrics(y_true, y_score, k_list=k_list)\n",
    "\n",
    "def evaluate_svd_on_test(test_df, svd_model, k_list=K_LIST):\n",
    "    df = test_df.copy()\n",
    "    df[\"svd_score\"] = df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = df[\"svd_score\"]\n",
    "    return compute_all_metrics(y_true, y_score, k_list=k_list)\n",
    "\n",
    "# --- Example Training/Evaluation Pipeline ---\n",
    "import pandas as pd\n",
    "# Load data for training and validation\n",
    "users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/users.csv')\n",
    "events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/events.csv\")\n",
    "interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/fully_synthetic_data/data/interactions.csv')\n",
    "\n",
    "# Load real-world test data\n",
    "test_users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_users_data.csv')\n",
    "test_events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_events_data.csv\")\n",
    "test_interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_interactions_data.csv')\n",
    "\n",
    "# Rename columns in test data\n",
    "test_users_df.rename(columns={\n",
    "    'lat': 'user_lat',\n",
    "    'lng': 'user_lon',\n",
    "    'location': 'user_city',\n",
    "    'indoor_outdoor_preference': 'user_weather_preference',\n",
    "    'joinedAt': 'signup_date'\n",
    "}, inplace=True)\n",
    "\n",
    "test_events_df.rename(columns={\n",
    "    'category': 'event_type',\n",
    "    'lat': 'event_lat',\n",
    "    'lng': 'event_lon',\n",
    "    'city': 'event_city',\n",
    "    'weather_description': 'weather_condition',\n",
    "    'temperature_2m_mean': 'temperature'\n",
    "}, inplace=True)\n",
    "\n",
    "test_interactions_df.rename(columns={\n",
    "    'distance_to_event': 'interaction_distance_to_event'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_common(interactions_df, users_df, events_df):\n",
    "    # Create copies of the input dataframes\n",
    "    interactions_df = interactions_df.copy()\n",
    "    users_df = users_df.copy()\n",
    "    events_df = events_df.copy()\n",
    "\n",
    "    # Drop rows with missing user_id or event_id\n",
    "    interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "    \n",
    "    # Convert distance_to_event to float\n",
    "    interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "    \n",
    "    # Ensure correct types\n",
    "    for df in [interactions_df, users_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    \n",
    "    for df in [interactions_df, events_df]:\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "    interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Convert to string type for TF-IDF fields\n",
    "    events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "    users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "    users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "    # Ensure all numeric fields are float\n",
    "    numeric_cols = [\"temperature\", \"attendance_rate\"]\n",
    "    for col in numeric_cols:\n",
    "        events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    # Return all three dataframes\n",
    "    return interactions_df, users_df, events_df\n",
    "\n",
    "# Preprocess training/validation data\n",
    "interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "merged_df = interactions_df.merge(events_df, on=\"event_id\").merge(users_df, on=\"user_id\")\n",
    "#(merged_df.columns)\n",
    "# Split into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Preprocess test data\n",
    "test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "test_df = test_interactions_df.merge(test_events_df, on=\"event_id\").merge(test_users_df, on=\"user_id\")\n",
    "#print(test_df.columns)\n",
    "\n",
    "# Train models\n",
    "content_model, content_val_metrics, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder = train_content_model(train_df, val_df)\n",
    "svd_model, svd_val_metrics = train_svd(train_df, val_df)\n",
    "hybrid_model_obj, hybrid_val_metrics, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, hybrid_svd_model = train_hybrid_model(train_df, val_df)\n",
    "\n",
    "# Evaluate on test set\n",
    "content_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, content_model, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder, model_type=\"content\", k_list=K_LIST\n",
    ")\n",
    "svd_test_metrics = evaluate_svd_on_test(test_df, svd_model, k_list=K_LIST)\n",
    "hybrid_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, hybrid_model_obj, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, model_type=\"hybrid\", svd_model=hybrid_svd_model, k_list=K_LIST\n",
    ")\n",
    "\n",
    "# --- Save results in a DataFrame (excluding AUC and MAP) ---\n",
    "def filter_metrics(metrics_dict):\n",
    "    return {k: v for k, v in metrics_dict.items() if not (k.startswith(\"AUC\") or k.startswith(\"MAP\"))}\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Content-Based\": filter_metrics(content_test_metrics),\n",
    "    \"SVD\": filter_metrics(svd_test_metrics),\n",
    "    \"Hybrid\": filter_metrics(hybrid_test_metrics),\n",
    "}).T\n",
    "\n",
    "results_df.to_csv(\"test_metrics_k_results.csv\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>MRR@1</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>MRR@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>MRR@10</th>\n",
       "      <th>Precision@50</th>\n",
       "      <th>Recall@50</th>\n",
       "      <th>NDCG@50</th>\n",
       "      <th>MRR@50</th>\n",
       "      <th>Precision@100</th>\n",
       "      <th>Recall@100</th>\n",
       "      <th>NDCG@100</th>\n",
       "      <th>MRR@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Content-Based</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.981407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.943707</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.966914</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.886381</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybrid</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.899723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.915821</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision@1  Recall@1  NDCG@1  MRR@1  Precision@5  Recall@5  \\\n",
       "Content-Based          1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
       "SVD                    1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
       "Hybrid                 1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
       "\n",
       "               NDCG@5  MRR@5  Precision@10  Recall@10  NDCG@10  MRR@10  \\\n",
       "Content-Based     1.0    1.0           1.0   0.000375      1.0     1.0   \n",
       "SVD               1.0    1.0           1.0   0.000375      1.0     1.0   \n",
       "Hybrid            1.0    1.0           1.0   0.000375      1.0     1.0   \n",
       "\n",
       "               Precision@50  Recall@50   NDCG@50  MRR@50  Precision@100  \\\n",
       "Content-Based          0.98   0.001838  0.981407     1.0           0.93   \n",
       "SVD                    0.96   0.001801  0.966914     1.0           0.86   \n",
       "Hybrid                 0.88   0.001650  0.899723     1.0           0.91   \n",
       "\n",
       "               Recall@100  NDCG@100  MRR@100  \n",
       "Content-Based    0.003489  0.943707      1.0  \n",
       "SVD              0.003226  0.886381      1.0  \n",
       "Hybrid           0.003413  0.915821      1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "      <th>interaction_label</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>event_indoor_capability</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lon</th>\n",
       "      <th>user_city</th>\n",
       "      <th>user_weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>user_interests</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>social_connectedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CJ011J</td>\n",
       "      <td>PG158Y</td>\n",
       "      <td>PP391M</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>2025-08-14 19:16:49.777185</td>\n",
       "      <td>10.675627</td>\n",
       "      <td>1</td>\n",
       "      <td>16693</td>\n",
       "      <td>persistent fullrange encoding community causes...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>11299</td>\n",
       "      <td>40.732831</td>\n",
       "      <td>-73.951693</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>25.0</td>\n",
       "      <td>music sports fashion</td>\n",
       "      <td>2023-12-02 02:45:09.610317</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51136</td>\n",
       "      <td>WB346E</td>\n",
       "      <td>PG158Y</td>\n",
       "      <td>AM416J</td>\n",
       "      <td>invited &amp; no</td>\n",
       "      <td>2025-07-08 04:57:54.696089</td>\n",
       "      <td>5.022271</td>\n",
       "      <td>0</td>\n",
       "      <td>12872</td>\n",
       "      <td>openarchitected secondary local area network i...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>11299</td>\n",
       "      <td>40.732831</td>\n",
       "      <td>-73.951693</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>25.0</td>\n",
       "      <td>music sports fashion</td>\n",
       "      <td>2023-12-02 02:45:09.610317</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39875</td>\n",
       "      <td>YL968R</td>\n",
       "      <td>PG158Y</td>\n",
       "      <td>VT464H</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>2025-07-23 17:55:09.485708</td>\n",
       "      <td>11.276411</td>\n",
       "      <td>1</td>\n",
       "      <td>15364</td>\n",
       "      <td>phased didactic array business networking in n...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>11299</td>\n",
       "      <td>40.732831</td>\n",
       "      <td>-73.951693</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>25.0</td>\n",
       "      <td>music sports fashion</td>\n",
       "      <td>2023-12-02 02:45:09.610317</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2742</td>\n",
       "      <td>VT157H</td>\n",
       "      <td>PG158Y</td>\n",
       "      <td>CU455G</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>2025-08-23 11:48:59.945844</td>\n",
       "      <td>2.537383</td>\n",
       "      <td>1</td>\n",
       "      <td>9647</td>\n",
       "      <td>standalone needsbased orchestration entertainm...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>11299</td>\n",
       "      <td>40.732831</td>\n",
       "      <td>-73.951693</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>25.0</td>\n",
       "      <td>music sports fashion</td>\n",
       "      <td>2023-12-02 02:45:09.610317</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50458</td>\n",
       "      <td>QA779Q</td>\n",
       "      <td>PG158Y</td>\n",
       "      <td>FJ447J</td>\n",
       "      <td>maybe</td>\n",
       "      <td>2025-05-02 10:28:30.708459</td>\n",
       "      <td>1.274298</td>\n",
       "      <td>1</td>\n",
       "      <td>15857</td>\n",
       "      <td>synergized 24hour emulation arts culture in ne...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>11299</td>\n",
       "      <td>40.732831</td>\n",
       "      <td>-73.951693</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>25.0</td>\n",
       "      <td>music sports fashion</td>\n",
       "      <td>2023-12-02 02:45:09.610317</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x interaction_id user_id event_id interaction_type  \\\n",
       "0             0         CJ011J  PG158Y   PP391M  invited & maybe   \n",
       "1         51136         WB346E  PG158Y   AM416J     invited & no   \n",
       "2         39875         YL968R  PG158Y   VT464H  invited & maybe   \n",
       "3          2742         VT157H  PG158Y   CU455G  invited & maybe   \n",
       "4         50458         QA779Q  PG158Y   FJ447J            maybe   \n",
       "\n",
       "             interaction_time  interaction_distance_to_event  \\\n",
       "0  2025-08-14 19:16:49.777185                      10.675627   \n",
       "1  2025-07-08 04:57:54.696089                       5.022271   \n",
       "2  2025-07-23 17:55:09.485708                      11.276411   \n",
       "3  2025-08-23 11:48:59.945844                       2.537383   \n",
       "4  2025-05-02 10:28:30.708459                       1.274298   \n",
       "\n",
       "   interaction_label  Unnamed: 0_y  \\\n",
       "0                  1         16693   \n",
       "1                  0         12872   \n",
       "2                  1         15364   \n",
       "3                  1          9647   \n",
       "4                  1         15857   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0  persistent fullrange encoding community causes...  ...   \n",
       "1  openarchitected secondary local area network i...  ...   \n",
       "2  phased didactic array business networking in n...  ...   \n",
       "3  standalone needsbased orchestration entertainm...  ...   \n",
       "4  synergized 24hour emulation arts culture in ne...  ...   \n",
       "\n",
       "  event_indoor_capability  Unnamed: 0   user_lat   user_lon user_city  \\\n",
       "0                   False       11299  40.732831 -73.951693  New York   \n",
       "1                    True       11299  40.732831 -73.951693  New York   \n",
       "2                    True       11299  40.732831 -73.951693  New York   \n",
       "3                    True       11299  40.732831 -73.951693  New York   \n",
       "4                    True       11299  40.732831 -73.951693  New York   \n",
       "\n",
       "   user_weather_preference   age        user_interests  \\\n",
       "0                  outdoor  25.0  music sports fashion   \n",
       "1                  outdoor  25.0  music sports fashion   \n",
       "2                  outdoor  25.0  music sports fashion   \n",
       "3                  outdoor  25.0  music sports fashion   \n",
       "4                  outdoor  25.0  music sports fashion   \n",
       "\n",
       "                  signup_date  social_connectedness  \n",
       "0  2023-12-02 02:45:09.610317                    16  \n",
       "1  2023-12-02 02:45:09.610317                    16  \n",
       "2  2023-12-02 02:45:09.610317                    16  \n",
       "3  2023-12-02 02:45:09.610317                    16  \n",
       "4  2023-12-02 02:45:09.610317                    16  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_common(interactions_df, users_df, events_df):\n",
    " \n",
    "    # Create copies of the input dataframes\n",
    "    interactions_df = interactions_df.copy()\n",
    "    users_df = users_df.copy()\n",
    "    events_df = events_df.copy()\n",
    "\n",
    "    # Drop rows with missing user_id or event_id\n",
    "    interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "    \n",
    "    # Convert distance_to_event to float\n",
    "    interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "    \n",
    "    # Ensure correct types\n",
    "    for df in [interactions_df, users_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    \n",
    "    for df in [interactions_df, events_df]:\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "    interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Convert to string type for TF-IDF fields\n",
    "    events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "    users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "    users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "    # Ensure all numeric fields are float - fixed to reference events_df instead of df\n",
    "    numeric_cols = [\"temperature\", \"attendance_rate\"]\n",
    "    for col in numeric_cols:\n",
    "        events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    # Return all three dataframes\n",
    "    return interactions_df, users_df, events_df\n",
    "\n",
    "interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "len(interactions_df) + len(users_df) + len(events_df)\n",
    "merged_df = interactions_df.merge(events_df,on=\"event_id\")\\\n",
    "    .merge(users_df, on=\"user_id\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "len(interactions_df) + len(users_df) + len(events_df)\n",
    "test_df = test_interactions_df.merge(test_events_df,on=\"event_id\")\\\n",
    "    .merge(test_users_df, on=\"user_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "      <th>interaction_label</th>\n",
       "      <th>event_weather_condition</th>\n",
       "      <th>event_temperature</th>\n",
       "      <th>event_precipitation_sum</th>\n",
       "      <th>user_weather_condition</th>\n",
       "      <th>...</th>\n",
       "      <th>event_indoor_capability</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lon</th>\n",
       "      <th>user_city</th>\n",
       "      <th>user_weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>user_interests</th>\n",
       "      <th>signup_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3468617687</td>\n",
       "      <td>702719295</td>\n",
       "      <td>maybe</td>\n",
       "      <td>522.753486</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>3.745500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>366</td>\n",
       "      <td>43.158</td>\n",
       "      <td>-79.244</td>\n",
       "      <td>Saint Catharines  Ontario</td>\n",
       "      <td>indoor</td>\n",
       "      <td>20</td>\n",
       "      <td>drink technology business seasonal food entert...</td>\n",
       "      <td>2012-09-25T17:48:37.804Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3468617687</td>\n",
       "      <td>284003894</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>55.942166</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>11.129666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>366</td>\n",
       "      <td>43.158</td>\n",
       "      <td>-79.244</td>\n",
       "      <td>Saint Catharines  Ontario</td>\n",
       "      <td>indoor</td>\n",
       "      <td>20</td>\n",
       "      <td>drink technology business seasonal food entert...</td>\n",
       "      <td>2012-09-25T17:48:37.804Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>3468617687</td>\n",
       "      <td>2951450859</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>55.670330</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>1.859583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>366</td>\n",
       "      <td>43.158</td>\n",
       "      <td>-79.244</td>\n",
       "      <td>Saint Catharines  Ontario</td>\n",
       "      <td>indoor</td>\n",
       "      <td>20</td>\n",
       "      <td>drink technology business seasonal food entert...</td>\n",
       "      <td>2012-09-25T17:48:37.804Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>3468617687</td>\n",
       "      <td>3961159311</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>55.045346</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>6.204750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>366</td>\n",
       "      <td>43.158</td>\n",
       "      <td>-79.244</td>\n",
       "      <td>Saint Catharines  Ontario</td>\n",
       "      <td>indoor</td>\n",
       "      <td>20</td>\n",
       "      <td>drink technology business seasonal food entert...</td>\n",
       "      <td>2012-09-25T17:48:37.804Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>3468617687</td>\n",
       "      <td>2017343689</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>56.013814</td>\n",
       "      <td>1</td>\n",
       "      <td>Light Rain</td>\n",
       "      <td>8.367833</td>\n",
       "      <td>16.800001</td>\n",
       "      <td>Light Rain</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>16.800001</td>\n",
       "      <td>366</td>\n",
       "      <td>43.158</td>\n",
       "      <td>-79.244</td>\n",
       "      <td>Saint Catharines  Ontario</td>\n",
       "      <td>indoor</td>\n",
       "      <td>20</td>\n",
       "      <td>drink technology business seasonal food entert...</td>\n",
       "      <td>2012-09-25T17:48:37.804Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x     user_id    event_id interaction_type  \\\n",
       "0             0  3468617687   702719295            maybe   \n",
       "1             4  3468617687   284003894    invited & yes   \n",
       "2            45  3468617687  2951450859    invited & yes   \n",
       "3            76  3468617687  3961159311    invited & yes   \n",
       "4            87  3468617687  2017343689    invited & yes   \n",
       "\n",
       "   interaction_distance_to_event  interaction_label event_weather_condition  \\\n",
       "0                     522.753486                  1                  Cloudy   \n",
       "1                      55.942166                  1                  Cloudy   \n",
       "2                      55.670330                  1                  Cloudy   \n",
       "3                      55.045346                  1                  Cloudy   \n",
       "4                      56.013814                  1              Light Rain   \n",
       "\n",
       "   event_temperature  event_precipitation_sum user_weather_condition  ...  \\\n",
       "0           3.745500                 0.000000                 Cloudy  ...   \n",
       "1          11.129666                 0.000000          Partly Cloudy  ...   \n",
       "2           1.859583                 0.000000                 Cloudy  ...   \n",
       "3           6.204750                 0.000000                 Cloudy  ...   \n",
       "4           8.367833                16.800001             Light Rain  ...   \n",
       "\n",
       "   event_indoor_capability  precipitation_sum  Unnamed: 0 user_lat user_lon  \\\n",
       "0                     True           0.000000         366   43.158  -79.244   \n",
       "1                     True           0.000000         366   43.158  -79.244   \n",
       "2                     True           0.000000         366   43.158  -79.244   \n",
       "3                     True           0.000000         366   43.158  -79.244   \n",
       "4                     True          16.800001         366   43.158  -79.244   \n",
       "\n",
       "                   user_city  user_weather_preference age  \\\n",
       "0  Saint Catharines  Ontario                   indoor  20   \n",
       "1  Saint Catharines  Ontario                   indoor  20   \n",
       "2  Saint Catharines  Ontario                   indoor  20   \n",
       "3  Saint Catharines  Ontario                   indoor  20   \n",
       "4  Saint Catharines  Ontario                   indoor  20   \n",
       "\n",
       "                                      user_interests               signup_date  \n",
       "0  drink technology business seasonal food entert...  2012-09-25T17:48:37.804Z  \n",
       "1  drink technology business seasonal food entert...  2012-09-25T17:48:37.804Z  \n",
       "2  drink technology business seasonal food entert...  2012-09-25T17:48:37.804Z  \n",
       "3  drink technology business seasonal food entert...  2012-09-25T17:48:37.804Z  \n",
       "4  drink technology business seasonal food entert...  2012-09-25T17:48:37.804Z  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "len(interactions_df) + len(users_df) + len(events_df)\n",
    "test_df = test_interactions_df.merge(test_events_df,on=\"event_id\")\\\n",
    "    .merge(test_users_df, on=\"user_id\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Content-Based model...\n",
      "Training SVD model...\n",
      "Training Hybrid (SVD + Content + Weather) model with strict validation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Content-Based</th>\n",
       "      <td>0.893437</td>\n",
       "      <td>0.991727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.990446</td>\n",
       "      <td>0.999411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybrid</th>\n",
       "      <td>0.712929</td>\n",
       "      <td>0.966202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUC       MAP  Precision@5  Recall@5  NDCG@5  \\\n",
       "Content-Based  0.893437  0.991727          1.0  0.000235     1.0   \n",
       "SVD            0.990446  0.999411          1.0  0.000235     1.0   \n",
       "Hybrid         0.712929  0.966202          1.0  0.000935     1.0   \n",
       "\n",
       "               Precision@10  Recall@10  NDCG@10  \n",
       "Content-Based           1.0   0.000469      1.0  \n",
       "SVD                     1.0   0.000469      1.0  \n",
       "Hybrid                  1.0   0.001871      1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Enhanced benchmarking evaluation and training pipeline with complete metrics and weather features included\n",
    "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import classification_report\n",
    "# from surprise import Dataset, Reader, SVD\n",
    "# from scipy.sparse import hstack\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # --- Shared Evaluation ---\n",
    "# def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "#     # Sort indices by descending score\n",
    "#     sorted_indices = np.argsort(y_score)[::-1]\n",
    "#     # Get top-k true labels\n",
    "#     top_k = np.array(y_true)[sorted_indices][:k]\n",
    "#     # Calculate precision\n",
    "#     precision = np.mean(top_k)\n",
    "#     # Calculate recall\n",
    "#     recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "#     # Calculate DCG\n",
    "#     dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "#     # Calculate ideal DCG\n",
    "#     ideal_k = min(int(np.sum(y_true)), k)\n",
    "#     idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "#     # Calculate NDCG\n",
    "#     ndcg = dcg / idcg if idcg > 0 else 0\n",
    "#     return precision, recall, ndcg\n",
    "\n",
    "# def compute_all_metrics(y_true, y_score):\n",
    "#     # Ensure there are enough positive examples for AUC calculation\n",
    "#     if len(np.unique(y_true)) < 2:\n",
    "#         metrics = {\n",
    "#             \"AUC\": np.nan,\n",
    "#             \"MAP\": np.nan\n",
    "#         }\n",
    "#     else:\n",
    "#         metrics = {\n",
    "#             \"AUC\": roc_auc_score(y_true, y_score),\n",
    "#             \"MAP\": average_precision_score(y_true, y_score)\n",
    "#         }\n",
    "    \n",
    "#     for k in [5, 10]:\n",
    "#         p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "#         metrics[f\"Precision@{k}\"] = p\n",
    "#         metrics[f\"Recall@{k}\"] = r\n",
    "#         metrics[f\"NDCG@{k}\"] = n\n",
    "#     return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Content-Based model...\n",
      "Training SVD model...\n",
      "Training Hybrid (SVD + Content + Weather) model with strict validation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Content-Based</th>\n",
       "      <td>0.775522</td>\n",
       "      <td>0.980886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.797640</td>\n",
       "      <td>0.980718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybrid</th>\n",
       "      <td>0.708555</td>\n",
       "      <td>0.964895</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.853932</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.831848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUC       MAP  Precision@5  Recall@5    NDCG@5  \\\n",
       "Content-Based  0.775522  0.980886          1.0  0.000935  1.000000   \n",
       "SVD            0.797640  0.980718          1.0  0.000935  1.000000   \n",
       "Hybrid         0.708555  0.964895          0.8  0.000748  0.853932   \n",
       "\n",
       "               Precision@10  Recall@10   NDCG@10  \n",
       "Content-Based           1.0   0.001871  1.000000  \n",
       "SVD                     1.0   0.001871  1.000000  \n",
       "Hybrid                  0.8   0.001497  0.831848  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # Enhanced benchmarking evaluation and training pipeline with complete metrics and weather features included\n",
    "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import classification_report\n",
    "# from surprise import Dataset, Reader, SVD\n",
    "# from scipy.sparse import hstack\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # --- Shared Evaluation ---\n",
    "# def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "#     # Sort indices by descending score\n",
    "#     sorted_indices = np.argsort(y_score)[::-1]\n",
    "#     # Get top-k true labels\n",
    "#     top_k = np.array(y_true)[sorted_indices][:k]\n",
    "#     # Calculate precision\n",
    "#     precision = np.mean(top_k)\n",
    "#     # Calculate recall\n",
    "#     recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "#     # Calculate DCG\n",
    "#     dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "#     # Calculate ideal DCG\n",
    "#     ideal_k = min(int(np.sum(y_true)), k)\n",
    "#     idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "#     # Calculate NDCG\n",
    "#     ndcg = dcg / idcg if idcg > 0 else 0\n",
    "#     return precision, recall, ndcg\n",
    "\n",
    "# def compute_all_metrics(y_true, y_score):\n",
    "#     # Ensure there are enough positive examples for AUC calculation\n",
    "#     if len(np.unique(y_true)) < 2:\n",
    "#         metrics = {\n",
    "#             \"AUC\": np.nan,\n",
    "#             \"MAP\": np.nan\n",
    "#         }\n",
    "#     else:\n",
    "#         metrics = {\n",
    "#             \"AUC\": roc_auc_score(y_true, y_score),\n",
    "#             \"MAP\": average_precision_score(y_true, y_score)\n",
    "#         }\n",
    "    \n",
    "#     for k in [5, 10]:\n",
    "#         p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "#         metrics[f\"Precision@{k}\"] = p\n",
    "#         metrics[f\"Recall@{k}\"] = r\n",
    "#         metrics[f\"NDCG@{k}\"] = n\n",
    "#     return metrics\n",
    "\n",
    "\n",
    "# def train_content_model(train_df, val_df):\n",
    "#     # Clean and prepare training data\n",
    "#     train_clean = train_df.copy()\n",
    "#     train_clean[\"title\"] = train_clean[\"title\"].fillna(\"\")\n",
    "#     train_clean[\"user_interests\"] = train_clean[\"user_interests\"].fillna(\"\")\n",
    "    \n",
    "#     # Clean and prepare validation data\n",
    "#     val_clean = val_df.copy()\n",
    "#     val_clean[\"title\"] = val_clean[\"title\"].fillna(\"\")\n",
    "#     val_clean[\"user_interests\"] = val_clean[\"user_interests\"].fillna(\"\")\n",
    "\n",
    "#     # TF-IDF features - fit only on training data\n",
    "#     tfidf_title = TfidfVectorizer(max_features=100)\n",
    "#     tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "#     tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "#     tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    \n",
    "#     # Transform validation data using fitted vectorizers\n",
    "#     tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "#     tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "\n",
    "#     # Numeric features\n",
    "#     numeric_cols = [\"distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "#     train_clean[numeric_cols] = train_clean[numeric_cols].fillna(0)\n",
    "#     val_clean[numeric_cols] = val_clean[numeric_cols].fillna(0)\n",
    "    \n",
    "#     # Fit scaler on training data only\n",
    "#     scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "#     X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "#     X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    \n",
    "#     # Categorical features with OneHotEncoder\n",
    "#     categorical_cols = ['events_weather_condition', 'events_event_indoor_capability', 'users_user_weather_preference']\n",
    "    \n",
    "#     # Fill missing values in categorical columns\n",
    "#     for col in categorical_cols:\n",
    "#         train_clean[col] = train_clean[col].fillna('unknown')\n",
    "#         val_clean[col] = val_clean[col].fillna('unknown')\n",
    "    \n",
    "#     # Create OneHotEncoder for categorical features\n",
    "#     encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "#     X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "#     X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "\n",
    "#     # Combine all features\n",
    "#     X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "#     X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    \n",
    "#     y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "#     y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "\n",
    "#     # Train CatBoost on training data only\n",
    "#     model = CatBoostClassifier(\n",
    "#         iterations=200,\n",
    "#         depth=6,\n",
    "#         learning_rate=0.1,\n",
    "#         loss_function='Logloss',\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on validation data\n",
    "#     val_scores = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "#     return model, compute_all_metrics(y_val, val_scores)\n",
    "\n",
    "# def train_svd(train_df, val_df):\n",
    "#     # Prepare training data for SVD\n",
    "#     reader = Reader(rating_scale=(0, 1))\n",
    "#     train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    \n",
    "#     # Convert all to strings to ensure compatibility\n",
    "#     train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "#     train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "#     train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    \n",
    "#     data = Dataset.load_from_df(train_svd, reader)\n",
    "#     trainset = data.build_full_trainset()\n",
    "    \n",
    "#     # Train SVD model on training data only\n",
    "#     svd = SVD(n_epochs=50).fit(trainset)\n",
    "    \n",
    "#     # Generate predictions for validation data\n",
    "#     val_copy = val_df.copy()\n",
    "#     val_copy[\"svd_score\"] = val_copy.apply(\n",
    "#         lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, \n",
    "#         axis=1\n",
    "#     )\n",
    "    \n",
    "#     return svd, compute_all_metrics(val_copy[\"interaction_label\"].astype(int), val_copy[\"svd_score\"])\n",
    "\n",
    "# def hybrid_model(train_df, val_df):\n",
    "#     from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "#     # Step 1: Train SVD model on train_df\n",
    "#     reader = Reader(rating_scale=(0, 1))\n",
    "#     train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "#     trainset = train_svd_data.build_full_trainset()\n",
    "#     svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "\n",
    "#     # Step 2: Compute SVD scores for both train and val\n",
    "#     train_df = train_df.copy()\n",
    "#     val_df = val_df.copy()\n",
    "#     train_df[\"svd_score\"] = train_df.apply(\n",
    "#         lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "#     )\n",
    "#     val_df[\"svd_score\"] = val_df.apply(\n",
    "#         lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "#     )\n",
    "\n",
    "#     # Step 3: TF-IDF + numeric features\n",
    "#     tfidf_title = TfidfVectorizer(max_features=50)\n",
    "#     tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "#     tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "#     tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "\n",
    "#     X_train_text = hstack([\n",
    "#         tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "#         tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "#     ])\n",
    "#     X_val_text = hstack([\n",
    "#         tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "#         tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "#     ])\n",
    "\n",
    "#     # Numeric features including SVD score\n",
    "#     numeric_cols = [\"distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "#     train_df[numeric_cols] = train_df[numeric_cols].fillna(0)\n",
    "#     val_df[numeric_cols] = val_df[numeric_cols].fillna(0)\n",
    "\n",
    "#     scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "#     X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "#     X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    \n",
    "#     # Categorical features with OneHotEncoder\n",
    "#     categorical_cols = ['events_weather_condition', 'events_event_indoor_capability', 'users_user_weather_preference']\n",
    "    \n",
    "#     # Fill missing values in categorical columns\n",
    "#     for col in categorical_cols:\n",
    "#         train_df[col] = train_df[col].fillna('unknown')\n",
    "#         val_df[col] = val_df[col].fillna('unknown')\n",
    "    \n",
    "#     # Create OneHotEncoder for categorical features\n",
    "#     encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "#     X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "#     X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "\n",
    "#     # Combine all features\n",
    "#     X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "#     X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    \n",
    "#     y_train = train_df[\"interaction_label\"].astype(int)\n",
    "#     y_val = val_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "#     model = CatBoostClassifier(\n",
    "#         iterations=200,\n",
    "#         depth=6,\n",
    "#         learning_rate=0.1,\n",
    "#         loss_function='Logloss',\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     scores = model.predict_proba(X_val)[:, 1]\n",
    "#     return model, compute_all_metrics(y_val, scores)\n",
    "# '✅ Hybrid model updated to use CatBoost for better non-linear modeling.'\n",
    "\n",
    "\n",
    "# def compare_all_models(train_df, val_df):\n",
    "#     train_df = train_df.copy()\n",
    "#     val_df = val_df.copy()\n",
    "\n",
    "#     # Ensure correct types and fill missing\n",
    "#     for df in [train_df, val_df]:\n",
    "#         df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "#         df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "#         df[\"interaction_label\"] = df[\"interaction_label\"].astype(int)\n",
    "#         df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "#         df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "        \n",
    "#         # Ensure categorical columns exist\n",
    "#         for col in ['events_weather_condition', 'events_event_indoor_capability', 'users_user_weather_preference']:\n",
    "#             if col not in df.columns:\n",
    "#                 df[col] = \"unknown\"\n",
    "\n",
    "#     print(\"Training Content-Based model...\")\n",
    "#     _, content_scores = train_content_model(train_df, val_df)\n",
    "\n",
    "#     print(\"Training SVD model...\")\n",
    "#     _, svd_scores = train_svd(train_df, val_df)\n",
    "\n",
    "#     print(\"Training Hybrid (SVD + Content + Weather) model with strict validation...\")\n",
    "#     _, hybrid_scores = hybrid_model(train_df, val_df)\n",
    "\n",
    "#     # Assemble final results\n",
    "#     results_df = pd.DataFrame({\n",
    "#         \"Content-Based\": content_scores,\n",
    "#         \"SVD\": svd_scores,\n",
    "#         \"Hybrid\": hybrid_scores,\n",
    "#     })\n",
    "\n",
    "#     return results_df.T\n",
    "\n",
    "# compare_all_models(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training content-based model...\n",
      "Training SVD model...\n",
      "Training hybrid model...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "from surprise import Dataset, Reader, SVD\n",
    "# ====================== COMMON EVALUATION FRAMEWORK ======================\n",
    "class RecommenderEvaluator:\n",
    "    def __init__(self, events_df, max_distance_km=300, weather_threshold=0.3):\n",
    "        self.events_df = events_df\n",
    "        self.max_distance = max_distance_km\n",
    "        self.weather_thresh = weather_threshold\n",
    "        \n",
    "    def compute_weather_match(self, user_weather, event_weather, user_pref, is_indoor, \n",
    "                            user_temp, event_temp, user_precip, event_precip):\n",
    "        \"\"\"Identical weather scoring to target model\"\"\"\n",
    "        score = 0.0\n",
    "        if user_weather == event_weather:\n",
    "            score += 0.4\n",
    "        elif (user_weather in ['Sunny','Cloudy','Clear','Partly Cloudy'] and \n",
    "              event_weather in ['Sunny','Cloudy','Clear','Partly Cloudy']):\n",
    "            score += 0.2\n",
    "        elif (user_weather in ['Rain','Light Drizzle','Heavy Rain','Thunderstorm'] and \n",
    "              event_weather in ['Rain','Light Drizzle','Heavy Rain','Thunderstorm']):\n",
    "            score += 0.2\n",
    "            \n",
    "        if user_pref == 'any':\n",
    "            score += 0.3\n",
    "        elif (user_pref == 'indoor' and is_indoor) or (user_pref == 'outdoor' and not is_indoor):\n",
    "            score += 0.3\n",
    "            \n",
    "        temp_diff = abs(user_temp - event_temp)\n",
    "        precip_diff = abs(user_precip - event_precip)\n",
    "        temp_score = max(0, 1 - (temp_diff / 20))\n",
    "        precip_score = max(0, 1 - (precip_diff / 10))\n",
    "        score += 0.3 * (0.6 * temp_score + 0.4 * precip_score)\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def filter_candidates(self, user_data, candidates):\n",
    "        \"\"\"Apply weather/distance filtering\"\"\"\n",
    "        if 'interaction_distance_to_event' not in candidates:\n",
    "            candidates = candidates.merge(\n",
    "                user_data[['event_id', 'interaction_distance_to_event']].drop_duplicates(),\n",
    "                on='event_id', how='left'\n",
    "            )\n",
    "            \n",
    "        # Distance filter\n",
    "        candidates = candidates[candidates['interaction_distance_to_event'] <= self.max_distance]\n",
    "        \n",
    "        # Weather scoring\n",
    "        weather_scores = []\n",
    "        for _, event in candidates.iterrows():\n",
    "            score = self.compute_weather_match(\n",
    "                user_data['user_weather_preference'].iloc[0],\n",
    "                event.get('weather_condition', 'any'),\n",
    "                user_data['user_weather_preference'].iloc[0],\n",
    "                event['event_indoor_capability'],\n",
    "                user_data.get('temperature', 20),\n",
    "                event.get('temperature', 20),\n",
    "                user_data.get('precipitation', 0),\n",
    "                event.get('precipitation', 0)\n",
    "            )\n",
    "            weather_scores.append(score)\n",
    "            \n",
    "        candidates = candidates.assign(weather_score=weather_scores)\n",
    "        return candidates[candidates['weather_score'] >= self.weather_thresh]\n",
    "    \n",
    "    def evaluate(self, model, test_df, model_type='content', k_list=[1,5,10]):\n",
    "        \"\"\"Unified evaluation for all model types\"\"\"\n",
    "        metrics = {f'{m}@{k}':[] for m in ['P','R','NDCG'] for k in k_list}\n",
    "        metrics['MRR'] = []\n",
    "        \n",
    "        for user_id, user_data in tqdm(test_df.groupby('user_id'), desc=f\"Evaluating {model_type}\"):\n",
    "            # Generate candidates (positives + sampled negatives)\n",
    "            positives = user_data[user_data['interaction_label'] == 1]\n",
    "            if len(positives) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get all possible negatives\n",
    "            negative_pool = self.events_df[\n",
    "                ~self.events_df['event_id'].isin(positives['event_id'])\n",
    "            ]\n",
    "            \n",
    "            # Sample negatives (handle case where pool is smaller than 100)\n",
    "            n_sample = min(100, len(negative_pool))\n",
    "            negatives = negative_pool.sample(n_sample) if n_sample > 0 else pd.DataFrame()\n",
    "            \n",
    "            if len(negatives) == 0:\n",
    "                candidates = positives[['event_id','interaction_label']].merge(\n",
    "                    self.events_df, on='event_id')\n",
    "            else:\n",
    "                candidates = pd.concat([\n",
    "                    positives[['event_id','interaction_label']],\n",
    "                    negatives[['event_id']].assign(interaction_label=0)\n",
    "                ]).merge(self.events_df, on='event_id')\n",
    "            \n",
    "            # Apply filters\n",
    "            candidates = self.filter_candidates(user_data, candidates)\n",
    "            if len(candidates) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get predictions based on model type\n",
    "            if model_type == 'content':\n",
    "                scores = self._predict_content(model, user_data, candidates)\n",
    "            elif model_type == 'svd':\n",
    "                scores = self._predict_svd(model, user_data, candidates)\n",
    "            else:  # hybrid\n",
    "                scores = self._predict_hybrid(model, user_data, candidates)\n",
    "                \n",
    "            # Compute metrics\n",
    "            y_true = candidates['interaction_label'].values\n",
    "            ranked_idx = np.argsort(scores)[::-1]\n",
    "            y_true_sorted = y_true[ranked_idx]\n",
    "            \n",
    "            for k in k_list:\n",
    "                rel = y_true_sorted[:k]\n",
    "                metrics[f'P@{k}'].append(np.sum(rel) / k)\n",
    "                metrics[f'R@{k}'].append(np.sum(rel) / np.sum(y_true))\n",
    "                metrics[f'NDCG@{k}'].append(ndcg_score([y_true], [scores], k=k))\n",
    "                \n",
    "            # MRR\n",
    "            pos_ranks = np.where(y_true_sorted == 1)[0]\n",
    "            metrics['MRR'].append(1/(pos_ranks[0]+1) if len(pos_ranks) > 0 else 0)\n",
    "        \n",
    "        return {k:np.mean(v) for k,v in metrics.items()}\n",
    "    \n",
    "    def _predict_content(self, model, user_data, candidates):\n",
    "        \"\"\"Content-based prediction\"\"\"\n",
    "        # Assume model contains: tfidf_title, tfidf_interests, scaler, encoder, cb_model\n",
    "        X_text = hstack([\n",
    "            model['tfidf_title'].transform(candidates['title'].fillna(\"\")),\n",
    "            model['tfidf_interests'].transform(\n",
    "                [user_data['user_interests'].iloc[0]] * len(candidates))\n",
    "        ])\n",
    "        \n",
    "        X_num = model['scaler'].transform(candidates[\n",
    "            ['interaction_distance_to_event', 'temperature', 'age', 'attendance_rate']\n",
    "        ])\n",
    "        X_cat = model['encoder'].transform(candidates[\n",
    "            ['weather_condition', 'events_event_indoor_capability', \n",
    "             'user_weather_preference']\n",
    "        ])\n",
    "        \n",
    "        return model['cb_model'].predict_proba(\n",
    "            hstack([X_text, X_num, X_cat]).toarray()\n",
    "        )[:, 1]\n",
    "    \n",
    "    def _predict_svd(self, model, user_data, candidates):\n",
    "        \"\"\"SVD prediction\"\"\"\n",
    "        return candidates.apply(\n",
    "            lambda x: model.predict(str(user_data['user_id'].iloc[0]), \n",
    "                                  str(x['event_id'])).est,\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    def _predict_hybrid(self, model, user_data, candidates):\n",
    "        \"\"\"Hybrid prediction\"\"\"\n",
    "        # Get SVD scores\n",
    "        svd_scores = candidates.apply(\n",
    "            lambda x: model['svd'].predict(str(user_data['user_id'].iloc[0]), \n",
    "                                         str(x['event_id'])).est,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Get content features\n",
    "        X_text = hstack([\n",
    "            model['tfidf_title'].transform(candidates['title'].fillna(\"\")),\n",
    "            model['tfidf_interests'].transform(\n",
    "                [user_data['user_interests'].iloc[0]] * len(candidates))\n",
    "        ])\n",
    "        X_num = model['scaler'].transform(\n",
    "            candidates[['distance_to_event', 'temperature', 'age', 'attendance_rate']]\n",
    "            .assign(svd_score=svd_scores)  # Include SVD score as feature\n",
    "        )\n",
    "        X_cat = model['encoder'].transform(candidates[\n",
    "            ['weather_condition', 'events_event_indoor_capability', \n",
    "             'user_weather_preference']\n",
    "        ])\n",
    "        \n",
    "        return model['cb_model'].predict_proba(\n",
    "            hstack([X_text, X_num, X_cat]).toarray()\n",
    "        )[:, 1]\n",
    "\n",
    "# ====================== EVALUATION EXECUTION ======================\n",
    "def evaluate_all_models(test_df, events_df, models):\n",
    "    \"\"\"Evaluate all baseline models on real-world data\"\"\"\n",
    "    evaluator = RecommenderEvaluator(events_df)\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model_type = 'content' if 'cb_model' in model else \\\n",
    "                    'svd' if isinstance(model, SVD) else 'hybrid'\n",
    "        results[name] = evaluator.evaluate(model, test_df, model_type)\n",
    "    \n",
    "    return pd.DataFrame(results).T.round(4)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_save_path=\"/home/nkama/masters_thesis_project/thesis/models\"\n",
    "\n",
    "def train_and_save_models(train_df, model_save_path=model_save_path):\n",
    "    \"\"\"Train all baseline models and return them in evaluation-ready format\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # ===== 1. Train Content-Based Model =====\n",
    "    print(\"Training content-based model...\")\n",
    "    # Feature engineering\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat = tfidf_title.fit_transform(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests_mat = tfidf_interests.fit_transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    \n",
    "    # Numeric and categorical\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols].fillna(0))\n",
    "    \n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', \n",
    "                       'user_weather_preference']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "    encoder.fit(train_df[categorical_cols].fillna('unknown'))\n",
    "    \n",
    "    # Combine features\n",
    "    X_train = hstack([\n",
    "        tfidf_title_mat,\n",
    "        tfidf_interests_mat,\n",
    "        scaler.transform(train_df[numeric_cols].fillna(0)),\n",
    "        encoder.transform(train_df[categorical_cols].fillna('unknown'))\n",
    "    ]).toarray()\n",
    "    \n",
    "    # Train CatBoost\n",
    "    cb_model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        verbose=False\n",
    "    )\n",
    "    cb_model.fit(X_train, train_df[\"interaction_label\"].astype(int))\n",
    "    \n",
    "    # Package content model\n",
    "    models['content'] = {\n",
    "        'tfidf_title': tfidf_title,\n",
    "        'tfidf_interests': tfidf_interests,\n",
    "        'scaler': scaler,\n",
    "        'encoder': encoder,\n",
    "        'cb_model': cb_model\n",
    "    }\n",
    "    \n",
    "    # ===== 2. Train SVD Model =====\n",
    "    print(\"Training SVD model...\")\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    \n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    svd_model = SVD(n_epochs=50)\n",
    "    svd_model.fit(trainset)\n",
    "    models['svd'] = svd_model\n",
    "    \n",
    "        # ===== 3. Train Hybrid Model =====\n",
    "    print(\"Training hybrid model...\")\n",
    "    # First get SVD scores for training data\n",
    "    train_df = train_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create new scaler for hybrid model that includes svd_score\n",
    "    numeric_cols_hybrid = numeric_cols + [\"svd_score\"]\n",
    "    scaler_hybrid = StandardScaler().fit(train_df[numeric_cols_hybrid].fillna(0))\n",
    "    \n",
    "    # TF-IDF features (reuse from content model)\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    \n",
    "    # Numeric features with new scaler\n",
    "    X_train_numeric = scaler_hybrid.transform(train_df[numeric_cols_hybrid].fillna(0))\n",
    "    \n",
    "    # Categorical features (reuse encoder)\n",
    "    X_cat_train = encoder.transform(train_df[categorical_cols].fillna('unknown'))\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_hybrid = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    \n",
    "    # Train hybrid CatBoost\n",
    "    hybrid_cb = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        verbose=False\n",
    "    )\n",
    "    hybrid_cb.fit(X_train_hybrid, train_df[\"interaction_label\"].astype(int))\n",
    "    \n",
    "    # Package hybrid model\n",
    "    models['hybrid'] = {\n",
    "        'svd': svd_model,\n",
    "        'tfidf_title': tfidf_title,\n",
    "        'tfidf_interests': tfidf_interests,\n",
    "        'scaler': scaler_hybrid,  # Use the new scaler\n",
    "        'encoder': encoder,\n",
    "        'cb_model': hybrid_cb\n",
    "    }\n",
    "    \n",
    "    # ===== Save Models =====\n",
    "    if model_save_path:\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        for name, model in models.items():\n",
    "            with open(f\"{model_save_path}/{name}_model.pkl\", \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Example Usage:\n",
    "models = train_and_save_models(train_df)\n",
    "# Then evaluate on real-world data:\n",
    "results = evaluate_all_models(test_df, events_df, models)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training content-based model...\n",
      "Training SVD model...\n",
      "Training hybrid model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating content: 100%|██████████| 5315/5315 [02:23<00:00, 37.14it/s] \n",
      "Evaluating svd: 100%|██████████| 5315/5315 [03:56<00:00, 22.44it/s] \n",
      "Evaluating hybrid: 100%|██████████| 5315/5315 [04:10<00:00, 21.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         P@1  P@5  P@10  P@50  P@100  R@1  R@5  R@10  R@50  R@100  NDCG@1  \\\n",
      "content  NaN  NaN   NaN   NaN    NaN  NaN  NaN   NaN   NaN    NaN     NaN   \n",
      "svd      NaN  NaN   NaN   NaN    NaN  NaN  NaN   NaN   NaN    NaN     NaN   \n",
      "hybrid   NaN  NaN   NaN   NaN    NaN  NaN  NaN   NaN   NaN    NaN     NaN   \n",
      "\n",
      "         NDCG@5  NDCG@10  NDCG@50  NDCG@100  MRR  \n",
      "content     NaN      NaN      NaN       NaN  NaN  \n",
      "svd         NaN      NaN      NaN       NaN  NaN  \n",
      "hybrid      NaN      NaN      NaN       NaN  NaN  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "from surprise import Dataset, Reader, SVD\n",
    "# ====================== COMMON EVALUATION FRAMEWORK ======================\n",
    "class RecommenderEvaluator:\n",
    "    def __init__(self, events_df, max_distance_km=2000, weather_threshold=0.2):\n",
    "        self.events_df = events_df\n",
    "        self.max_distance = max_distance_km\n",
    "        self.weather_thresh = weather_threshold\n",
    "        \n",
    "    def compute_weather_match(self, user_weather, event_weather, user_pref, is_indoor, \n",
    "                            user_temp, event_temp, user_precip, event_precip):\n",
    "        \"\"\"Identical weather scoring to target model\"\"\"\n",
    "        score = 0.0\n",
    "        if user_weather == event_weather:\n",
    "            score += 0.4\n",
    "        elif (user_weather in ['Sunny','Cloudy','Clear','Partly Cloudy'] and \n",
    "              event_weather in ['Sunny','Cloudy','Clear','Partly Cloudy']):\n",
    "            score += 0.2\n",
    "        elif (user_weather in ['Rain','Light Drizzle','Heavy Rain','Thunderstorm'] and \n",
    "              event_weather in ['Rain','Light Drizzle','Heavy Rain','Thunderstorm']):\n",
    "            score += 0.2\n",
    "            \n",
    "        if user_pref == 'any':\n",
    "            score += 0.3\n",
    "        elif (user_pref == 'indoor' and is_indoor) or (user_pref == 'outdoor' and not is_indoor):\n",
    "            score += 0.3\n",
    "            \n",
    "        temp_diff = abs(user_temp - event_temp)\n",
    "        precip_diff = abs(user_precip - event_precip)\n",
    "        temp_score = max(0, 1 - (temp_diff / 20))\n",
    "        precip_score = max(0, 1 - (precip_diff / 10))\n",
    "        score += 0.3 * (0.6 * temp_score + 0.4 * precip_score)\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def filter_candidates(self, user_data, candidates):\n",
    "        \"\"\"Apply weather/distance filtering\"\"\"\n",
    "        if 'interaction_distance_to_event' not in candidates:\n",
    "            candidates = candidates.merge(\n",
    "                user_data[['event_id', 'interaction_distance_to_event']].drop_duplicates(),\n",
    "                on='event_id', how='left'\n",
    "            )\n",
    "            \n",
    "        # Distance filter\n",
    "        candidates = candidates[candidates['interaction_distance_to_event'] <= self.max_distance]\n",
    "        \n",
    "        # Weather scoring\n",
    "        weather_scores = []\n",
    "        for _, event in candidates.iterrows():\n",
    "            score = self.compute_weather_match(\n",
    "                user_data['user_weather_preference'].iloc[0],\n",
    "                event.get('weather_condition', 'any'),\n",
    "                user_data['user_weather_preference'].iloc[0],\n",
    "                event['event_indoor_capability'],\n",
    "                user_data.get('temperature', 20),\n",
    "                event.get('temperature', 20),\n",
    "                user_data.get('precipitation', 0),\n",
    "                event.get('precipitation', 0)\n",
    "            )\n",
    "            weather_scores.append(score)\n",
    "        candidates = candidates.assign(weather_score=weather_scores)\n",
    "        return candidates[candidates['weather_score'] >= self.weather_thresh]\n",
    "    \n",
    "    def evaluate(self, model, test_df, model_type='content', k_list=[1,5,10]):\n",
    "        \"\"\"Unified evaluation for all model types\"\"\"\n",
    "        metrics = {f'{m}@{k}':[] for m in ['P','R','NDCG'] for k in k_list}\n",
    "        metrics['MRR'] = []\n",
    "        \n",
    "        for user_id, user_data in tqdm(test_df.groupby('user_id'), desc=f\"Evaluating {model_type}\"):\n",
    "            # Generate candidates (positives + sampled negatives)\n",
    "            positives = user_data[user_data['interaction_label'] == 1]\n",
    "            if len(positives) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get all possible negatives\n",
    "            negative_pool = self.events_df[\n",
    "                ~self.events_df['event_id'].isin(positives['event_id'])\n",
    "            ]\n",
    "            \n",
    "            # Sample negatives (handle case where pool is smaller than 100)\n",
    "            n_sample = min(100, len(negative_pool))\n",
    "            negatives = negative_pool.sample(n_sample) if n_sample > 0 else pd.DataFrame()\n",
    "            \n",
    "            if len(negatives) == 0:\n",
    "                candidates = positives[['event_id','interaction_label']].merge(\n",
    "                    self.events_df, on='event_id')\n",
    "            else:\n",
    "                candidates = pd.concat([\n",
    "                    positives[['event_id','interaction_label']],\n",
    "                    negatives[['event_id']].assign(interaction_label=0)\n",
    "                ]).merge(self.events_df, on='event_id')\n",
    "            \n",
    "            # Apply filters\n",
    "            candidates = self.filter_candidates(user_data, candidates)\n",
    "            # Add this inside evaluate() after filtering candidates\n",
    "\n",
    "            if len(candidates) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get predictions based on model type\n",
    "            if model_type == 'content':\n",
    "                scores = self._predict_content(model, user_data, candidates)\n",
    "            elif model_type == 'svd':\n",
    "                scores = self._predict_svd(model, user_data, candidates)\n",
    "            else:  # hybrid\n",
    "                scores = self._predict_hybrid(model, user_data, candidates)\n",
    "                \n",
    "            # Compute metrics\n",
    "            y_true = candidates['interaction_label'].values\n",
    "            if np.sum(y_true) == 0:\n",
    "                print(f\"User {user_id} has no positive examples after filtering\")\n",
    "                continue\n",
    "            ranked_idx = np.argsort(scores)[::-1]\n",
    "            y_true_sorted = y_true[ranked_idx]\n",
    "            \n",
    "            for k in k_list:\n",
    "                rel = y_true_sorted[:k]\n",
    "                metrics[f'P@{k}'].append(np.sum(rel) / k)\n",
    "                metrics[f'R@{k}'].append(np.sum(rel) / np.sum(y_true))\n",
    "                metrics[f'NDCG@{k}'].append(ndcg_score([y_true], [scores], k=k))\n",
    "                \n",
    "            # MRR\n",
    "            pos_ranks = np.where(y_true_sorted == 1)[0]\n",
    "            metrics['MRR'].append(1/(pos_ranks[0]+1) if len(pos_ranks) > 0 else 0)\n",
    "        \n",
    "        return {k:np.mean(v) for k,v in metrics.items()}\n",
    "    \n",
    "    def _predict_content(self, model, user_data, candidates):\n",
    "        \"\"\"Content-based prediction\"\"\"\n",
    "        # Assume model contains: tfidf_title, tfidf_interests, scaler, encoder, cb_model\n",
    "        X_text = hstack([\n",
    "            model['tfidf_title'].transform(candidates['title'].fillna(\"\")),\n",
    "            model['tfidf_interests'].transform(\n",
    "                [user_data['user_interests'].iloc[0]] * len(candidates))\n",
    "        ])\n",
    "        \n",
    "        X_num = model['scaler'].transform(candidates[\n",
    "            ['interaction_distance_to_event', 'temperature', 'age', 'attendance_rate']\n",
    "        ].fillna(0))\n",
    "        \n",
    "        X_cat = model['encoder'].transform(candidates[\n",
    "            ['weather_condition', 'event_indoor_capability', \n",
    "             'user_weather_preference']\n",
    "        ].fillna('unknown'))\n",
    "        \n",
    "        return model['cb_model'].predict_proba(\n",
    "            hstack([X_text, X_num, X_cat]).toarray()\n",
    "        )[:, 1]\n",
    "    \n",
    "    def _predict_svd(self, model, user_data, candidates):\n",
    "        \"\"\"SVD prediction\"\"\"\n",
    "        return candidates.apply(\n",
    "            lambda x: model.predict(str(user_data['user_id'].iloc[0]), \n",
    "                                  str(x['event_id'])).est,\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    def _predict_hybrid(self, model, user_data, candidates):\n",
    "        \"\"\"Hybrid prediction\"\"\"\n",
    "        # Get SVD scores\n",
    "        svd_scores = candidates.apply(\n",
    "            lambda x: model['svd'].predict(str(user_data['user_id'].iloc[0]), \n",
    "                                         str(x['event_id'])).est,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Get content features\n",
    "        X_text = hstack([\n",
    "            model['tfidf_title'].transform(candidates['title'].fillna(\"\")),\n",
    "            model['tfidf_interests'].transform(\n",
    "                [user_data['user_interests'].iloc[0]] * len(candidates))\n",
    "        ])\n",
    "        \n",
    "        # Correct column name from distance_to_event to interaction_distance_to_event\n",
    "        X_num = model['scaler'].transform(\n",
    "            candidates[['interaction_distance_to_event', 'temperature', 'age', 'attendance_rate']]\n",
    "            .assign(svd_score=svd_scores)  # Include SVD score as feature\n",
    "            .fillna(0)\n",
    "        )\n",
    "        \n",
    "        X_cat = model['encoder'].transform(candidates[\n",
    "            ['weather_condition', 'event_indoor_capability', \n",
    "             'user_weather_preference']\n",
    "        ].fillna('unknown'))\n",
    "        \n",
    "        return model['cb_model'].predict_proba(\n",
    "            hstack([X_text, X_num, X_cat]).toarray()\n",
    "        )[:, 1]\n",
    "\n",
    "# ====================== EVALUATION EXECUTION ======================\n",
    "def evaluate_all_models(test_df, events_df, models):\n",
    "    \"\"\"Evaluate all baseline models on real-world data\"\"\"\n",
    "    evaluator = RecommenderEvaluator(events_df)\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model_type = 'svd' if isinstance(model, SVD) else \\\n",
    "            'content' if isinstance(model, dict) and 'cb_model' in model and 'svd' not in model else 'hybrid'\n",
    "\n",
    "        results[name] = evaluator.evaluate(model, test_df, model_type, k_list=[1, 5, 10, 50, 100])\n",
    "    \n",
    "    # Create DataFrame and exclude AUC/MAP scores\n",
    "    results_df = pd.DataFrame(results).T.round(4)\n",
    "    return results_df\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_save_path=\"/home/nkama/masters_thesis_project/thesis/models\"\n",
    "\n",
    "def train_and_save_models(train_df, model_save_path=model_save_path):\n",
    "    \"\"\"Train all baseline models and return them in evaluation-ready format\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # ===== 1. Train Content-Based Model =====\n",
    "    print(\"Training content-based model...\")\n",
    "    # Feature engineering\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat = tfidf_title.fit_transform(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests_mat = tfidf_interests.fit_transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    \n",
    "    # Numeric and categorical\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols].fillna(0))\n",
    "    \n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', \n",
    "                       'user_weather_preference']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "    encoder.fit(train_df[categorical_cols].fillna('unknown'))\n",
    "    \n",
    "    # Combine features\n",
    "    X_train = hstack([\n",
    "        tfidf_title_mat,\n",
    "        tfidf_interests_mat,\n",
    "        scaler.transform(train_df[numeric_cols].fillna(0)),\n",
    "        encoder.transform(train_df[categorical_cols].fillna('unknown'))\n",
    "    ]).toarray()\n",
    "    \n",
    "    # Train CatBoost\n",
    "    cb_model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        verbose=False\n",
    "    )\n",
    "    cb_model.fit(X_train, train_df[\"interaction_label\"].astype(int))\n",
    "    \n",
    "    # Package content model\n",
    "    models['content'] = {\n",
    "        'tfidf_title': tfidf_title,\n",
    "        'tfidf_interests': tfidf_interests,\n",
    "        'scaler': scaler,\n",
    "        'encoder': encoder,\n",
    "        'cb_model': cb_model\n",
    "    }\n",
    "    \n",
    "    # ===== 2. Train SVD Model =====\n",
    "    print(\"Training SVD model...\")\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    \n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    svd_model = SVD(n_epochs=50)\n",
    "    svd_model.fit(trainset)\n",
    "    models['svd'] = svd_model\n",
    "    \n",
    "    # ===== 3. Train Hybrid Model =====\n",
    "    print(\"Training hybrid model...\")\n",
    "    # First get SVD scores for training data\n",
    "    train_df = train_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create new scaler for hybrid model that includes svd_score\n",
    "    numeric_cols_hybrid = numeric_cols + [\"svd_score\"]\n",
    "    scaler_hybrid = StandardScaler().fit(train_df[numeric_cols_hybrid].fillna(0))\n",
    "    \n",
    "    # TF-IDF features (reuse from content model)\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    \n",
    "    # Numeric features with new scaler\n",
    "    X_train_numeric = scaler_hybrid.transform(train_df[numeric_cols_hybrid].fillna(0))\n",
    "    \n",
    "    # Categorical features (reuse encoder)\n",
    "    X_cat_train = encoder.transform(train_df[categorical_cols].fillna('unknown'))\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_hybrid = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    \n",
    "    # Train hybrid CatBoost\n",
    "    hybrid_cb = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        verbose=False\n",
    "    )\n",
    "    hybrid_cb.fit(X_train_hybrid, train_df[\"interaction_label\"].astype(int))\n",
    "    \n",
    "    # Package hybrid model\n",
    "    models['hybrid'] = {\n",
    "        'svd': svd_model,\n",
    "        'tfidf_title': tfidf_title,\n",
    "        'tfidf_interests': tfidf_interests,\n",
    "        'scaler': scaler_hybrid,  # Use the new scaler\n",
    "        'encoder': encoder,\n",
    "        'cb_model': hybrid_cb\n",
    "    }\n",
    "    \n",
    "    # ===== Save Models =====\n",
    "    if model_save_path:\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        for name, model in models.items():\n",
    "            with open(f\"{model_save_path}/{name}_model.pkl\", \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Example Usage:\n",
    "models = train_and_save_models(train_df)\n",
    "#Then evaluate on real-world data:\n",
    "results = evaluate_all_models(test_df, events_df, models)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-18 01:57:38,184 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-05-18 01:57:38,189 INFO: Initializing external client\n",
      "2025-05-18 01:57:38,190 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-18 01:57:39,385 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hsml.deployment_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m mr \u001b[38;5;241m=\u001b[39m project\u001b[38;5;241m.\u001b[39mget_model_registry()  \u001b[38;5;66;03m# Get the model registry\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ranking_model \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mget_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_ranking_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Get the model \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhsml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployment_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeploymentConfig\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create deployment configuration with environment variables\u001b[39;00m\n\u001b[1;32m     13\u001b[0m deployment_config \u001b[38;5;241m=\u001b[39m DeploymentConfig(\n\u001b[1;32m     14\u001b[0m     environment\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSKLEARN_SERVER_EXTRA_REQUIREMENTS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcatboost==1.2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hsml.deployment_config'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# Connect to Hopsworks\n",
    "project = hopsworks.login()\n",
    "ms = project.get_model_serving()\n",
    "mr = project.get_model_registry()  # Get the model registry\n",
    "ranking_model = mr.get_model(\"weather_ranking_model\", version=1) # Get the model \n",
    "\n",
    "\n",
    "from hsml.deployment_config import DeploymentConfig\n",
    "\n",
    "# Create deployment configuration with environment variables\n",
    "deployment_config = DeploymentConfig(\n",
    "    environment={\"SKLEARN_SERVER_EXTRA_REQUIREMENTS\": \"catboost==1.2.1\"}\n",
    ")\n",
    "\n",
    "# Deploy with configuration\n",
    "ranking_deployment = ranking_model.deploy(\n",
    "    name=\"weathermodel\",\n",
    "    deployment_config=deployment_config\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
