{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:08:21,387 INFO: Initializing external client\n",
      "2025-05-26 22:08:21,389 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-26 22:08:22,944 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.14s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.23s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (7.10s) \n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "# Connect to Hopsworks Feature Store\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "# Load from Hopsworks feature groups\n",
    "users_fg = fs.get_feature_group(name=\"users\", version=1)\n",
    "events_fg = fs.get_feature_group(name=\"events\", version=1)\n",
    "interactions_fg = fs.get_feature_group(name=\"interactions\", version=1)\n",
    "\n",
    "#read data from hopsworks\n",
    "users_df = users_fg.read()\n",
    "events_df = events_fg.read()\n",
    "interactions_df =interactions_fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lon</th>\n",
       "      <th>user_city</th>\n",
       "      <th>indoor_outdoor_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>user_interests</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>social_connectedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RC268C</td>\n",
       "      <td>40.962648</td>\n",
       "      <td>-73.613922</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>53</td>\n",
       "      <td>sports fashion fitness</td>\n",
       "      <td>2024-09-15 17:37:26.277721+00:00</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH989Z</td>\n",
       "      <td>35.366580</td>\n",
       "      <td>139.371354</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>21</td>\n",
       "      <td>art literature travel fitness</td>\n",
       "      <td>2024-09-13 07:24:04.467340+00:00</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RA922W</td>\n",
       "      <td>35.851979</td>\n",
       "      <td>139.316280</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>45</td>\n",
       "      <td>fitness tech food art</td>\n",
       "      <td>2025-02-14 21:46:33.410035+00:00</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EE813A</td>\n",
       "      <td>40.518719</td>\n",
       "      <td>-74.162980</td>\n",
       "      <td>New York</td>\n",
       "      <td>any</td>\n",
       "      <td>55</td>\n",
       "      <td>sports fitness art</td>\n",
       "      <td>2024-02-20 06:27:12.752684+00:00</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FX457B</td>\n",
       "      <td>43.796518</td>\n",
       "      <td>-79.489105</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>any</td>\n",
       "      <td>42</td>\n",
       "      <td>music food</td>\n",
       "      <td>2023-10-12 08:01:57.659010+00:00</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id   user_lat    user_lon user_city indoor_outdoor_preference  age  \\\n",
       "0  RC268C  40.962648  -73.613922  New York                   outdoor   53   \n",
       "1  OH989Z  35.366580  139.371354     Tokyo                   outdoor   21   \n",
       "2  RA922W  35.851979  139.316280     Tokyo                   outdoor   45   \n",
       "3  EE813A  40.518719  -74.162980  New York                       any   55   \n",
       "4  FX457B  43.796518  -79.489105   Toronto                       any   42   \n",
       "\n",
       "                  user_interests                      signup_date  \\\n",
       "0         sports fashion fitness 2024-09-15 17:37:26.277721+00:00   \n",
       "1  art literature travel fitness 2024-09-13 07:24:04.467340+00:00   \n",
       "2          fitness tech food art 2025-02-14 21:46:33.410035+00:00   \n",
       "3             sports fitness art 2024-02-20 06:27:12.752684+00:00   \n",
       "4                     music food 2023-10-12 08:01:57.659010+00:00   \n",
       "\n",
       "   social_connectedness  \n",
       "0                    13  \n",
       "1                    13  \n",
       "2                    18  \n",
       "3                    20  \n",
       "4                    20  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "      <th>interaction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MS115P</td>\n",
       "      <td>JQ744Q</td>\n",
       "      <td>WC393C</td>\n",
       "      <td>yes</td>\n",
       "      <td>2025-06-14 12:11:15.415927+00:00</td>\n",
       "      <td>42.471559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FA113Q</td>\n",
       "      <td>KD996K</td>\n",
       "      <td>BL473T</td>\n",
       "      <td>no</td>\n",
       "      <td>2025-06-14 19:38:16.109952+00:00</td>\n",
       "      <td>70.228559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IR265L</td>\n",
       "      <td>NT251J</td>\n",
       "      <td>LG400F</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>2025-08-03 05:49:34.516328+00:00</td>\n",
       "      <td>8.747134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MY826W</td>\n",
       "      <td>JT337Z</td>\n",
       "      <td>CW093D</td>\n",
       "      <td>no</td>\n",
       "      <td>2025-06-07 16:19:05.147064+00:00</td>\n",
       "      <td>90.449921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OZ813P</td>\n",
       "      <td>JX049Z</td>\n",
       "      <td>UO577O</td>\n",
       "      <td>maybe</td>\n",
       "      <td>2025-06-21 11:26:39.643373+00:00</td>\n",
       "      <td>36.451322</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interaction_id user_id event_id interaction_type  \\\n",
       "0         MS115P  JQ744Q   WC393C              yes   \n",
       "1         FA113Q  KD996K   BL473T               no   \n",
       "2         IR265L  NT251J   LG400F  invited & maybe   \n",
       "3         MY826W  JT337Z   CW093D               no   \n",
       "4         OZ813P  JX049Z   UO577O            maybe   \n",
       "\n",
       "                  interaction_time  interaction_distance_to_event  \\\n",
       "0 2025-06-14 12:11:15.415927+00:00                      42.471559   \n",
       "1 2025-06-14 19:38:16.109952+00:00                      70.228559   \n",
       "2 2025-08-03 05:49:34.516328+00:00                       8.747134   \n",
       "3 2025-06-07 16:19:05.147064+00:00                      90.449921   \n",
       "4 2025-06-21 11:26:39.643373+00:00                      36.451322   \n",
       "\n",
       "   interaction_label  \n",
       "0                  1  \n",
       "1                  0  \n",
       "2                  1  \n",
       "3                  0  \n",
       "4                  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# --- Metrics ---\n",
    "K_LIST = [1, 5, 10, 50, 100]\n",
    "\n",
    "def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    top_k = np.array(y_true)[sorted_indices][:k]\n",
    "    precision = np.mean(top_k)\n",
    "    recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "    dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "    ideal_k = min(int(np.sum(y_true)), k)\n",
    "    idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_score, k=100):\n",
    "    sorted_indices = np.argsort(y_score)[::-1][:k]\n",
    "    top_k = np.array(y_true)[sorted_indices]\n",
    "    ranks = np.where(top_k == 1)[0]\n",
    "    if len(ranks) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 / (ranks[0] + 1)\n",
    "\n",
    "def compute_all_metrics(y_true, y_score, k_list=K_LIST):\n",
    "    metrics = {}\n",
    "    for k in k_list:\n",
    "        p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "        metrics[f\"Precision@{k}\"] = p\n",
    "        metrics[f\"Recall@{k}\"] = r\n",
    "        metrics[f\"NDCG@{k}\"] = n\n",
    "        metrics[f\"MRR@{k}\"] = mean_reciprocal_rank(y_true, y_score, k)\n",
    "    return metrics\n",
    "\n",
    "# --- Model Training ---\n",
    "def train_content_model(train_df, val_df):\n",
    "    train_clean = train_df.copy()\n",
    "    val_clean = val_df.copy()\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "        df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "    tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "    tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "    X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "    X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'indoor_outdoor_preference']\n",
    "    for df in [train_clean, val_clean]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "    X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "    X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "    y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder\n",
    "\n",
    "def train_svd(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    svd = SVD(n_epochs=50).fit(trainset)\n",
    "    val_copy = val_df.copy()\n",
    "    val_copy[\"svd_score\"] = val_copy.apply(\n",
    "        lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_val = val_copy[\"interaction_label\"].astype(int)\n",
    "    val_scores = val_copy[\"svd_score\"]\n",
    "    return svd, compute_all_metrics(y_val, val_scores)\n",
    "\n",
    "def train_hybrid_model(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "    trainset = train_svd_data.build_full_trainset()\n",
    "    svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    val_df[\"svd_score\"] = val_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    for df in [train_df, val_df]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'indoor_outdoor_preference']\n",
    "    for df in [train_df, val_df]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "    X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    y_train = train_df[\"interaction_label\"].astype(int)\n",
    "    y_val = val_df[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder, svd_model\n",
    "\n",
    "# --- Test Set Evaluation ---\n",
    "def evaluate_content_or_hybrid_on_test(test_df, model, tfidf_title, tfidf_interests, scaler, encoder, model_type=\"content\", svd_model=None, k_list=K_LIST):\n",
    "    df = test_df.copy()\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    if model_type == \"hybrid\":\n",
    "        df[\"svd_score\"] = df.apply(\n",
    "            lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "        )\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    else:\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    tfidf_title_mat = tfidf_title.transform(df[\"title\"])\n",
    "    tfidf_interests_mat = tfidf_interests.transform(df[\"user_interests\"])\n",
    "    X_numeric = scaler.transform(df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'indoor_outdoor_preference']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    X_cat = encoder.transform(df[categorical_cols])\n",
    "    X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric, X_cat]).toarray()\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = model.predict_proba(X)[:, 1]\n",
    "    return compute_all_metrics(y_true, y_score, k_list=k_list)\n",
    "\n",
    "def evaluate_svd_on_test(test_df, svd_model, k_list=K_LIST):\n",
    "    df = test_df.copy()\n",
    "    df[\"svd_score\"] = df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = df[\"svd_score\"]\n",
    "    return compute_all_metrics(y_true, y_score, k_list=k_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real-world test data\n",
    "test_users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_users_data.csv')\n",
    "test_events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_events_data.csv\")\n",
    "test_interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_interactions_data.csv')\n",
    "\n",
    "# Rename columns in test data\n",
    "test_users_df.rename(columns={\n",
    "    'lat': 'user_lat',\n",
    "    'lng': 'user_lon',\n",
    "    'location': 'user_city',\n",
    "    'joinedAt': 'signup_date'\n",
    "}, inplace=True)\n",
    "\n",
    "test_events_df.rename(columns={\n",
    "    'category': 'event_type',\n",
    "    'lat': 'event_lat',\n",
    "    'lng': 'event_lon',\n",
    "    'city': 'event_city',\n",
    "    'weather_description': 'weather_condition',\n",
    "    'temperature_2m_mean': 'temperature'\n",
    "}, inplace=True)\n",
    "\n",
    "test_interactions_df.rename(columns={\n",
    "    'distance_to_event': 'interaction_distance_to_event'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess data\n",
    "def preprocess_common(interactions_df, users_df, events_df):\n",
    "    # Create copies of the input dataframes\n",
    "    interactions_df = interactions_df.copy()\n",
    "    users_df = users_df.copy()\n",
    "    events_df = events_df.copy()\n",
    "\n",
    "    # Drop rows with missing user_id or event_id\n",
    "    interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "    \n",
    "    # Convert distance_to_event to float\n",
    "    interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "    \n",
    "    # Ensure correct types\n",
    "    for df in [interactions_df, users_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    \n",
    "    for df in [interactions_df, events_df]:\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "    interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Convert to string type for TF-IDF fields\n",
    "    events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "    users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "    users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "    # Ensure all numeric fields are float\n",
    "    numeric_cols = [\"temperature\", \"attendance_rate\"]\n",
    "    for col in numeric_cols:\n",
    "        events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    # Return all three dataframes\n",
    "    return interactions_df, users_df, events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['interaction_id', 'user_id', 'event_id', 'interaction_type',\n",
       "       'interaction_time', 'interaction_distance_to_event',\n",
       "       'interaction_label', 'title', 'event_type', 'event_lat', 'event_lon',\n",
       "       'event_city', 'start_time', 'duration', 'weather_condition',\n",
       "       'temperature', 'attendance_rate', 'event_indoor_capability', 'user_lat',\n",
       "       'user_lon', 'user_city', 'indoor_outdoor_preference', 'age',\n",
       "       'user_interests', 'signup_date', 'social_connectedness'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess training/validation data\n",
    "interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "merged_df = interactions_df.merge(events_df, on=\"event_id\").merge(users_df, on=\"user_id\")\n",
    "merged_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess test data\n",
    "test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "test_df = test_interactions_df.merge(test_events_df, on=\"event_id\").merge(test_users_df, on=\"user_id\")\n",
    "#print(test_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train models\n",
    "content_model, content_val_metrics, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder = train_content_model(train_df, val_df)\n",
    "svd_model, svd_val_metrics = train_svd(train_df, val_df)\n",
    "hybrid_model_obj, hybrid_val_metrics, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, hybrid_svd_model = train_hybrid_model(train_df, val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate on test set\n",
    "content_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, content_model, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder, model_type=\"content\", k_list=K_LIST\n",
    ")\n",
    "svd_test_metrics = evaluate_svd_on_test(test_df, svd_model, k_list=K_LIST)\n",
    "hybrid_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, hybrid_model_obj, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, model_type=\"hybrid\", svd_model=hybrid_svd_model, k_list=K_LIST\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Precision@1  Recall@1  NDCG@1  MRR@1  Precision@5  Recall@5  \\\n",
      "Content-Based          1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
      "SVD                    1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
      "Hybrid                 1.0  0.000038     1.0    1.0          0.8  0.000150   \n",
      "\n",
      "                 NDCG@5  MRR@5  Precision@10  Recall@10   NDCG@10  MRR@10  \\\n",
      "Content-Based  1.000000    1.0           1.0   0.000375  1.000000     1.0   \n",
      "SVD            1.000000    1.0           1.0   0.000375  1.000000     1.0   \n",
      "Hybrid         0.853932    1.0           0.9   0.000338  0.905212     1.0   \n",
      "\n",
      "               Precision@50  Recall@50   NDCG@50  MRR@50  Precision@100  \\\n",
      "Content-Based          0.96   0.001801  0.969856     1.0           0.93   \n",
      "SVD                    0.96   0.001801  0.966914     1.0           0.86   \n",
      "Hybrid                 0.94   0.001763  0.930284     1.0           0.96   \n",
      "\n",
      "               Recall@100  NDCG@100  MRR@100  \n",
      "Content-Based    0.003489  0.943068      1.0  \n",
      "SVD              0.003226  0.886381      1.0  \n",
      "Hybrid           0.003601  0.949067      1.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Save results in a DataFrame (excluding AUC and MAP) ---\n",
    "def filter_metrics(metrics_dict):\n",
    "    return {k: v for k, v in metrics_dict.items() if not (k.startswith(\"AUC\") or k.startswith(\"MAP\"))}\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Content-Based\": filter_metrics(content_test_metrics),\n",
    "    \"SVD\": filter_metrics(svd_test_metrics),\n",
    "    \"Hybrid\": filter_metrics(hybrid_test_metrics),\n",
    "}).T\n",
    "\n",
    "results_df.to_csv(\"test_metrics_k_results.csv\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>MRR@1</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>MRR@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>MRR@10</th>\n",
       "      <th>Precision@50</th>\n",
       "      <th>Recall@50</th>\n",
       "      <th>NDCG@50</th>\n",
       "      <th>MRR@50</th>\n",
       "      <th>Precision@100</th>\n",
       "      <th>Recall@100</th>\n",
       "      <th>NDCG@100</th>\n",
       "      <th>MRR@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Content-Based</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.969856</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.943068</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.966914</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.886381</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybrid</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.853932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.905212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.930284</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision@1  Recall@1  NDCG@1  MRR@1  Precision@5  Recall@5  \\\n",
       "Content-Based          1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
       "SVD                    1.0  0.000038     1.0    1.0          1.0  0.000188   \n",
       "Hybrid                 1.0  0.000038     1.0    1.0          0.8  0.000150   \n",
       "\n",
       "                 NDCG@5  MRR@5  Precision@10  Recall@10   NDCG@10  MRR@10  \\\n",
       "Content-Based  1.000000    1.0           1.0   0.000375  1.000000     1.0   \n",
       "SVD            1.000000    1.0           1.0   0.000375  1.000000     1.0   \n",
       "Hybrid         0.853932    1.0           0.9   0.000338  0.905212     1.0   \n",
       "\n",
       "               Precision@50  Recall@50   NDCG@50  MRR@50  Precision@100  \\\n",
       "Content-Based          0.96   0.001801  0.969856     1.0           0.93   \n",
       "SVD                    0.96   0.001801  0.966914     1.0           0.86   \n",
       "Hybrid                 0.94   0.001763  0.930284     1.0           0.96   \n",
       "\n",
       "               Recall@100  NDCG@100  MRR@100  \n",
       "Content-Based    0.003489  0.943068      1.0  \n",
       "SVD              0.003226  0.886381      1.0  \n",
       "Hybrid           0.003601  0.949067      1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hopsworks\n",
    "# Connect to Hopsworks Feature Store\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "# Load from Hopsworks feature groups\n",
    "users_fg = fs.get_feature_group(name=\"users\", version=1)\n",
    "events_fg = fs.get_feature_group(name=\"events\", version=1)\n",
    "interactions_fg = fs.get_feature_group(name=\"interactions\", version=1)\n",
    "\n",
    "#read data from hopsworks\n",
    "users_df = users_fg.read()\n",
    "events_df = events_fg.read()\n",
    "interactions_df =interactions_fg.read()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from scipy.sparse import hstack\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# --- Metrics ---\n",
    "K_LIST = [1, 5, 10, 50, 100]\n",
    "\n",
    "def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    top_k = np.array(y_true)[sorted_indices][:k]\n",
    "    precision = np.mean(top_k)\n",
    "    recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "    dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "    ideal_k = min(int(np.sum(y_true)), k)\n",
    "    idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_score, k=100):\n",
    "    sorted_indices = np.argsort(y_score)[::-1][:k]\n",
    "    top_k = np.array(y_true)[sorted_indices]\n",
    "    ranks = np.where(top_k == 1)[0]\n",
    "    if len(ranks) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 / (ranks[0] + 1)\n",
    "\n",
    "def compute_all_metrics(y_true, y_score, k_list=K_LIST):\n",
    "    metrics = {}\n",
    "    for k in k_list:\n",
    "        p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "        metrics[f\"Precision@{k}\"] = p\n",
    "        metrics[f\"Recall@{k}\"] = r\n",
    "        metrics[f\"NDCG@{k}\"] = n\n",
    "        metrics[f\"MRR@{k}\"] = mean_reciprocal_rank(y_true, y_score, k)\n",
    "    return metrics\n",
    "\n",
    "# --- Model Training ---\n",
    "def train_content_model(train_df, val_df):\n",
    "    train_clean = train_df.copy()\n",
    "    val_clean = val_df.copy()\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "        df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "    tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "    tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    for df in [train_clean, val_clean]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "    X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "    X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'indoor_outdoor_preference']\n",
    "    for df in [train_clean, val_clean]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "    X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "    X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "    y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder\n",
    "\n",
    "def train_svd(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    svd = SVD(n_epochs=50).fit(trainset)\n",
    "    val_copy = val_df.copy()\n",
    "    val_copy[\"svd_score\"] = val_copy.apply(\n",
    "        lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_val = val_copy[\"interaction_label\"].astype(int)\n",
    "    val_scores = val_copy[\"svd_score\"]\n",
    "    return svd, compute_all_metrics(y_val, val_scores)\n",
    "\n",
    "def train_hybrid_model(train_df, val_df):\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "    trainset = train_svd_data.build_full_trainset()\n",
    "    svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    val_df[\"svd_score\"] = val_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    for df in [train_df, val_df]:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'indoor_outdoor_preference']\n",
    "    for df in [train_df, val_df]:\n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "    X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    y_train = train_df[\"interaction_label\"].astype(int)\n",
    "    y_val = val_df[\"interaction_label\"].astype(int)\n",
    "    model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, val_scores), tfidf_title, tfidf_interests, scaler, encoder, svd_model\n",
    "\n",
    "# --- Test Set Evaluation ---\n",
    "def evaluate_content_or_hybrid_on_test(test_df, model, tfidf_title, tfidf_interests, scaler, encoder, model_type=\"content\", svd_model=None, k_list=K_LIST):\n",
    "    df = test_df.copy()\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\")\n",
    "    if model_type == \"hybrid\":\n",
    "        df[\"svd_score\"] = df.apply(\n",
    "            lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "        )\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    else:\n",
    "        numeric_cols = [\"interaction_distance_to_event\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    tfidf_title_mat = tfidf_title.transform(df[\"title\"])\n",
    "    tfidf_interests_mat = tfidf_interests.transform(df[\"user_interests\"])\n",
    "    X_numeric = scaler.transform(df[numeric_cols])\n",
    "    categorical_cols = ['weather_condition', 'event_indoor_capability', 'indoor_outdoor_preference']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    X_cat = encoder.transform(df[categorical_cols])\n",
    "    X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric, X_cat]).toarray()\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = model.predict_proba(X)[:, 1]\n",
    "    return compute_all_metrics(y_true, y_score, k_list=k_list)\n",
    "\n",
    "def evaluate_svd_on_test(test_df, svd_model, k_list=K_LIST):\n",
    "    df = test_df.copy()\n",
    "    df[\"svd_score\"] = df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    y_true = df[\"interaction_label\"].astype(int)\n",
    "    y_score = df[\"svd_score\"]\n",
    "    return compute_all_metrics(y_true, y_score, k_list=k_list)\n",
    "\n",
    "\n",
    "# Load real-world test data\n",
    "test_users_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_users_data.csv')\n",
    "test_events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_events_data.csv\")\n",
    "test_interactions_df = pd.read_csv('/home/nkama/masters_thesis_project/thesis/partially_synthetic/notebooks/test_interactions_data.csv')\n",
    "\n",
    "# Rename columns in test data\n",
    "test_users_df.rename(columns={\n",
    "    'lat': 'user_lat',\n",
    "    'lng': 'user_lon',\n",
    "    'location': 'user_city',\n",
    "    'joinedAt': 'signup_date'\n",
    "}, inplace=True)\n",
    "\n",
    "test_events_df.rename(columns={\n",
    "    'category': 'event_type',\n",
    "    'lat': 'event_lat',\n",
    "    'lng': 'event_lon',\n",
    "    'city': 'event_city',\n",
    "    'weather_description': 'weather_condition',\n",
    "    'temperature_2m_mean': 'temperature'\n",
    "}, inplace=True)\n",
    "\n",
    "test_interactions_df.rename(columns={\n",
    "    'distance_to_event': 'interaction_distance_to_event'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_common(interactions_df, users_df, events_df):\n",
    "    # Create copies of the input dataframes\n",
    "    interactions_df = interactions_df.copy()\n",
    "    users_df = users_df.copy()\n",
    "    events_df = events_df.copy()\n",
    "\n",
    "    # Drop rows with missing user_id or event_id\n",
    "    interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "    \n",
    "    # Convert distance_to_event to float\n",
    "    interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "    \n",
    "    # Ensure correct types\n",
    "    for df in [interactions_df, users_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    \n",
    "    for df in [interactions_df, events_df]:\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "    interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Convert to string type for TF-IDF fields\n",
    "    events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "    users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "    users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "    # Ensure all numeric fields are float\n",
    "    numeric_cols = [\"temperature\", \"attendance_rate\"]\n",
    "    for col in numeric_cols:\n",
    "        events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    # Return all three dataframes\n",
    "    return interactions_df, users_df, events_df\n",
    "\n",
    "# Preprocess training/validation data\n",
    "interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "merged_df = interactions_df.merge(events_df, on=\"event_id\").merge(users_df, on=\"user_id\")\n",
    "merged_df.columns\n",
    "\n",
    "# Split into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Preprocess test data\n",
    "test_interactions_df, test_users_df, test_events_df = preprocess_common(test_interactions_df, test_users_df, test_events_df)\n",
    "test_df = test_interactions_df.merge(test_events_df, on=\"event_id\").merge(test_users_df, on=\"user_id\")\n",
    "#print(test_df.columns)\n",
    "\n",
    "\n",
    "# Train models\n",
    "content_model, content_val_metrics, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder = train_content_model(train_df, val_df)\n",
    "svd_model, svd_val_metrics = train_svd(train_df, val_df)\n",
    "hybrid_model_obj, hybrid_val_metrics, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, hybrid_svd_model = train_hybrid_model(train_df, val_df)\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "content_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, content_model, content_tfidf_title, content_tfidf_interests, content_scaler, content_encoder, model_type=\"content\", k_list=K_LIST\n",
    ")\n",
    "svd_test_metrics = evaluate_svd_on_test(test_df, svd_model, k_list=K_LIST)\n",
    "hybrid_test_metrics = evaluate_content_or_hybrid_on_test(\n",
    "    test_df, hybrid_model_obj, hybrid_tfidf_title, hybrid_tfidf_interests, hybrid_scaler, hybrid_encoder, model_type=\"hybrid\", svd_model=hybrid_svd_model, k_list=K_LIST\n",
    ")\n",
    "\n",
    "\n",
    "# --- Save results in a DataFrame (excluding AUC and MAP) ---\n",
    "def filter_metrics(metrics_dict):\n",
    "    return {k: v for k, v in metrics_dict.items() if not (k.startswith(\"AUC\") or k.startswith(\"MAP\"))}\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Content-Based\": filter_metrics(content_test_metrics),\n",
    "    \"SVD\": filter_metrics(svd_test_metrics),\n",
    "    \"Hybrid\": filter_metrics(hybrid_test_metrics),\n",
    "}).T\n",
    "\n",
    "results_df.to_csv(\"test_metrics_k_results.csv\")\n",
    "print(results_df)\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
