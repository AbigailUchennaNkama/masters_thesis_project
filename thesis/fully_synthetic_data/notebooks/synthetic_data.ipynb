{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ======================\n",
    "# 1. User Data Generation\n",
    "# ======================\n",
    "def generate_users(n_users=50000):\n",
    "    users = []\n",
    "    cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "    \n",
    "    # Approximate coordinates for each city\n",
    "    city_coords = {\n",
    "        'New York': (40.7128, -74.0060),\n",
    "        'London': (51.5074, -0.1278),\n",
    "        'Paris': (48.8566, 2.3522),\n",
    "        'Tokyo': (35.6762, 139.6503),\n",
    "        'Sydney': (-33.8688, 151.2093),\n",
    "        'Berlin': (52.5200, 13.4050),\n",
    "        'Mumbai': (19.0760, 72.8777),\n",
    "        'São Paulo': (-23.5505, -46.6333),\n",
    "        'Toronto': (43.6532, -79.3832),\n",
    "        'Dubai': (25.2048, 55.2708)\n",
    "    }\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=[0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05])\n",
    "        \n",
    "        # Generate lat and lon based on the selected city with some random variation\n",
    "        base_lat, base_lon = city_coords[city]\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "        \n",
    "        # Generate weather preferences with Dirichlet distribution\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]  # indoor/outdoor/any\n",
    "        weather_pref = np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs)\n",
    "        \n",
    "        # Generate declared interests for cold-start handling\n",
    "        interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "        declared_interests = random.sample(interests, k=random.randint(0,4)) if random.random() < 0.7 else []\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': fake.uuid4(),\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'weather_preference': weather_pref,\n",
    "            'age': int(skewnorm.rvs(5, loc=25, scale=15)),\n",
    "            'declared_interests': declared_interests,\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 2. Event Data Generation\n",
    "# =======================\n",
    "def generate_events(n_events=10000):\n",
    "    events = []\n",
    "    event_types = ['concert', 'sports', 'conference', 'festival', 'workshop', 'exhibition', 'seminar', 'networking']\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "\n",
    "    # Approximate coordinates for each city (same as in generate_users)\n",
    "    city_coords = {\n",
    "        'New York': (40.7128, -74.0060),\n",
    "        'London': (51.5074, -0.1278),\n",
    "        'Paris': (48.8566, 2.3522),\n",
    "        'Tokyo': (35.6762, 139.6503),\n",
    "        'Sydney': (-33.8688, 151.2093),\n",
    "        'Berlin': (52.5200, 13.4050),\n",
    "        'Mumbai': (19.0760, 72.8777),\n",
    "        'São Paulo': (-23.5505, -46.6333),\n",
    "        'Toronto': (43.6532, -79.3832),\n",
    "        'Dubai': (25.2048, 55.2708)\n",
    "    }\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        # 20% of events in random cities\n",
    "        city = np.random.choice(cities) if random.random() < 0.8 else np.random.choice(cities)\n",
    "        \n",
    "        # Generate lat and lon based on the selected city with some random variation\n",
    "        base_lat, base_lon = city_coords[city]\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "        \n",
    "        # Generate weather-sensitive attributes\n",
    "        weather_condition = np.random.choice(weather_conditions, p=[0.5, 0.2, 0.05, 0.2, 0.05])\n",
    "        if event_type in ['sports', 'festival']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['conference', 'workshop', 'seminar']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])  # Indoor events less weather-dependent\n",
    "            \n",
    "        # Generate temporal features\n",
    "        start_time = fake.date_time_between(start_date='now', end_date='+6M')\n",
    "        if start_time.weekday() >= 5:  # Weekend\n",
    "            start_time = start_time.replace(hour=np.random.choice([10,14,18]))\n",
    "        else:\n",
    "            start_time = start_time.replace(hour=np.random.choice([9,13,18,19]))\n",
    "            \n",
    "        events.append({\n",
    "            'event_id': fake.uuid4(),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type.capitalize()} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'historical_attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'indoor_capability': event_type in ['conference', 'workshop', 'exhibition', 'seminar']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3. Interaction Data Generation\n",
    "# =============================\n",
    "def generate_interactions(users, events, n_interactions=500000):\n",
    "    interactions = []\n",
    "    event_coords = events[['location_lat', 'location_lon']].values\n",
    "    event_weather = dict(zip(events['event_id'], events['weather_condition']))\n",
    "    \n",
    "    for _ in range(n_interactions):\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        # Calculate distance between user and event\n",
    "        user_coord = (user['location_lat'], user['location_lon'])\n",
    "        event_coord = (event['location_lat'], event['location_lon'])\n",
    "        distance = geodesic(user_coord, event_coord).km\n",
    "        \n",
    "        # Generate interaction probability components\n",
    "        weather_score = 1 if (event['weather_condition'] == 'Clear') or \\\n",
    "                          (user['weather_preference'] == 'any') else 0.3\n",
    "\n",
    "\n",
    "        distance_score = np.exp(-distance/5)  # Faster decay (from 20 to 10 km)\n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and \n",
    "                            user['weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        # Combine scores for interaction probability\n",
    "        #interaction_prob = 0.4*weather_score + 0.3*distance_score + 0.3*social_score\n",
    "        interaction_prob = 0.7*distance_score + 0.2*weather_score + 0.1*social_score\n",
    "        # Introduce more dispersed locations\n",
    "        if random.random() < 0.2:\n",
    "            user_coords = (base_lat + np.random.uniform(-2,2), \n",
    "                        base_lon + np.random.uniform(-2,2))\n",
    "\n",
    "        # Add a minimum distance threshold for interactions\n",
    "        if distance < 10 and (random.random() < interaction_prob):\n",
    "            interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(\n",
    "                    ['click', 'save', 'attend'],\n",
    "                    p=[0.7, 0.25, 0.05]\n",
    "                ),\n",
    "                'interaction_time': fake.date_time_between(\n",
    "                    start_date=event['start_time'] - timedelta(days=30),\n",
    "                    end_date=event['start_time']\n",
    "                ),\n",
    "                'weather_at_interaction': event_weather[event['event_id']],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "# ======================\n",
    "# 4. Cold-Start Handling\n",
    "# ======================\n",
    "def augment_cold_start(users, events, interactions):\n",
    "    cold_users = users[users['declared_interests'].str.len() > 0]\n",
    "    trending_events = events.nlargest(100, 'historical_attendance_rate')\n",
    "    \n",
    "    cold_interactions = []\n",
    "    for _, user in cold_users.iterrows():\n",
    "        for _ in range(random.randint(1,5)):\n",
    "            event = trending_events.sample(1).iloc[0]\n",
    "            cold_interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': 'click',\n",
    "                'interaction_time': fake.date_time_between(\n",
    "                    start_date=user['signup_date'],\n",
    "                    end_date=user['signup_date'] + timedelta(days=7)\n",
    "                ),\n",
    "                'weather_at_interaction': event['weather_condition'],\n",
    "                'distance_to_event': geodesic(\n",
    "                    (user['location_lat'], user['location_lon']),\n",
    "                    (event['location_lat'], event['location_lon'])\n",
    "                ).km\n",
    "            })\n",
    "    \n",
    "    return pd.concat([interactions, pd.DataFrame(cold_interactions)])\n",
    "\n",
    "# ================\n",
    "# Execution Pipeline\n",
    "# ================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating users...\")\n",
    "    users_df = generate_users(20000)\n",
    "    \n",
    "    print(\"Generating events...\")\n",
    "    events_df = generate_events(5000)\n",
    "    \n",
    "    print(\"Generating base interactions...\")\n",
    "    interactions_df = generate_interactions(users_df, events_df, 100000)\n",
    "    \n",
    "    print(\"Adding cold-start interactions...\")\n",
    "    full_interactions = augment_cold_start(users_df, events_df, interactions_df)\n",
    "    \n",
    "    # Save datasets\n",
    "    users_df.to_csv('synthetic_users.csv', index=False)\n",
    "    events_df.to_csv('synthetic_events.csv', index=False)\n",
    "    full_interactions.to_csv('synthetic_interactions.csv', index=False)\n",
    "\n",
    "    print(f\"Generated {len(users_df)} users, {len(events_df)} events, and {len(full_interactions)} interactions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        users.append({\n",
    "            'user_id': fake.uuid4(),\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': int(skewnorm.rvs(5, loc=25, scale=15)),\n",
    "            'declared_interests': random.sample(interests, k=random.randint(0,4)) if random.random() < 0.7 else [],\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    events = []\n",
    "    event_types = ['concert', 'sports', 'conference', 'festival', 'workshop', 'exhibition', 'seminar', 'networking']\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities)\n",
    "        lat, lon = generate_location(city)\n",
    "        weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        if event_type in ['sports', 'festival']:\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['conference', 'workshop', 'seminar']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        \n",
    "        start_time = fake.date_time_between(start_date='now', end_date='+6M')\n",
    "        start_time = start_time.replace(hour=np.random.choice([10,14,18]) if start_time.weekday() >= 5 else np.random.choice([9,13,18,19]))\n",
    "        \n",
    "        events.append({\n",
    "            'event_id': fake.uuid4(),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type.capitalize()} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),\n",
    "            'weather_condition': weather_condition,\n",
    "            'historical_attendance_rate': np.random.beta(a=2, b=5) * 100,\n",
    "            'indoor_capability': event_type in ['conference', 'workshop', 'exhibition', 'seminar']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    interactions = []\n",
    "    event_weather = dict(zip(events['event_id'], events['weather_condition']))\n",
    "    \n",
    "    for _ in range(n_interactions):\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        distance = geodesic((user['location_lat'], user['location_lon']), (event['location_lat'], event['location_lon'])).km\n",
    "        \n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and user['weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        distance_score = np.exp(-distance/5)\n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        interaction_prob = 0.7*distance_score + 0.2*weather_score + 0.1*social_score\n",
    "        \n",
    "        if distance < 300 and (random.random() < interaction_prob):\n",
    "            interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(['click', 'save', 'attend'], p=[0.7, 0.25, 0.05]),\n",
    "                'interaction_time': fake.date_time_between(start_date=event['start_time'] - timedelta(days=30), end_date=event['start_time']),\n",
    "                'weather_at_interaction': event_weather[event['event_id']],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "def augment_cold_start(users, events, interactions):\n",
    "    cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "    trending_events = events.nlargest(100, 'historical_attendance_rate')\n",
    "    \n",
    "    cold_interactions = []\n",
    "    for _, user in cold_users.iterrows():\n",
    "        for _ in range(random.randint(1,5)):\n",
    "            event = trending_events.sample(1).iloc[0]\n",
    "            cold_interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': 'click',\n",
    "                'interaction_time': fake.date_time_between(start_date=user['signup_date'], end_date=user['signup_date'] + timedelta(days=7)),\n",
    "                'weather_at_interaction': event['weather_condition'],\n",
    "                'distance_to_event': geodesic((user['location_lat'], user['location_lon']), (event['location_lat'], event['location_lon'])).km\n",
    "            })\n",
    "    return pd.concat([interactions, pd.DataFrame(cold_interactions)])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating users...\")\n",
    "    users_df = generate_users(50000)\n",
    "    \n",
    "    print(\"Generating events...\")\n",
    "    events_df = generate_events(10000)\n",
    "    \n",
    "    print(\"Generating base interactions...\")\n",
    "    interactions_df = generate_interactions(users_df, events_df, 500000)\n",
    "    \n",
    "    print(\"Adding cold-start interactions...\")\n",
    "    full_interactions = augment_cold_start(users_df, events_df, interactions_df)\n",
    "    \n",
    "    users_df.to_csv('synthetic_users.csv', index=False)\n",
    "    events_df.to_csv('synthetic_events.csv', index=False)\n",
    "    full_interactions.to_csv('synthetic_interactions.csv', index=False)\n",
    "\n",
    "    print(f\"Generated {len(users_df)} users, {len(events_df)} events, and {len(full_interactions)} interactions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_events.csv')\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"declared_interests\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_users.csv')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_interactions.csv')\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions[\"interaction_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(users, events, interactions):\n",
    "    print(f\"Total users: {len(users)}\")\n",
    "    print(f\"Total events: {len(events)}\")\n",
    "    print(f\"Total interactions: {len(interactions)}\")\n",
    "    \n",
    "    # Check user-event ratio\n",
    "    assert len(users) > len(events), \"There should be more users than events\"\n",
    "    \n",
    "    # Check city distribution\n",
    "    city_distribution = users['city'].value_counts(normalize=True)\n",
    "    print(\"City distribution:\")\n",
    "    print(city_distribution)\n",
    "    assert len(city_distribution) == 10, \"Should have 10 cities\"\n",
    "    \n",
    "    # Check weather-event alignment\n",
    "    outdoor_events = events[events['event_type'].isin(['sports', 'festival'])]\n",
    "    assert (outdoor_events['weather_condition'] == 'Clear').mean() > 0.7\n",
    "    \n",
    "    # In validate_data()\n",
    "    distance_bins = pd.cut(interactions['distance_to_event'], \n",
    "                        bins=[0, 5, 20, 50, 100, 200],\n",
    "                        include_lowest=True)\n",
    "    click_rates = (interactions.groupby(distance_bins, observed=True)['interaction_type']\n",
    "                   .value_counts(normalize=True))\n",
    "    \n",
    "    # Get first and last bin with actual data\n",
    "    valid_bins = click_rates.index.get_level_values(0).unique()\n",
    "    if len(valid_bins) >= 2:\n",
    "        first_bin = valid_bins[0]\n",
    "        last_bin = valid_bins[-1]\n",
    "        # assert (click_rates.loc[first_bin, 'click'] > \n",
    "        #         click_rates.loc[last_bin, 'click']), \"Distance decay pattern not observed\"\n",
    "        # # Instead of strict greater-than\n",
    "        # Add diagnostic printouts before the assert\n",
    "        print(\"\\nDistance Decay Pattern Analysis:\")\n",
    "        print(f\"First bin ({valid_bins[0]}): {click_rates.loc[valid_bins[0], 'click']:.2%} click rate\")\n",
    "        print(f\"Last bin ({valid_bins[-1]}): {click_rates.loc[valid_bins[-1], 'click']:.2%} click rate\")\n",
    "        print(f\"Decay ratio: {click_rates.loc[valid_bins[0], 'click']/click_rates.loc[valid_bins[-1], 'click']:.1f}x\")\n",
    "\n",
    "        #assert click_rates.loc[first_bin, 'click'] > click_rates.loc[last_bin, 'click'] * 0.8,  \"Distance decay pattern not observed\"\n",
    "\n",
    "    # Check cold-start coverage using value_counts with normalize\n",
    "    cold_users = users[users['declared_interests'].str.len() > 0]\n",
    "    cold_interaction_counts = interactions[interactions['user_id'].isin(cold_users['user_id'])].groupby('user_id').size()\n",
    "    cold_coverage = (cold_interaction_counts >= 3).mean()\n",
    "    assert cold_coverage > 0.8, f\"Cold-start coverage insufficient: {cold_coverage:.1%}\"\n",
    "\n",
    "\n",
    "validate_data(users_df, events_df, full_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "import os\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "# Verify that probabilities sum to 1\n",
    "if not np.isclose(sum(city_probs), 1.0):\n",
    "    raise ValueError(f\"City probabilities must sum to 1.0, but sum to {sum(city_probs)}\")\n",
    "\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    \"\"\"Generate random coordinates near a city center, with occasional outliers.\"\"\"\n",
    "    if city not in city_coords:\n",
    "        raise ValueError(f\"Unknown city: {city}\")\n",
    "        \n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        # Most locations are near city center\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        # Some locations are farther out (suburbs or neighboring areas)\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    \"\"\"Generate synthetic user data.\"\"\"\n",
    "    if n_users <= 0:\n",
    "        raise ValueError(\"Number of users must be positive\")\n",
    "        \n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Ensure age is reasonable (avoid negative values from skewnorm)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Generate weather preference probabilities\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': fake.uuid4(),\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'declared_interests': random.sample(interests, k=random.randint(0, min(4, len(interests)))) if random.random() < 0.7 else [],\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    \"\"\"Generate synthetic event data.\"\"\"\n",
    "    if n_events <= 0:\n",
    "        raise ValueError(\"Number of events must be positive\")\n",
    "        \n",
    "    events = []\n",
    "    event_types = ['concert', 'sports', 'conference', 'festival', 'workshop', 'exhibition', 'seminar', 'networking']\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    # Verify weather probabilities sum to 1\n",
    "    if not np.isclose(sum(weather_probs), 1.0):\n",
    "        raise ValueError(f\"Weather probabilities must sum to 1.0, but sum to {sum(weather_probs)}\")\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Weather conditions with conditional probabilities based on event type\n",
    "        if event_type in ['sports', 'festival']:\n",
    "            # Ensure most outdoor events have Clear weather to meet validation requirements\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['conference', 'workshop', 'seminar']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        \n",
    "        # Generate start time with reasonable hours based on weekday/weekend\n",
    "        start_time = fake.date_time_between(start_date='now', end_date='+6M')\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        \n",
    "        # Generate other event attributes\n",
    "        events.append({\n",
    "            'event_id': fake.uuid4(),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type.capitalize()} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),  # Duration in minutes\n",
    "            'weather_condition': weather_condition,\n",
    "            'historical_attendance_rate': np.random.beta(a=2, b=5) * 100,  # Percentage\n",
    "            'indoor_capability': event_type in ['conference', 'workshop', 'exhibition', 'seminar']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    \"\"\"Generate synthetic user-event interactions.\"\"\"\n",
    "    if n_interactions <= 0:\n",
    "        raise ValueError(\"Number of interactions must be positive\")\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    interactions = []\n",
    "    event_weather = dict(zip(events['event_id'], events['weather_condition']))\n",
    "    \n",
    "    for _ in range(n_interactions):\n",
    "        # Sample a random user and event\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        # Calculate distance between user and event\n",
    "        distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                           (event['location_lat'], event['location_lon'])).km\n",
    "        \n",
    "        # Calculate interaction probability based on various factors\n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and \n",
    "                               user['weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        # Modified distance decay to ensure validation requirements are met\n",
    "        distance_score = np.exp(-distance/5)  # Decreases with distance\n",
    "        \n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        interaction_prob = 0.7*distance_score + 0.2*weather_score + 0.1*social_score\n",
    "        \n",
    "        # Create interaction if probability and distance criteria are met\n",
    "        if distance < 300 and (random.random() < interaction_prob):\n",
    "            interaction_time = fake.date_time_between(\n",
    "                start_date=event['start_time'] - timedelta(days=30), \n",
    "                end_date=event['start_time']\n",
    "            )\n",
    "            \n",
    "            # Set interaction type with bias toward clicks at closer distances\n",
    "            if distance < 20:\n",
    "                interaction_type_probs = [0.8, 0.15, 0.05]  # Higher click rate for nearby events\n",
    "            else:\n",
    "                interaction_type_probs = [0.7, 0.25, 0.05]\n",
    "                \n",
    "            interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(['click', 'save', 'attend'], p=interaction_type_probs),\n",
    "                'interaction_time': interaction_time,\n",
    "                'weather_at_interaction': event_weather[event['event_id']],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "def augment_cold_start(users, events, interactions):\n",
    "    \"\"\"Add interactions for new users to help with cold-start problem.\"\"\"\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    # Get users with declared interests (potential cold start users)\n",
    "    cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "    if cold_users.empty:\n",
    "        print(\"Warning: No users with declared interests found for cold start augmentation\")\n",
    "        return interactions\n",
    "        \n",
    "    # Get trending events\n",
    "    trending_events = events.nlargest(min(100, len(events)), 'historical_attendance_rate')\n",
    "    \n",
    "    cold_interactions = []\n",
    "    for _, user in cold_users.iterrows():\n",
    "        # Ensure sufficient interactions for cold-start users (at least 3)\n",
    "        for _ in range(max(3, random.randint(3, 6))):\n",
    "            event = trending_events.sample(1).iloc[0]\n",
    "            \n",
    "            # Calculate distance\n",
    "            distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                              (event['location_lat'], event['location_lon'])).km\n",
    "            \n",
    "            cold_interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': 'click',\n",
    "                'interaction_time': fake.date_time_between(\n",
    "                    start_date=user['signup_date'], \n",
    "                    end_date=user['signup_date'] + timedelta(days=7)\n",
    "                ),\n",
    "                'weather_at_interaction': event['weather_condition'],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    # Combine with original interactions\n",
    "    return pd.concat([interactions, pd.DataFrame(cold_interactions)], ignore_index=True)\n",
    "\n",
    "def validate_data(users, events, interactions):\n",
    "    \"\"\"\n",
    "    Validate the generated data to ensure it meets quality requirements.\n",
    "    Raises AssertionError if validation fails.\n",
    "    \"\"\"\n",
    "    print(f\"Total users: {len(users)}\")\n",
    "    print(f\"Total events: {len(events)}\")\n",
    "    print(f\"Total interactions: {len(interactions)}\")\n",
    "    \n",
    "    # Check user-event ratio\n",
    "    assert len(users) > len(events), \"There should be more users than events\"\n",
    "    \n",
    "    # Check city distribution\n",
    "    city_distribution = users['city'].value_counts(normalize=True)\n",
    "    print(\"City distribution:\")\n",
    "    print(city_distribution)\n",
    "    assert len(city_distribution) == 10, \"Should have 10 cities\"\n",
    "    \n",
    "    # Check weather-event alignment\n",
    "    outdoor_events = events[events['event_type'].isin(['sports', 'festival'])]\n",
    "    outdoor_clear_rate = (outdoor_events['weather_condition'] == 'Clear').mean()\n",
    "    print(f\"Outdoor events with Clear weather: {outdoor_clear_rate:.1%}\")\n",
    "    assert outdoor_clear_rate > 0.7, f\"Too few outdoor events have Clear weather ({outdoor_clear_rate:.1%})\"\n",
    "    \n",
    "    # Check distance decay pattern in interaction types\n",
    "    try:\n",
    "        distance_bins = pd.cut(interactions['distance_to_event'], \n",
    "                          bins=[0, 5, 20, 50, 100, 200],\n",
    "                          include_lowest=True)\n",
    "        click_rates = (interactions.groupby([distance_bins, 'interaction_type'])\n",
    "                      .size()\n",
    "                      .unstack(fill_value=0)\n",
    "                      .apply(lambda x: x / x.sum(), axis=1))\n",
    "        \n",
    "        # Get first and last bin with actual data\n",
    "        if 'click' in click_rates.columns:\n",
    "            valid_bins = click_rates.index.dropna()\n",
    "            if len(valid_bins) >= 2:\n",
    "                first_bin = valid_bins[0]\n",
    "                last_bin = valid_bins[-1]\n",
    "                \n",
    "                # Add diagnostic printouts\n",
    "                print(\"\\nDistance Decay Pattern Analysis:\")\n",
    "                print(f\"First bin ({first_bin}): {click_rates.loc[first_bin, 'click']:.2%} click rate\")\n",
    "                print(f\"Last bin ({last_bin}): {click_rates.loc[last_bin, 'click']:.2%} click rate\")\n",
    "                \n",
    "                if click_rates.loc[first_bin, 'click'] > 0 and click_rates.loc[last_bin, 'click'] > 0:\n",
    "                    decay_ratio = click_rates.loc[first_bin, 'click'] / click_rates.loc[last_bin, 'click']\n",
    "                    print(f\"Decay ratio: {decay_ratio:.1f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not analyze distance decay pattern: {e}\")\n",
    "    \n",
    "    # Check cold-start coverage\n",
    "    try:\n",
    "        cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "        if not cold_users.empty:\n",
    "            cold_user_ids = set(cold_users['user_id'])\n",
    "            cold_interaction_counts = (interactions[interactions['user_id'].isin(cold_user_ids)]\n",
    "                                      .groupby('user_id').size())\n",
    "            \n",
    "            cold_coverage = (cold_interaction_counts >= 3).mean() if not cold_interaction_counts.empty else 0\n",
    "            print(f\"Cold-start users with 3+ interactions: {cold_coverage:.1%}\")\n",
    "            assert cold_coverage > 0.8, f\"Cold-start coverage insufficient: {cold_coverage:.1%}\"\n",
    "        else:\n",
    "            print(\"Warning: No cold-start users identified\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not validate cold-start coverage: {e}\")\n",
    "\n",
    "def main(output_dir='.', n_users=10000, n_events=5000, n_interactions=50000, validate=True):\n",
    "    \"\"\"Main function to generate and save all synthetic datasets.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Generating {n_users} users...\")\n",
    "        users_df = generate_users(n_users)\n",
    "        \n",
    "        print(f\"Generating {n_events} events...\")\n",
    "        events_df = generate_events(n_events)\n",
    "        \n",
    "        print(f\"Generating {n_interactions} base interactions...\")\n",
    "        interactions_df = generate_interactions(users_df, events_df, n_interactions)\n",
    "        \n",
    "        print(\"Adding cold-start interactions...\")\n",
    "        full_interactions = augment_cold_start(users_df, events_df, interactions_df)\n",
    "        \n",
    "        # Validate data if requested\n",
    "        if validate:\n",
    "            print(\"\\n=== Data Validation ===\")\n",
    "            validate_data(users_df, events_df, full_interactions)\n",
    "            print(\"=== Validation Successful ===\\n\")\n",
    "        \n",
    "        # Save to CSV files\n",
    "        users_df.to_csv(os.path.join(output_dir, 'synthetic_users.csv'), index=False)\n",
    "        events_df.to_csv(os.path.join(output_dir, 'synthetic_events.csv'), index=False)\n",
    "        full_interactions.to_csv(os.path.join(output_dir, 'synthetic_interactions.csv'), index=False)\n",
    "        \n",
    "        print(f\"Successfully generated:\")\n",
    "        print(f\"- {len(users_df)} users\")\n",
    "        print(f\"- {len(events_df)} events\")\n",
    "        print(f\"- {len(full_interactions)} interactions (including {len(full_interactions) - len(interactions_df)} cold-start interactions)\")\n",
    "        print(f\"Files saved to {output_dir}\")\n",
    "        \n",
    "        return users_df, events_df, full_interactions\n",
    "        \n",
    "    except AssertionError as ae:\n",
    "        print(f\"Validation failed: {ae}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data generation: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 users...\n",
      "Generating 10000 events...\n",
      "Generating 500000 base interactions...\n",
      "Distance distribution in sampling:\n",
      "0.0-5.0 km: 32539 samples (1.3%)\n",
      "5.0-20.0 km: 154390 samples (6.2%)\n",
      "20.0-50.0 km: 10834 samples (0.4%)\n",
      "50.0-100.0 km: 17372 samples (0.7%)\n",
      "100.0-200.0 km: 60599 samples (2.4%)\n",
      "200.0-inf km: 2224266 samples (89.0%)\n",
      "\n",
      "Final distance distribution in interactions:\n",
      "count    88262.000000\n",
      "mean        14.117202\n",
      "std         31.302218\n",
      "min          0.020860\n",
      "25%          4.828174\n",
      "50%          8.028938\n",
      "75%         12.098520\n",
      "max        299.968107\n",
      "Name: distance_to_event, dtype: float64\n",
      "Adding cold-start interactions...\n",
      "\n",
      "=== Data Validation ===\n",
      "Total users: 50000\n",
      "Total events: 10000\n",
      "Total interactions: 213573\n",
      "City distribution:\n",
      "city\n",
      "New York     0.20016\n",
      "London       0.14892\n",
      "Mumbai       0.10296\n",
      "Paris        0.10106\n",
      "Tokyo        0.10032\n",
      "Berlin       0.09880\n",
      "Toronto      0.09834\n",
      "Dubai        0.05072\n",
      "Sydney       0.04960\n",
      "São Paulo    0.04912\n",
      "Name: proportion, dtype: float64\n",
      "Outdoor events with Clear weather: 80.0%\n",
      "\n",
      "Distance Decay Pattern Analysis:\n",
      "interaction_type  click  total  click_rate\n",
      "distance_bin                              \n",
      "(0, 5]            24294  24777    0.980506\n",
      "(5, 20]           60488  67164    0.900602\n",
      "(20, 50]           1661   2035    0.816216\n",
      "(50, 100]           958   1412    0.678470\n",
      "(100, 200]         2474   5474    0.451955\n",
      "\n",
      "First bin ((0, 5]): 98.05% click rate\n",
      "Last bin ((100, 200]): 45.20% click rate\n",
      "Decay ratio: 2.17x\n",
      "Cold-start users with 3+ interactions: 100.0%\n",
      "=== Validation Successful ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_377187/2839872014.py:323: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bin_stats = interactions.groupby(['distance_bin', 'interaction_type']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated:\n",
      "- 50000 users\n",
      "- 10000 events\n",
      "- 213573 interactions (including 125311 cold-start interactions)\n",
      "Files saved to .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "import os\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "# Verify that probabilities sum to 1\n",
    "if not np.isclose(sum(city_probs), 1.0):\n",
    "    raise ValueError(f\"City probabilities must sum to 1.0, but sum to {sum(city_probs)}\")\n",
    "\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    \"\"\"Generate random coordinates near a city center, with occasional outliers.\"\"\"\n",
    "    if city not in city_coords:\n",
    "        raise ValueError(f\"Unknown city: {city}\")\n",
    "        \n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        # Most locations are near city center\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        # Some locations are farther out (suburbs or neighboring areas)\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    \"\"\"Generate synthetic user data.\"\"\"\n",
    "    if n_users <= 0:\n",
    "        raise ValueError(\"Number of users must be positive\")\n",
    "        \n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Ensure age is reasonable (avoid negative values from skewnorm)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Generate weather preference probabilities\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': fake.uuid4(),\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'declared_interests': random.sample(interests, k=random.randint(0, min(4, len(interests)))) if random.random() < 0.7 else [],\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    \"\"\"Generate synthetic event data.\"\"\"\n",
    "    if n_events <= 0:\n",
    "        raise ValueError(\"Number of events must be positive\")\n",
    "        \n",
    "    events = []\n",
    "    event_types = ['concert', 'sports', 'conference', 'festival', 'workshop', 'exhibition', 'seminar', 'networking']\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    # Verify weather probabilities sum to 1\n",
    "    if not np.isclose(sum(weather_probs), 1.0):\n",
    "        raise ValueError(f\"Weather probabilities must sum to 1.0, but sum to {sum(weather_probs)}\")\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Weather conditions with conditional probabilities based on event type\n",
    "        if event_type in ['sports', 'festival']:\n",
    "            # Ensure most outdoor events have Clear weather to meet validation requirements\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['conference', 'workshop', 'seminar']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        \n",
    "        # Generate start time with reasonable hours based on weekday/weekend\n",
    "        start_time = fake.date_time_between(start_date='now', end_date='+6M')\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        \n",
    "        # Generate other event attributes\n",
    "        events.append({\n",
    "            'event_id': fake.uuid4(),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type.capitalize()} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),  # Duration in minutes\n",
    "            'weather_condition': weather_condition,\n",
    "            'historical_attendance_rate': np.random.beta(a=2, b=5) * 100,  # Percentage\n",
    "            'indoor_capability': event_type in ['conference', 'workshop', 'exhibition', 'seminar']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def calculate_time_weight(interaction_time, current_time, half_life=30):\n",
    "    time_diff = (current_time - interaction_time).days\n",
    "    return np.exp(np.log(0.5) * time_diff / half_life)\n",
    "\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    \"\"\"Generate synthetic user-event interactions with strong distance decay pattern.\"\"\"\n",
    "    if n_interactions <= 0:\n",
    "        raise ValueError(\"Number of interactions must be positive\")\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    interactions = []\n",
    "    event_weather = dict(zip(events['event_id'], events['weather_condition']))\n",
    "    \n",
    "    # Track distance distribution for reporting\n",
    "    all_distances = []\n",
    "    \n",
    "    # FIXED: Sample more potential interactions but only keep ones meeting our criteria\n",
    "    attempts = n_interactions * 5  # Sample more to account for filtering\n",
    "    \n",
    "    for _ in range(attempts):\n",
    "        if len(interactions) >= n_interactions:\n",
    "            break\n",
    "            \n",
    "        # Sample a random user and event\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        # Calculate distance between user and event\n",
    "        distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                           (event['location_lat'], event['location_lon'])).km\n",
    "        all_distances.append(distance)\n",
    "        \n",
    "        # FIXED: Much stronger distance decay formula\n",
    "        # Use a steeper exponential decay for distance\n",
    "        distance_score = np.exp(-distance/10)  # Reduced from /5 to /20 for stronger decay\n",
    "        \n",
    "        # Calculate other factors\n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and \n",
    "                               user['weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        # FIXED: Give much more weight to distance in the interaction probability\n",
    "        interaction_prob = 0.85*distance_score + 0.1*weather_score + 0.05*social_score\n",
    "\n",
    "        # Adjust max distance based on interaction probability\n",
    "        max_distance = 50 if random.random() < 0.7 else 300\n",
    "        \n",
    "        # FIXED: Create interaction with probability more strongly influenced by distance\n",
    "        if distance < max_distance and (random.random() < interaction_prob):\n",
    "            interaction_time = fake.date_time_between(\n",
    "                start_date=event['start_time'] - timedelta(days=30), \n",
    "                end_date=event['start_time']\n",
    "            )\n",
    "            # Add the suggested time weighting\n",
    "            current_time = datetime(2025, 3, 22, 19, 10, 0)  # Use the provided date and time\n",
    "            time_weight = calculate_time_weight(interaction_time, current_time)\n",
    "            interaction_prob *= time_weight\n",
    "            # FIXED: Strongly differentiate interaction type probabilities based on distance\n",
    "            # Much higher click rate for nearby events\n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.98, 0.01, 0.01]  # Increased click probability for very close events\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.90, 0.08, 0.02]  # Increased click probability for nearby events\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.80, 0.15, 0.05]  # Adjusted for medium distance\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.65, 0.30, 0.05]  # Slightly increased click probability for longer distance\n",
    "            else:\n",
    "                interaction_type_probs = [0.45, 0.50, 0.05]  # Slightly increased click probability for distant events\n",
    "\n",
    "                \n",
    "            interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(['click', 'save', 'attend'], p=interaction_type_probs),\n",
    "                'interaction_time': interaction_time,\n",
    "                'weather_at_interaction': event_weather[event['event_id']],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(interactions)\n",
    "    \n",
    "    # Report distance distribution if enough interactions\n",
    "    if all_distances:\n",
    "        print(\"Distance distribution in sampling:\")\n",
    "        dist_bins = [0, 5, 20, 50, 100, 200, np.inf]\n",
    "        hist, edges = np.histogram(all_distances, bins=dist_bins)\n",
    "        for i in range(len(hist)):\n",
    "            print(f\"{edges[i]:.1f}-{edges[i+1] if edges[i+1] != np.inf else 'inf'} km: {hist[i]} samples ({hist[i]/len(all_distances):.1%})\")\n",
    "    \n",
    "    # Limit to requested number of interactions\n",
    "    if len(result_df) > n_interactions:\n",
    "        result_df = result_df.sample(n_interactions)\n",
    "        \n",
    "    # Report final distance statistics for accepted interactions\n",
    "    if not result_df.empty:\n",
    "        print(\"\\nFinal distance distribution in interactions:\")\n",
    "        print(result_df['distance_to_event'].describe())\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def augment_cold_start(users, events, interactions):\n",
    "    \"\"\"Add interactions for new users to help with cold-start problem.\"\"\"\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    # Get users with declared interests (potential cold start users)\n",
    "    cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "    if cold_users.empty:\n",
    "        print(\"Warning: No users with declared interests found for cold start augmentation\")\n",
    "        return interactions\n",
    "        \n",
    "    # Get trending events\n",
    "    trending_events = events.nlargest(min(100, len(events)), 'historical_attendance_rate')\n",
    "    \n",
    "    cold_interactions = []\n",
    "    for _, user in cold_users.iterrows():\n",
    "        # Ensure sufficient interactions for cold-start users (at least 3)\n",
    "        for _ in range(max(3, random.randint(3, 6))):\n",
    "            event = trending_events.sample(1).iloc[0]\n",
    "            \n",
    "            # Calculate distance\n",
    "            distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                              (event['location_lat'], event['location_lon'])).km\n",
    "            \n",
    "            # FIXED: Add similar distance-based probabilities for cold start interactions\n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.98, 0.01, 0.01]  # Increased click probability for very close events\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.90, 0.08, 0.02]  # Increased click probability for nearby events\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.80, 0.15, 0.05]  # Adjusted for medium distance\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.65, 0.30, 0.05]  # Slightly increased click probability for longer distance\n",
    "            else:\n",
    "                interaction_type_probs = [0.45, 0.50, 0.05]  # Slightly increased click probability for distant events\n",
    "\n",
    "                \n",
    "            cold_interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(['click', 'save', 'attend'], p=interaction_type_probs),\n",
    "                'interaction_time': fake.date_time_between(\n",
    "                    start_date=user['signup_date'], \n",
    "                    end_date=user['signup_date'] + timedelta(days=7)\n",
    "                ),\n",
    "                'weather_at_interaction': event['weather_condition'],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    # Combine with original interactions\n",
    "    return pd.concat([interactions, pd.DataFrame(cold_interactions)], ignore_index=True)\n",
    "\n",
    "def validate_data(users, events, interactions):\n",
    "    \"\"\"\n",
    "    Validate the generated data to ensure it meets quality requirements.\n",
    "    Raises AssertionError if validation fails.\n",
    "    \"\"\"\n",
    "    print(f\"Total users: {len(users)}\")\n",
    "    print(f\"Total events: {len(events)}\")\n",
    "    print(f\"Total interactions: {len(interactions)}\")\n",
    "    \n",
    "    # Check user-event ratio\n",
    "    assert len(users) > len(events), \"There should be more users than events\"\n",
    "    \n",
    "    # Check city distribution\n",
    "    city_distribution = users['city'].value_counts(normalize=True)\n",
    "    print(\"City distribution:\")\n",
    "    print(city_distribution)\n",
    "    assert len(city_distribution) == 10, \"Should have 10 cities\"\n",
    "    \n",
    "    # Check weather-event alignment\n",
    "    outdoor_events = events[events['event_type'].isin(['sports', 'festival'])]\n",
    "    outdoor_clear_rate = (outdoor_events['weather_condition'] == 'Clear').mean()\n",
    "    print(f\"Outdoor events with Clear weather: {outdoor_clear_rate:.1%}\")\n",
    "    assert outdoor_clear_rate > 0.7, f\"Too few outdoor events have Clear weather ({outdoor_clear_rate:.1%})\"\n",
    "    \n",
    "    # Check distance decay pattern in interaction types\n",
    "    try:\n",
    "        # FIXED: More robust distance binning approach\n",
    "        distance_bins = [0, 5, 20, 50, 100, 200]\n",
    "        bin_labels = [f\"({distance_bins[i]}, {distance_bins[i+1]}]\" for i in range(len(distance_bins)-1)]\n",
    "        \n",
    "        # Create a categorical bin column\n",
    "        interactions['distance_bin'] = pd.cut(\n",
    "            interactions['distance_to_event'], \n",
    "            bins=distance_bins,\n",
    "            labels=bin_labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        # Compute aggregated click rates by distance bin\n",
    "        bin_stats = interactions.groupby(['distance_bin', 'interaction_type']).size().unstack(fill_value=0)\n",
    "        \n",
    "        if 'click' in bin_stats.columns:\n",
    "            bin_stats['total'] = bin_stats.sum(axis=1)\n",
    "            bin_stats['click_rate'] = bin_stats['click'] / bin_stats['total']\n",
    "            \n",
    "            # Display results for all bins\n",
    "            print(\"\\nDistance Decay Pattern Analysis:\")\n",
    "            print(bin_stats[['click', 'total', 'click_rate']])\n",
    "            \n",
    "            # Only compare bins that have data\n",
    "            valid_bins = bin_stats.index.dropna().tolist()\n",
    "            if len(valid_bins) >= 2:\n",
    "                first_bin = valid_bins[0]\n",
    "                last_bin = valid_bins[-1]\n",
    "                \n",
    "                first_bin_rate = bin_stats.loc[first_bin, 'click_rate']\n",
    "                last_bin_rate = bin_stats.loc[last_bin, 'click_rate']\n",
    "                \n",
    "                print(f\"\\nFirst bin ({first_bin}): {first_bin_rate:.2%} click rate\")\n",
    "                print(f\"Last bin ({last_bin}): {last_bin_rate:.2%} click rate\")\n",
    "                \n",
    "                if first_bin_rate > 0 and last_bin_rate > 0:\n",
    "                    decay_ratio = first_bin_rate / last_bin_rate\n",
    "                    print(f\"Decay ratio: {decay_ratio:.2f}x\")\n",
    "                    \n",
    "                    # FIXED: Better validation criterion\n",
    "                    #assert decay_ratio > 1.5, f\"Distance decay ratio ({decay_ratio:.2f}) is too low\"\n",
    "                    assert decay_ratio > 1.75, f\"Distance decay ratio ({decay_ratio:.2f}) is too low\"\n",
    "\n",
    "                else:\n",
    "                    print(\"Warning: Cannot calculate decay ratio due to zero values\")\n",
    "        else:\n",
    "            print(\"Warning: No 'click' interactions found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not analyze distance decay pattern: {e}\")\n",
    "        raise  # Re-raise to ensure validation fails if this check fails\n",
    "    \n",
    "    # Check cold-start coverage\n",
    "    try:\n",
    "        cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "        if not cold_users.empty:\n",
    "            cold_user_ids = set(cold_users['user_id'])\n",
    "            cold_interaction_counts = (interactions[interactions['user_id'].isin(cold_user_ids)]\n",
    "                                      .groupby('user_id').size())\n",
    "            \n",
    "            cold_coverage = (cold_interaction_counts >= 3).mean() if not cold_interaction_counts.empty else 0\n",
    "            print(f\"Cold-start users with 3+ interactions: {cold_coverage:.1%}\")\n",
    "            assert cold_coverage > 0.8, f\"Cold-start coverage insufficient: {cold_coverage:.1%}\"\n",
    "        else:\n",
    "            print(\"Warning: No cold-start users identified\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not validate cold-start coverage: {e}\")\n",
    "        raise  # Re-raise to ensure validation fails\n",
    "\n",
    "def main(output_dir='.', n_users=50000, n_events=10000, n_interactions=500000, validate=True):\n",
    "    \"\"\"Main function to generate and save all synthetic datasets.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Generating {n_users} users...\")\n",
    "        users_df = generate_users(n_users)\n",
    "        \n",
    "        print(f\"Generating {n_events} events...\")\n",
    "        events_df = generate_events(n_events)\n",
    "        \n",
    "        print(f\"Generating {n_interactions} base interactions...\")\n",
    "        interactions_df = generate_interactions(users_df, events_df, n_interactions)\n",
    "        \n",
    "        print(\"Adding cold-start interactions...\")\n",
    "        full_interactions = augment_cold_start(users_df, events_df, interactions_df)\n",
    "        \n",
    "        # Validate data if requested\n",
    "        if validate:\n",
    "            print(\"\\n=== Data Validation ===\")\n",
    "            validate_data(users_df, events_df, full_interactions)\n",
    "            print(\"=== Validation Successful ===\\n\")\n",
    "        \n",
    "        # Save to CSV files\n",
    "        users_df.to_csv(os.path.join(output_dir, 'synthetic_users.csv'), index=False)\n",
    "        events_df.to_csv(os.path.join(output_dir, 'synthetic_events.csv'), index=False)\n",
    "        full_interactions.to_csv(os.path.join(output_dir, 'synthetic_interactions.csv'), index=False)\n",
    "        \n",
    "        print(f\"Successfully generated:\")\n",
    "        print(f\"- {len(users_df)} users\")\n",
    "        print(f\"- {len(events_df)} events\")\n",
    "        print(f\"- {len(full_interactions)} interactions (including {len(full_interactions) - len(interactions_df)} cold-start interactions)\")\n",
    "        print(f\"Files saved to {output_dir}\")\n",
    "        \n",
    "        return users_df, events_df, full_interactions\n",
    "        \n",
    "    except AssertionError as ae:\n",
    "        print(f\"Validation failed: {ae}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data generation: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>location_lon</th>\n",
       "      <th>city</th>\n",
       "      <th>weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>declared_interests</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>social_connectedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3da1ea2f-4c4f-44d2-9a0c-5da6738ab760</td>\n",
       "      <td>48.946743</td>\n",
       "      <td>2.398599</td>\n",
       "      <td>Paris</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>40</td>\n",
       "      <td>['food', 'fashion']</td>\n",
       "      <td>2023-08-27 20:24:17.685679</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8fe5bce2-e435-40af-9ac3-849c7b71072d</td>\n",
       "      <td>51.512351</td>\n",
       "      <td>-0.141411</td>\n",
       "      <td>London</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>32</td>\n",
       "      <td>['sports', 'cinema', 'music', 'travel']</td>\n",
       "      <td>2025-01-30 20:41:02.329210</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a0377d00-0f58-4c3b-a57b-866dcc246a1a</td>\n",
       "      <td>40.802577</td>\n",
       "      <td>-73.912874</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>31</td>\n",
       "      <td>['music', 'fitness', 'food', 'literature']</td>\n",
       "      <td>2024-06-22 23:44:22.111659</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87be619c-cd68-416e-9e9e-72a4a39a2106</td>\n",
       "      <td>25.259827</td>\n",
       "      <td>55.358700</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>indoor</td>\n",
       "      <td>30</td>\n",
       "      <td>['travel']</td>\n",
       "      <td>2023-07-07 12:44:32.996677</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7460053-45bb-4a34-86c8-109c1c465423</td>\n",
       "      <td>51.515939</td>\n",
       "      <td>-0.199615</td>\n",
       "      <td>London</td>\n",
       "      <td>indoor</td>\n",
       "      <td>36</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-08-21 04:00:07.703554</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id  location_lat  location_lon      city  \\\n",
       "0  3da1ea2f-4c4f-44d2-9a0c-5da6738ab760     48.946743      2.398599     Paris   \n",
       "1  8fe5bce2-e435-40af-9ac3-849c7b71072d     51.512351     -0.141411    London   \n",
       "2  a0377d00-0f58-4c3b-a57b-866dcc246a1a     40.802577    -73.912874  New York   \n",
       "3  87be619c-cd68-416e-9e9e-72a4a39a2106     25.259827     55.358700     Dubai   \n",
       "4  f7460053-45bb-4a34-86c8-109c1c465423     51.515939     -0.199615    London   \n",
       "\n",
       "  weather_preference  age                          declared_interests  \\\n",
       "0            outdoor   40                         ['food', 'fashion']   \n",
       "1            outdoor   32     ['sports', 'cinema', 'music', 'travel']   \n",
       "2            outdoor   31  ['music', 'fitness', 'food', 'literature']   \n",
       "3             indoor   30                                  ['travel']   \n",
       "4             indoor   36                                          []   \n",
       "\n",
       "                  signup_date  social_connectedness  \n",
       "0  2023-08-27 20:24:17.685679                    11  \n",
       "1  2025-01-30 20:41:02.329210                    16  \n",
       "2  2024-06-22 23:44:22.111659                    15  \n",
       "3  2023-07-07 12:44:32.996677                    19  \n",
       "4  2024-08-21 04:00:07.703554                    13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_users.csv')\n",
    "users.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>weather_at_interaction</th>\n",
       "      <th>distance_to_event</th>\n",
       "      <th>distance_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04b70e35-a19d-4c3d-84ee-07c555538476</td>\n",
       "      <td>de772063-09eb-4751-a71b-70e9cb3966cc</td>\n",
       "      <td>9e71bfde-b7f6-4ebe-abe9-c50db4a34009</td>\n",
       "      <td>click</td>\n",
       "      <td>2025-07-10 03:37:52.030895</td>\n",
       "      <td>Clear</td>\n",
       "      <td>4.197641</td>\n",
       "      <td>(0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a435450e-838b-495f-81b2-2ea21aac5c75</td>\n",
       "      <td>9b3f59f2-b5a8-4c71-9c5e-5746d64f4206</td>\n",
       "      <td>e5dd2d2e-d7d0-46dd-bd7a-57715de6beb0</td>\n",
       "      <td>save</td>\n",
       "      <td>2025-06-09 14:42:00.436545</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>5.632956</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d6d27b7-3c25-4423-a3de-c213701cc61f</td>\n",
       "      <td>cbef590d-7074-48dd-8772-27ee3c5bf296</td>\n",
       "      <td>d0ef7ea8-aa4b-4cc7-8827-6f6e0010214f</td>\n",
       "      <td>click</td>\n",
       "      <td>2025-06-26 19:11:35.270447</td>\n",
       "      <td>Clear</td>\n",
       "      <td>4.046758</td>\n",
       "      <td>(0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>754e67f3-d9c8-46a0-97d6-bbde51424bcb</td>\n",
       "      <td>61b3280f-2aa1-47d2-8a50-a4729cc29c14</td>\n",
       "      <td>dae457fe-bd9a-4d4b-bbcf-1e3f3391cf2a</td>\n",
       "      <td>click</td>\n",
       "      <td>2025-06-30 01:29:04.915806</td>\n",
       "      <td>Clear</td>\n",
       "      <td>12.100435</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6fe87c17-1f3c-4340-9457-a0e4ff8dc829</td>\n",
       "      <td>ebb0c722-1a5e-4443-b059-5bad4ae09fa4</td>\n",
       "      <td>ee782cfa-4000-45b4-bbe4-bb7007bae697</td>\n",
       "      <td>click</td>\n",
       "      <td>2025-03-08 00:04:26.987499</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>13.378184</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id                               user_id  \\\n",
       "0  04b70e35-a19d-4c3d-84ee-07c555538476  de772063-09eb-4751-a71b-70e9cb3966cc   \n",
       "1  a435450e-838b-495f-81b2-2ea21aac5c75  9b3f59f2-b5a8-4c71-9c5e-5746d64f4206   \n",
       "2  7d6d27b7-3c25-4423-a3de-c213701cc61f  cbef590d-7074-48dd-8772-27ee3c5bf296   \n",
       "3  754e67f3-d9c8-46a0-97d6-bbde51424bcb  61b3280f-2aa1-47d2-8a50-a4729cc29c14   \n",
       "4  6fe87c17-1f3c-4340-9457-a0e4ff8dc829  ebb0c722-1a5e-4443-b059-5bad4ae09fa4   \n",
       "\n",
       "                               event_id interaction_type  \\\n",
       "0  9e71bfde-b7f6-4ebe-abe9-c50db4a34009            click   \n",
       "1  e5dd2d2e-d7d0-46dd-bd7a-57715de6beb0             save   \n",
       "2  d0ef7ea8-aa4b-4cc7-8827-6f6e0010214f            click   \n",
       "3  dae457fe-bd9a-4d4b-bbcf-1e3f3391cf2a            click   \n",
       "4  ee782cfa-4000-45b4-bbe4-bb7007bae697            click   \n",
       "\n",
       "             interaction_time weather_at_interaction  distance_to_event  \\\n",
       "0  2025-07-10 03:37:52.030895                  Clear           4.197641   \n",
       "1  2025-06-09 14:42:00.436545                 Cloudy           5.632956   \n",
       "2  2025-06-26 19:11:35.270447                  Clear           4.046758   \n",
       "3  2025-06-30 01:29:04.915806                  Clear          12.100435   \n",
       "4  2025-03-08 00:04:26.987499                 Cloudy          13.378184   \n",
       "\n",
       "  distance_bin  \n",
       "0       (0, 5]  \n",
       "1      (5, 20]  \n",
       "2       (0, 5]  \n",
       "3      (5, 20]  \n",
       "4      (5, 20]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_interactions.csv')\n",
    "interactions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions[\"interaction_type\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.isnu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>location_lon</th>\n",
       "      <th>city</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>historical_attendance_rate</th>\n",
       "      <th>indoor_capability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4491f835-61c3-4a0d-89a0-9da110ca2b68</td>\n",
       "      <td>Face-to-face optimal service-desk Exhibition i...</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>35.668559</td>\n",
       "      <td>139.720674</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>2025-07-15 19:34:05.914671</td>\n",
       "      <td>180</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>32.448197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de398dc6-2fd3-4e77-91b8-7d689a673407</td>\n",
       "      <td>Distributed context-sensitive service-desk Exh...</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>40.773806</td>\n",
       "      <td>-73.946061</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-04-05 18:11:13.680569</td>\n",
       "      <td>360</td>\n",
       "      <td>Snow</td>\n",
       "      <td>8.377251</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f168037-8f2d-489e-8acb-654d5bbfb5e3</td>\n",
       "      <td>User-centric fault-tolerant workforce Sports i...</td>\n",
       "      <td>sports</td>\n",
       "      <td>19.071756</td>\n",
       "      <td>72.911718</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2025-08-18 19:08:48.845907</td>\n",
       "      <td>360</td>\n",
       "      <td>Clear</td>\n",
       "      <td>22.178179</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459e72e4-310a-41ab-b47e-f437b99724d6</td>\n",
       "      <td>Enterprise-wide system-worthy analyzer Sports ...</td>\n",
       "      <td>sports</td>\n",
       "      <td>52.570974</td>\n",
       "      <td>13.396175</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-05-21 13:38:38.711574</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>46.583396</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d00363e2-303f-401e-a3d6-0b3279825d1e</td>\n",
       "      <td>Phased exuding project Sports in New York</td>\n",
       "      <td>sports</td>\n",
       "      <td>40.755926</td>\n",
       "      <td>-73.948998</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-05-15 19:41:37.281529</td>\n",
       "      <td>480</td>\n",
       "      <td>Clear</td>\n",
       "      <td>24.978247</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               event_id  \\\n",
       "0  4491f835-61c3-4a0d-89a0-9da110ca2b68   \n",
       "1  de398dc6-2fd3-4e77-91b8-7d689a673407   \n",
       "2  1f168037-8f2d-489e-8acb-654d5bbfb5e3   \n",
       "3  459e72e4-310a-41ab-b47e-f437b99724d6   \n",
       "4  d00363e2-303f-401e-a3d6-0b3279825d1e   \n",
       "\n",
       "                                               title  event_type  \\\n",
       "0  Face-to-face optimal service-desk Exhibition i...  exhibition   \n",
       "1  Distributed context-sensitive service-desk Exh...  exhibition   \n",
       "2  User-centric fault-tolerant workforce Sports i...      sports   \n",
       "3  Enterprise-wide system-worthy analyzer Sports ...      sports   \n",
       "4          Phased exuding project Sports in New York      sports   \n",
       "\n",
       "   location_lat  location_lon      city                  start_time  duration  \\\n",
       "0     35.668559    139.720674     Tokyo  2025-07-15 19:34:05.914671       180   \n",
       "1     40.773806    -73.946061  New York  2025-04-05 18:11:13.680569       360   \n",
       "2     19.071756     72.911718    Mumbai  2025-08-18 19:08:48.845907       360   \n",
       "3     52.570974     13.396175    Berlin  2025-05-21 13:38:38.711574       120   \n",
       "4     40.755926    -73.948998  New York  2025-05-15 19:41:37.281529       480   \n",
       "\n",
       "  weather_condition  historical_attendance_rate  indoor_capability  \n",
       "0            Cloudy                   32.448197               True  \n",
       "1              Snow                    8.377251               True  \n",
       "2             Clear                   22.178179              False  \n",
       "3             Clear                   46.583396              False  \n",
       "4             Clear                   24.978247              False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_events.csv')\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['interaction_id', 'user_id', 'event_id', 'interaction_type',\n",
       "       'interaction_time', 'weather_at_interaction', 'distance_to_event',\n",
       "       'distance_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'location_lat', 'location_lon', 'city', 'weather_preference',\n",
       "       'age', 'declared_interests', 'signup_date', 'social_connectedness'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 users...\n",
      "Generating 5000 events...\n",
      "Generating 100000 base interactions...\n",
      "Distance distribution in sampling:\n",
      "0.0-5.0 km: 6761 samples (1.4%)\n",
      "5.0-20.0 km: 30485 samples (6.1%)\n",
      "20.0-50.0 km: 2031 samples (0.4%)\n",
      "50.0-100.0 km: 3355 samples (0.7%)\n",
      "100.0-200.0 km: 12019 samples (2.4%)\n",
      "200.0-inf km: 445349 samples (89.1%)\n",
      "\n",
      "Final distance distribution in interactions:\n",
      "count    17423.000000\n",
      "mean        13.441899\n",
      "std         29.695545\n",
      "min          0.045958\n",
      "25%          4.658636\n",
      "50%          7.870139\n",
      "75%         11.955413\n",
      "max        298.508345\n",
      "Name: distance_to_event, dtype: float64\n",
      "Adding cold-start interactions...\n",
      "\n",
      "=== Data Validation ===\n",
      "Total users: 10000\n",
      "Total events: 5000\n",
      "Total interactions: 42455\n",
      "City distribution:\n",
      "city\n",
      "New York     0.1979\n",
      "London       0.1541\n",
      "Paris        0.1036\n",
      "Mumbai       0.1002\n",
      "Tokyo        0.1000\n",
      "Toronto      0.0981\n",
      "Berlin       0.0937\n",
      "Dubai        0.0513\n",
      "Sydney       0.0512\n",
      "São Paulo    0.0499\n",
      "Name: proportion, dtype: float64\n",
      "Outdoor events with Clear weather: 79.4%\n",
      "\n",
      "Distance Decay Pattern Analysis:\n",
      "  distance_bin  positive_rate\n",
      "0       (0, 5]       0.456433\n",
      "1      (5, 20]       0.352113\n",
      "2     (20, 50]       0.242105\n",
      "3    (50, 100]       0.152239\n",
      "4   (100, 200]       0.098621\n",
      "\n",
      "First bin ((0, 5]): 45.64% positive response rate\n",
      "Last bin ((100, 200]): 9.86% positive response rate\n",
      "Decay ratio: 4.63x\n",
      "Cold-start users with 3+ interactions: 100.0%\n",
      "=== Validation Successful ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_377187/4013612543.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bin_stats = interactions.groupby('distance_bin')['interaction_type'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated:\n",
      "- 10000 users\n",
      "- 5000 events\n",
      "- 42455 interactions (including 25032 cold-start interactions)\n",
      "Files saved to .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "import os\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "# Verify that probabilities sum to 1\n",
    "if not np.isclose(sum(city_probs), 1.0):\n",
    "    raise ValueError(f\"City probabilities must sum to 1.0, but sum to {sum(city_probs)}\")\n",
    "\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    \"\"\"Generate random coordinates near a city center, with occasional outliers.\"\"\"\n",
    "    if city not in city_coords:\n",
    "        raise ValueError(f\"Unknown city: {city}\")\n",
    "        \n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        # Most locations are near city center\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        # Some locations are farther out (suburbs or neighboring areas)\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    \"\"\"Generate synthetic user data.\"\"\"\n",
    "    if n_users <= 0:\n",
    "        raise ValueError(\"Number of users must be positive\")\n",
    "        \n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Ensure age is reasonable (avoid negative values from skewnorm)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Generate weather preference probabilities\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': fake.uuid4(),\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'declared_interests': random.sample(interests, k=random.randint(0, min(4, len(interests)))) if random.random() < 0.7 else [],\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    \"\"\"Generate synthetic event data.\"\"\"\n",
    "    if n_events <= 0:\n",
    "        raise ValueError(\"Number of events must be positive\")\n",
    "        \n",
    "    events = []\n",
    "    # Updated event types\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', 'Arts & Culture', \n",
    "        'Entertainment', 'Sports & Fitness', 'Business & Networking', 'Health & Wellness', \n",
    "        'Music & Concerts', 'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    # Verify weather probabilities sum to 1\n",
    "    if not np.isclose(sum(weather_probs), 1.0):\n",
    "        raise ValueError(f\"Weather probabilities must sum to 1.0, but sum to {sum(weather_probs)}\")\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Weather conditions with conditional probabilities based on event type\n",
    "        if event_type in ['Sports & Fitness', 'Seasonal & Festivals']:\n",
    "            # Ensure most outdoor events have Clear weather to meet validation requirements\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['Education & Learning', 'Technology', 'Business & Networking']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        \n",
    "        # Generate start time with reasonable hours based on weekday/weekend\n",
    "        start_time = fake.date_time_between(start_date='now', end_date='+6M')\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        \n",
    "        # Generate other event attributes\n",
    "        events.append({\n",
    "            'event_id': fake.uuid4(),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),  # Duration in minutes\n",
    "            'weather_condition': weather_condition,\n",
    "            'historical_attendance_rate': np.random.beta(a=2, b=5) * 100,  # Percentage\n",
    "            'indoor_capability': event_type in ['Education & Learning', 'Technology', 'Business & Networking', \n",
    "                                               'Arts & Culture', 'Entertainment', 'Immersive Experiences']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def calculate_time_weight(interaction_time, current_time, half_life=30):\n",
    "    time_diff = (current_time - interaction_time).days\n",
    "    return np.exp(np.log(0.5) * time_diff / half_life)\n",
    "\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    \"\"\"Generate synthetic user-event interactions with strong distance decay pattern.\"\"\"\n",
    "    if n_interactions <= 0:\n",
    "        raise ValueError(\"Number of interactions must be positive\")\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    interactions = []\n",
    "    event_weather = dict(zip(events['event_id'], events['weather_condition']))\n",
    "    \n",
    "    # Updated interaction types\n",
    "    interaction_types = ['maybe', 'invited & maybe', 'no', 'yes', 'invited & yes', 'invited & no', 'invited']\n",
    "    \n",
    "    # Track distance distribution for reporting\n",
    "    all_distances = []\n",
    "    \n",
    "    # Sample more potential interactions but only keep ones meeting our criteria\n",
    "    attempts = n_interactions * 5  # Sample more to account for filtering\n",
    "    \n",
    "    for _ in range(attempts):\n",
    "        if len(interactions) >= n_interactions:\n",
    "            break\n",
    "            \n",
    "        # Sample a random user and event\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        # Calculate distance between user and event\n",
    "        distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                           (event['location_lat'], event['location_lon'])).km\n",
    "        all_distances.append(distance)\n",
    "        \n",
    "        # Much stronger distance decay formula\n",
    "        # Use a steeper exponential decay for distance\n",
    "        distance_score = np.exp(-distance/10)  # Reduced from /5 to /20 for stronger decay\n",
    "        \n",
    "        # Calculate other factors\n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and \n",
    "                               user['weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        # Give much more weight to distance in the interaction probability\n",
    "        interaction_prob = 0.85*distance_score + 0.1*weather_score + 0.05*social_score\n",
    "\n",
    "        # Adjust max distance based on interaction probability\n",
    "        max_distance = 50 if random.random() < 0.7 else 300\n",
    "        \n",
    "        # Create interaction with probability more strongly influenced by distance\n",
    "        if distance < max_distance and (random.random() < interaction_prob):\n",
    "            interaction_time = fake.date_time_between(\n",
    "                start_date=event['start_time'] - timedelta(days=30), \n",
    "                end_date=event['start_time']\n",
    "            )\n",
    "            # Add the suggested time weighting\n",
    "            current_time = datetime(2025, 3, 22, 19, 10, 0)  # Use the provided date and time\n",
    "            time_weight = calculate_time_weight(interaction_time, current_time)\n",
    "            interaction_prob *= time_weight\n",
    "            \n",
    "            # Determine probabilities for different interaction types based on distance\n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.15, 0.20, 0.05, 0.25, 0.20, 0.05, 0.10]  # High positive response for very close events\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.20, 0.15, 0.10, 0.20, 0.15, 0.10, 0.10]  # Good mix for nearby events\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.25, 0.10, 0.15, 0.15, 0.10, 0.15, 0.10]  # More maybe/no for medium distance\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.20, 0.05, 0.25, 0.10, 0.05, 0.20, 0.15]  # Higher no rate for longer distance\n",
    "            else:\n",
    "                interaction_type_probs = [0.15, 0.05, 0.30, 0.05, 0.05, 0.25, 0.15]  # Highest no rate for distant events\n",
    "                \n",
    "            interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(interaction_types, p=interaction_type_probs),\n",
    "                'interaction_time': interaction_time,\n",
    "                'weather_at_interaction': event_weather[event['event_id']],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(interactions)\n",
    "    \n",
    "    # Report distance distribution if enough interactions\n",
    "    if all_distances:\n",
    "        print(\"Distance distribution in sampling:\")\n",
    "        dist_bins = [0, 5, 20, 50, 100, 200, np.inf]\n",
    "        hist, edges = np.histogram(all_distances, bins=dist_bins)\n",
    "        for i in range(len(hist)):\n",
    "            print(f\"{edges[i]:.1f}-{edges[i+1] if edges[i+1] != np.inf else 'inf'} km: {hist[i]} samples ({hist[i]/len(all_distances):.1%})\")\n",
    "    \n",
    "    # Limit to requested number of interactions\n",
    "    if len(result_df) > n_interactions:\n",
    "        result_df = result_df.sample(n_interactions)\n",
    "        \n",
    "    # Report final distance statistics for accepted interactions\n",
    "    if not result_df.empty:\n",
    "        print(\"\\nFinal distance distribution in interactions:\")\n",
    "        print(result_df['distance_to_event'].describe())\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def augment_cold_start(users, events, interactions):\n",
    "    \"\"\"Add interactions for new users to help with cold-start problem.\"\"\"\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    # Get users with declared interests (potential cold start users)\n",
    "    cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "    if cold_users.empty:\n",
    "        print(\"Warning: No users with declared interests found for cold start augmentation\")\n",
    "        return interactions\n",
    "        \n",
    "    # Get trending events\n",
    "    trending_events = events.nlargest(min(100, len(events)), 'historical_attendance_rate')\n",
    "    \n",
    "    # Updated interaction types\n",
    "    interaction_types = ['maybe', 'invited & maybe', 'no', 'yes', 'invited & yes', 'invited & no', 'invited']\n",
    "    \n",
    "    cold_interactions = []\n",
    "    for _, user in cold_users.iterrows():\n",
    "        # Ensure sufficient interactions for cold-start users (at least 3)\n",
    "        for _ in range(max(3, random.randint(3, 6))):\n",
    "            event = trending_events.sample(1).iloc[0]\n",
    "            \n",
    "            # Calculate distance\n",
    "            distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                              (event['location_lat'], event['location_lon'])).km\n",
    "            \n",
    "            # Add similar distance-based probabilities for cold start interactions\n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.15, 0.20, 0.05, 0.25, 0.20, 0.05, 0.10]  # High positive response for very close events\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.20, 0.15, 0.10, 0.20, 0.15, 0.10, 0.10]  # Good mix for nearby events\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.25, 0.10, 0.15, 0.15, 0.10, 0.15, 0.10]  # More maybe/no for medium distance\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.20, 0.05, 0.25, 0.10, 0.05, 0.20, 0.15]  # Higher no rate for longer distance\n",
    "            else:\n",
    "                interaction_type_probs = [0.15, 0.05, 0.30, 0.05, 0.05, 0.25, 0.15]  # Highest no rate for distant events\n",
    "                \n",
    "            cold_interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(interaction_types, p=interaction_type_probs),\n",
    "                'interaction_time': fake.date_time_between(\n",
    "                    start_date=user['signup_date'], \n",
    "                    end_date=user['signup_date'] + timedelta(days=7)\n",
    "                ),\n",
    "                'weather_at_interaction': event['weather_condition'],\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    # Combine with original interactions\n",
    "    return pd.concat([interactions, pd.DataFrame(cold_interactions)], ignore_index=True)\n",
    "\n",
    "def validate_data(users, events, interactions):\n",
    "    \"\"\"\n",
    "    Validate the generated data to ensure it meets quality requirements.\n",
    "    Raises AssertionError if validation fails.\n",
    "    \"\"\"\n",
    "    print(f\"Total users: {len(users)}\")\n",
    "    print(f\"Total events: {len(events)}\")\n",
    "    print(f\"Total interactions: {len(interactions)}\")\n",
    "    \n",
    "    # Check user-event ratio\n",
    "    assert len(users) > len(events), \"There should be more users than events\"\n",
    "    \n",
    "    # Check city distribution\n",
    "    city_distribution = users['city'].value_counts(normalize=True)\n",
    "    print(\"City distribution:\")\n",
    "    print(city_distribution)\n",
    "    assert len(city_distribution) == 10, \"Should have 10 cities\"\n",
    "    \n",
    "    # Check weather-event alignment\n",
    "    outdoor_events = events[events['event_type'].isin(['Sports & Fitness', 'Seasonal & Festivals'])]\n",
    "    outdoor_clear_rate = (outdoor_events['weather_condition'] == 'Clear').mean()\n",
    "    print(f\"Outdoor events with Clear weather: {outdoor_clear_rate:.1%}\")\n",
    "    assert outdoor_clear_rate > 0.7, f\"Too few outdoor events have Clear weather ({outdoor_clear_rate:.1%})\"\n",
    "    \n",
    "    # Check distance decay pattern in interaction types\n",
    "    try:\n",
    "        # More robust distance binning approach\n",
    "        distance_bins = [0, 5, 20, 50, 100, 200]\n",
    "        bin_labels = [f\"({distance_bins[i]}, {distance_bins[i+1]}]\" for i in range(len(distance_bins)-1)]\n",
    "        \n",
    "        # Create a categorical bin column\n",
    "        interactions['distance_bin'] = pd.cut(\n",
    "            interactions['distance_to_event'], \n",
    "            bins=distance_bins,\n",
    "            labels=bin_labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        # Compute aggregated positive response rates by distance bin\n",
    "        # Considering 'yes' and 'invited & yes' as positive responses\n",
    "        bin_stats = interactions.groupby('distance_bin')['interaction_type'].apply(\n",
    "            lambda x: (x.isin(['yes', 'invited & yes'])).mean()\n",
    "        ).reset_index(name='positive_rate')\n",
    "        \n",
    "        # Display results for all bins\n",
    "        print(\"\\nDistance Decay Pattern Analysis:\")\n",
    "        print(bin_stats)\n",
    "        \n",
    "        # Only compare bins that have data\n",
    "        valid_bins = bin_stats['distance_bin'].dropna().tolist()\n",
    "        if len(valid_bins) >= 2:\n",
    "            first_bin = valid_bins[0]\n",
    "            last_bin = valid_bins[-1]\n",
    "            \n",
    "            first_bin_rate = bin_stats.loc[bin_stats['distance_bin'] == first_bin, 'positive_rate'].values[0]\n",
    "            last_bin_rate = bin_stats.loc[bin_stats['distance_bin'] == last_bin, 'positive_rate'].values[0]\n",
    "            \n",
    "            print(f\"\\nFirst bin ({first_bin}): {first_bin_rate:.2%} positive response rate\")\n",
    "            print(f\"Last bin ({last_bin}): {last_bin_rate:.2%} positive response rate\")\n",
    "            \n",
    "            if first_bin_rate > 0 and last_bin_rate > 0:\n",
    "                decay_ratio = first_bin_rate / last_bin_rate\n",
    "                print(f\"Decay ratio: {decay_ratio:.2f}x\")\n",
    "                \n",
    "                # Better validation criterion\n",
    "                assert decay_ratio > 1.75, f\"Distance decay ratio ({decay_ratio:.2f}) is too low\"\n",
    "            else:\n",
    "                print(\"Warning: Cannot calculate decay ratio due to zero values\")\n",
    "        else:\n",
    "            print(\"Warning: Not enough distance bins for analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not analyze distance decay pattern: {e}\")\n",
    "        raise  # Re-raise to ensure validation fails if this check fails\n",
    "    \n",
    "    # Check cold-start coverage\n",
    "    try:\n",
    "        cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "        if not cold_users.empty:\n",
    "            cold_user_ids = set(cold_users['user_id'])\n",
    "            cold_interaction_counts = (interactions[interactions['user_id'].isin(cold_user_ids)]\n",
    "                                      .groupby('user_id').size())\n",
    "            \n",
    "            cold_coverage = (cold_interaction_counts >= 3).mean() if not cold_interaction_counts.empty else 0\n",
    "            print(f\"Cold-start users with 3+ interactions: {cold_coverage:.1%}\")\n",
    "            assert cold_coverage > 0.8, f\"Cold-start coverage insufficient: {cold_coverage:.1%}\"\n",
    "        else:\n",
    "            print(\"Warning: No cold-start users identified\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not validate cold-start coverage: {e}\")\n",
    "        raise  # Re-raise to ensure validation fails\n",
    "\n",
    "def main(output_dir='.', n_users=10000, n_events=5000, n_interactions=100000, validate=True):\n",
    "    \"\"\"Main function to generate and save all synthetic datasets.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Generating {n_users} users...\")\n",
    "        users_df = generate_users(n_users)\n",
    "        \n",
    "        print(f\"Generating {n_events} events...\")\n",
    "        events_df = generate_events(n_events)\n",
    "        \n",
    "        print(f\"Generating {n_interactions} base interactions...\")\n",
    "        interactions_df = generate_interactions(users_df, events_df, n_interactions)\n",
    "        \n",
    "        print(\"Adding cold-start interactions...\")\n",
    "        full_interactions = augment_cold_start(users_df, events_df, interactions_df)\n",
    "        \n",
    "        # Validate data if requested\n",
    "        if validate:\n",
    "            print(\"\\n=== Data Validation ===\")\n",
    "            validate_data(users_df, events_df, full_interactions)\n",
    "            print(\"=== Validation Successful ===\\n\")\n",
    "        \n",
    "        # Save to CSV files\n",
    "        users_df.to_csv(os.path.join(output_dir, 'synthetic_users.csv'), index=False)\n",
    "        events_df.to_csv(os.path.join(output_dir, 'synthetic_events.csv'), index=False)\n",
    "        full_interactions.to_csv(os.path.join(output_dir, 'synthetic_interactions.csv'), index=False)\n",
    "        \n",
    "        print(f\"Successfully generated:\")\n",
    "        print(f\"- {len(users_df)} users\")\n",
    "        print(f\"- {len(events_df)} events\")\n",
    "        print(f\"- {len(full_interactions)} interactions (including {len(full_interactions) - len(interactions_df)} cold-start interactions)\")\n",
    "        print(f\"Files saved to {output_dir}\")\n",
    "        \n",
    "        return users_df, events_df, full_interactions\n",
    "        \n",
    "    except AssertionError as ae:\n",
    "        print(f\"Validation failed: {ae}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data generation: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>location_lon</th>\n",
       "      <th>city</th>\n",
       "      <th>weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>declared_interests</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>social_connectedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35e1b687-69d8-463c-8dc8-ca69a6fb443c</td>\n",
       "      <td>48.946743</td>\n",
       "      <td>2.398599</td>\n",
       "      <td>Paris</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>40</td>\n",
       "      <td>['food', 'fashion']</td>\n",
       "      <td>2024-04-14 18:40:47.717122</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adc6995a-f086-4a20-b9bf-9833a4fbd8c9</td>\n",
       "      <td>51.512351</td>\n",
       "      <td>-0.141411</td>\n",
       "      <td>London</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>32</td>\n",
       "      <td>['sports', 'cinema', 'music', 'travel']</td>\n",
       "      <td>2024-12-27 07:12:36.083451</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563c8269-d89a-4d3f-9f16-ba0616e7c0a3</td>\n",
       "      <td>40.802577</td>\n",
       "      <td>-73.912874</td>\n",
       "      <td>New York</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>31</td>\n",
       "      <td>['music', 'fitness', 'food', 'literature']</td>\n",
       "      <td>2024-04-15 07:50:36.930433</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d02cc747-d2c1-433e-9aa3-dbdd087d02f2</td>\n",
       "      <td>25.259827</td>\n",
       "      <td>55.358700</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>indoor</td>\n",
       "      <td>30</td>\n",
       "      <td>['travel']</td>\n",
       "      <td>2024-01-18 05:13:44.494596</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>935aa5d5-64e2-448a-96b2-1c1bdb869bec</td>\n",
       "      <td>51.515939</td>\n",
       "      <td>-0.199615</td>\n",
       "      <td>London</td>\n",
       "      <td>indoor</td>\n",
       "      <td>36</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-02-25 06:10:20.602422</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id  location_lat  location_lon      city  \\\n",
       "0  35e1b687-69d8-463c-8dc8-ca69a6fb443c     48.946743      2.398599     Paris   \n",
       "1  adc6995a-f086-4a20-b9bf-9833a4fbd8c9     51.512351     -0.141411    London   \n",
       "2  563c8269-d89a-4d3f-9f16-ba0616e7c0a3     40.802577    -73.912874  New York   \n",
       "3  d02cc747-d2c1-433e-9aa3-dbdd087d02f2     25.259827     55.358700     Dubai   \n",
       "4  935aa5d5-64e2-448a-96b2-1c1bdb869bec     51.515939     -0.199615    London   \n",
       "\n",
       "  weather_preference  age                          declared_interests  \\\n",
       "0            outdoor   40                         ['food', 'fashion']   \n",
       "1            outdoor   32     ['sports', 'cinema', 'music', 'travel']   \n",
       "2            outdoor   31  ['music', 'fitness', 'food', 'literature']   \n",
       "3             indoor   30                                  ['travel']   \n",
       "4             indoor   36                                          []   \n",
       "\n",
       "                  signup_date  social_connectedness  \n",
       "0  2024-04-14 18:40:47.717122                    11  \n",
       "1  2024-12-27 07:12:36.083451                    16  \n",
       "2  2024-04-15 07:50:36.930433                    15  \n",
       "3  2024-01-18 05:13:44.494596                    19  \n",
       "4  2025-02-25 06:10:20.602422                    13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_users.csv')\n",
    "users.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   user_id               10000 non-null  object \n",
      " 1   location_lat          10000 non-null  float64\n",
      " 2   location_lon          10000 non-null  float64\n",
      " 3   city                  10000 non-null  object \n",
      " 4   weather_preference    10000 non-null  object \n",
      " 5   age                   10000 non-null  int64  \n",
      " 6   declared_interests    10000 non-null  object \n",
      " 7   signup_date           10000 non-null  object \n",
      " 8   social_connectedness  10000 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>distance_to_event</th>\n",
       "      <th>distance_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80c36646-fe7f-487f-bcba-248d663ff0bf</td>\n",
       "      <td>44814877-2f5d-4fef-a420-f305c0a33357</td>\n",
       "      <td>7be347c2-7ba9-423a-9565-610d6b21d90e</td>\n",
       "      <td>invited</td>\n",
       "      <td>2025-06-09 02:31:00.533790</td>\n",
       "      <td>6.152134</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0c4caf5b-d079-49bb-80ad-aa303da749f6</td>\n",
       "      <td>9f366ca7-e1b5-4db5-8e31-e9676a4165f2</td>\n",
       "      <td>39a2d3ae-c768-4bd6-b7c7-2d89f27a0968</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>2025-09-02 12:17:35.772288</td>\n",
       "      <td>5.526306</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6d597450-4b74-4960-ba89-ec35c816cb2c</td>\n",
       "      <td>da08c987-35b5-414f-8e51-300714fb2edb</td>\n",
       "      <td>e0d7517b-999c-4aa3-a8c7-19b26a7d4644</td>\n",
       "      <td>invited &amp; no</td>\n",
       "      <td>2025-06-13 22:34:14.975167</td>\n",
       "      <td>20.687302</td>\n",
       "      <td>(20, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d45245de-9c57-481a-a840-d12a4b220daa</td>\n",
       "      <td>05afca6c-914c-4f73-a29f-82b59efc163a</td>\n",
       "      <td>92b39250-3685-4a79-97ec-35434f1f143f</td>\n",
       "      <td>invited</td>\n",
       "      <td>2025-03-11 21:10:31.995579</td>\n",
       "      <td>17.244582</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7a976069-e90b-40cb-9c75-1a65caeba6be</td>\n",
       "      <td>031b412a-bf2b-4edc-9177-774af0d6f753</td>\n",
       "      <td>05e91fb2-6982-4989-8881-e25df3c71532</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>2025-07-07 20:26:55.940371</td>\n",
       "      <td>10.094918</td>\n",
       "      <td>(5, 20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id                               user_id  \\\n",
       "0  80c36646-fe7f-487f-bcba-248d663ff0bf  44814877-2f5d-4fef-a420-f305c0a33357   \n",
       "1  0c4caf5b-d079-49bb-80ad-aa303da749f6  9f366ca7-e1b5-4db5-8e31-e9676a4165f2   \n",
       "2  6d597450-4b74-4960-ba89-ec35c816cb2c  da08c987-35b5-414f-8e51-300714fb2edb   \n",
       "3  d45245de-9c57-481a-a840-d12a4b220daa  05afca6c-914c-4f73-a29f-82b59efc163a   \n",
       "4  7a976069-e90b-40cb-9c75-1a65caeba6be  031b412a-bf2b-4edc-9177-774af0d6f753   \n",
       "\n",
       "                               event_id interaction_type  \\\n",
       "0  7be347c2-7ba9-423a-9565-610d6b21d90e          invited   \n",
       "1  39a2d3ae-c768-4bd6-b7c7-2d89f27a0968  invited & maybe   \n",
       "2  e0d7517b-999c-4aa3-a8c7-19b26a7d4644     invited & no   \n",
       "3  92b39250-3685-4a79-97ec-35434f1f143f          invited   \n",
       "4  05e91fb2-6982-4989-8881-e25df3c71532    invited & yes   \n",
       "\n",
       "             interaction_time  distance_to_event distance_bin  \n",
       "0  2025-06-09 02:31:00.533790           6.152134      (5, 20]  \n",
       "1  2025-09-02 12:17:35.772288           5.526306      (5, 20]  \n",
       "2  2025-06-13 22:34:14.975167          20.687302     (20, 50]  \n",
       "3  2025-03-11 21:10:31.995579          17.244582      (5, 20]  \n",
       "4  2025-07-07 20:26:55.940371          10.094918      (5, 20]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_interactions.csv')\n",
    "interactions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>location_lon</th>\n",
       "      <th>city</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>temperature</th>\n",
       "      <th>historical_attendance_rate</th>\n",
       "      <th>indoor_capability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bd885fb7-6ab6-4033-9983-fa0929c4b5dd</td>\n",
       "      <td>Visionary heuristic orchestration Seasonal &amp; F...</td>\n",
       "      <td>Seasonal &amp; Festivals</td>\n",
       "      <td>40.626475</td>\n",
       "      <td>-74.061008</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-06-04 13:59:31.784973</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>18.4</td>\n",
       "      <td>20.656472</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>519d3580-b6a2-4c33-b24c-cbc479f3602d</td>\n",
       "      <td>Horizontal background customer loyalty Seasona...</td>\n",
       "      <td>Seasonal &amp; Festivals</td>\n",
       "      <td>-23.617500</td>\n",
       "      <td>-46.709916</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>2025-05-24 14:43:05.604231</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>28.9</td>\n",
       "      <td>48.875548</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26cb44b3-589b-4b39-be7a-265ff482ffd4</td>\n",
       "      <td>Persistent upward-trending hierarchy Education...</td>\n",
       "      <td>Education &amp; Learning</td>\n",
       "      <td>40.718535</td>\n",
       "      <td>-72.365064</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-08-25 19:46:52.947500</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>19.2</td>\n",
       "      <td>40.198146</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e77360aa-c46e-49fb-8f78-6b942cd41b91</td>\n",
       "      <td>Implemented static installation Education &amp; Le...</td>\n",
       "      <td>Education &amp; Learning</td>\n",
       "      <td>49.643585</td>\n",
       "      <td>0.210627</td>\n",
       "      <td>London</td>\n",
       "      <td>2025-06-15 18:10:08.978852</td>\n",
       "      <td>180</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.595591</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43d8e18c-fa2e-42f7-a7fa-ea0e144a2746</td>\n",
       "      <td>Inverse multi-tasking circuit Arts &amp; Culture i...</td>\n",
       "      <td>Arts &amp; Culture</td>\n",
       "      <td>19.140716</td>\n",
       "      <td>72.974755</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2025-04-17 09:48:38.589681</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>32.5</td>\n",
       "      <td>30.103833</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               event_id  \\\n",
       "0  bd885fb7-6ab6-4033-9983-fa0929c4b5dd   \n",
       "1  519d3580-b6a2-4c33-b24c-cbc479f3602d   \n",
       "2  26cb44b3-589b-4b39-be7a-265ff482ffd4   \n",
       "3  e77360aa-c46e-49fb-8f78-6b942cd41b91   \n",
       "4  43d8e18c-fa2e-42f7-a7fa-ea0e144a2746   \n",
       "\n",
       "                                               title            event_type  \\\n",
       "0  Visionary heuristic orchestration Seasonal & F...  Seasonal & Festivals   \n",
       "1  Horizontal background customer loyalty Seasona...  Seasonal & Festivals   \n",
       "2  Persistent upward-trending hierarchy Education...  Education & Learning   \n",
       "3  Implemented static installation Education & Le...  Education & Learning   \n",
       "4  Inverse multi-tasking circuit Arts & Culture i...        Arts & Culture   \n",
       "\n",
       "   location_lat  location_lon       city                  start_time  \\\n",
       "0     40.626475    -74.061008   New York  2025-06-04 13:59:31.784973   \n",
       "1    -23.617500    -46.709916  São Paulo  2025-05-24 14:43:05.604231   \n",
       "2     40.718535    -72.365064   New York  2025-08-25 19:46:52.947500   \n",
       "3     49.643585      0.210627     London  2025-06-15 18:10:08.978852   \n",
       "4     19.140716     72.974755     Mumbai  2025-04-17 09:48:38.589681   \n",
       "\n",
       "   duration weather_condition  temperature  historical_attendance_rate  \\\n",
       "0       120             Clear         18.4                   20.656472   \n",
       "1       120             Clear         28.9                   48.875548   \n",
       "2       180             Clear         19.2                   40.198146   \n",
       "3       180            Cloudy         13.5                   21.595591   \n",
       "4       120             Clear         32.5                   30.103833   \n",
       "\n",
       "   indoor_capability  \n",
       "0              False  \n",
       "1              False  \n",
       "2               True  \n",
       "3               True  \n",
       "4               True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = pd.read_csv('/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/synthetic_events.csv')\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42455"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_users_events = interactions[['event_id','user_id']].merge(users, on='user_id', how='inner') \\\n",
    "                                           .merge(events, on='event_id', how='inner')\n",
    "\n",
    "len(interaction_users_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>location_lat_x</th>\n",
       "      <th>location_lon_x</th>\n",
       "      <th>city_x</th>\n",
       "      <th>weather_preference</th>\n",
       "      <th>age</th>\n",
       "      <th>declared_interests</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>social_connectedness</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>location_lat_y</th>\n",
       "      <th>location_lon_y</th>\n",
       "      <th>city_y</th>\n",
       "      <th>start_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>historical_attendance_rate</th>\n",
       "      <th>indoor_capability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7fe13531-2c5f-4dd1-8fcf-a2ab793639f0</td>\n",
       "      <td>bf071cf2-9594-4bb8-b631-3936dce28e01</td>\n",
       "      <td>40.787850</td>\n",
       "      <td>-73.979098</td>\n",
       "      <td>New York</td>\n",
       "      <td>any</td>\n",
       "      <td>27</td>\n",
       "      <td>['music', 'travel']</td>\n",
       "      <td>2024-10-30 05:24:14.367302</td>\n",
       "      <td>13</td>\n",
       "      <td>Inverse analyzing flexibility Seasonal &amp; Festi...</td>\n",
       "      <td>Seasonal &amp; Festivals</td>\n",
       "      <td>40.750939</td>\n",
       "      <td>-74.061531</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-04-28 09:56:09.170428</td>\n",
       "      <td>240</td>\n",
       "      <td>Clear</td>\n",
       "      <td>22.808446</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17940102-d5c4-489c-854a-25ecc439dfa6</td>\n",
       "      <td>086c164a-e2e9-451d-9b83-1509a969b72b</td>\n",
       "      <td>52.604626</td>\n",
       "      <td>13.438382</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>any</td>\n",
       "      <td>39</td>\n",
       "      <td>['food', 'music', 'cinema', 'sports']</td>\n",
       "      <td>2024-08-23 20:38:57.363965</td>\n",
       "      <td>14</td>\n",
       "      <td>Public-key tangible contingency Sports &amp; Fitne...</td>\n",
       "      <td>Sports &amp; Fitness</td>\n",
       "      <td>52.452497</td>\n",
       "      <td>13.375187</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2025-08-01 09:44:58.106131</td>\n",
       "      <td>360</td>\n",
       "      <td>Clear</td>\n",
       "      <td>61.169269</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ad182f4-823c-48e1-9af5-0ac64e806ecb</td>\n",
       "      <td>f76ecafb-fc42-47a0-ad28-d6ae9061b18d</td>\n",
       "      <td>51.493821</td>\n",
       "      <td>-0.199043</td>\n",
       "      <td>London</td>\n",
       "      <td>indoor</td>\n",
       "      <td>42</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-03-27 01:11:13.720787</td>\n",
       "      <td>19</td>\n",
       "      <td>Balanced executive open system Entertainment i...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>51.592055</td>\n",
       "      <td>-0.142099</td>\n",
       "      <td>London</td>\n",
       "      <td>2025-07-01 19:58:34.853074</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>16.363479</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29b0111c-900e-4c04-ba0c-ae6e3dbaf8c4</td>\n",
       "      <td>3f0bda95-366a-4ccc-961b-f7ce45731749</td>\n",
       "      <td>51.506281</td>\n",
       "      <td>-0.154534</td>\n",
       "      <td>London</td>\n",
       "      <td>any</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-06-11 22:33:52.514397</td>\n",
       "      <td>12</td>\n",
       "      <td>Intuitive uniform workforce Entertainment in L...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>51.524089</td>\n",
       "      <td>-0.157917</td>\n",
       "      <td>London</td>\n",
       "      <td>2025-06-20 09:50:12.155368</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>57.868725</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4c5fdf3-ac81-485c-80ff-40ff7824f910</td>\n",
       "      <td>49c92ca8-6c05-4136-b5a5-ca48f91817cc</td>\n",
       "      <td>40.674816</td>\n",
       "      <td>-73.974169</td>\n",
       "      <td>New York</td>\n",
       "      <td>any</td>\n",
       "      <td>28</td>\n",
       "      <td>['literature', 'cinema']</td>\n",
       "      <td>2023-11-13 23:55:33.990432</td>\n",
       "      <td>19</td>\n",
       "      <td>Right-sized methodical Local Area Network Educ...</td>\n",
       "      <td>Education &amp; Learning</td>\n",
       "      <td>40.637339</td>\n",
       "      <td>-73.947771</td>\n",
       "      <td>New York</td>\n",
       "      <td>2025-06-30 18:32:57.854252</td>\n",
       "      <td>120</td>\n",
       "      <td>Clear</td>\n",
       "      <td>32.121606</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               event_id                               user_id  \\\n",
       "0  7fe13531-2c5f-4dd1-8fcf-a2ab793639f0  bf071cf2-9594-4bb8-b631-3936dce28e01   \n",
       "1  17940102-d5c4-489c-854a-25ecc439dfa6  086c164a-e2e9-451d-9b83-1509a969b72b   \n",
       "2  3ad182f4-823c-48e1-9af5-0ac64e806ecb  f76ecafb-fc42-47a0-ad28-d6ae9061b18d   \n",
       "3  29b0111c-900e-4c04-ba0c-ae6e3dbaf8c4  3f0bda95-366a-4ccc-961b-f7ce45731749   \n",
       "4  d4c5fdf3-ac81-485c-80ff-40ff7824f910  49c92ca8-6c05-4136-b5a5-ca48f91817cc   \n",
       "\n",
       "   location_lat_x  location_lon_x    city_x weather_preference  age  \\\n",
       "0       40.787850      -73.979098  New York                any   27   \n",
       "1       52.604626       13.438382    Berlin                any   39   \n",
       "2       51.493821       -0.199043    London             indoor   42   \n",
       "3       51.506281       -0.154534    London                any   38   \n",
       "4       40.674816      -73.974169  New York                any   28   \n",
       "\n",
       "                      declared_interests                 signup_date  \\\n",
       "0                    ['music', 'travel']  2024-10-30 05:24:14.367302   \n",
       "1  ['food', 'music', 'cinema', 'sports']  2024-08-23 20:38:57.363965   \n",
       "2                                     []  2024-03-27 01:11:13.720787   \n",
       "3                                     []  2024-06-11 22:33:52.514397   \n",
       "4               ['literature', 'cinema']  2023-11-13 23:55:33.990432   \n",
       "\n",
       "   social_connectedness                                              title  \\\n",
       "0                    13  Inverse analyzing flexibility Seasonal & Festi...   \n",
       "1                    14  Public-key tangible contingency Sports & Fitne...   \n",
       "2                    19  Balanced executive open system Entertainment i...   \n",
       "3                    12  Intuitive uniform workforce Entertainment in L...   \n",
       "4                    19  Right-sized methodical Local Area Network Educ...   \n",
       "\n",
       "             event_type  location_lat_y  location_lon_y    city_y  \\\n",
       "0  Seasonal & Festivals       40.750939      -74.061531  New York   \n",
       "1      Sports & Fitness       52.452497       13.375187    Berlin   \n",
       "2         Entertainment       51.592055       -0.142099    London   \n",
       "3         Entertainment       51.524089       -0.157917    London   \n",
       "4  Education & Learning       40.637339      -73.947771  New York   \n",
       "\n",
       "                   start_time  duration weather_condition  \\\n",
       "0  2025-04-28 09:56:09.170428       240             Clear   \n",
       "1  2025-08-01 09:44:58.106131       360             Clear   \n",
       "2  2025-07-01 19:58:34.853074       180             Clear   \n",
       "3  2025-06-20 09:50:12.155368       120             Clear   \n",
       "4  2025-06-30 18:32:57.854252       120             Clear   \n",
       "\n",
       "   historical_attendance_rate  indoor_capability  \n",
       "0                   22.808446              False  \n",
       "1                   61.169269              False  \n",
       "2                   16.363479               True  \n",
       "3                   57.868725               True  \n",
       "4                   32.121606               True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_users_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4169"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_users_events['event_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8628"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_users_events['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 users...\n",
      "Generating 5000 events...\n",
      "Generating 100000 base interactions...\n",
      "Distance distribution in sampling:\n",
      "0.0-5.0 km: 6383 samples (1.3%)\n",
      "5.0-20.0 km: 30395 samples (6.1%)\n",
      "20.0-50.0 km: 2131 samples (0.4%)\n",
      "50.0-100.0 km: 3478 samples (0.7%)\n",
      "100.0-200.0 km: 12059 samples (2.4%)\n",
      "200.0-inf km: 445554 samples (89.1%)\n",
      "\n",
      "Final distance distribution in interactions:\n",
      "count    17402.000000\n",
      "mean        13.638628\n",
      "std         30.182407\n",
      "min          0.034959\n",
      "25%          4.761164\n",
      "50%          7.969416\n",
      "75%         11.969957\n",
      "max        298.766857\n",
      "Name: distance_to_event, dtype: float64\n",
      "Adding cold-start interactions...\n",
      "\n",
      "=== Data Validation ===\n",
      "Total users: 10000\n",
      "Total events: 5000\n",
      "Total interactions: 42447\n",
      "City distribution:\n",
      "city\n",
      "New York     0.1979\n",
      "London       0.1541\n",
      "Paris        0.1036\n",
      "Mumbai       0.1002\n",
      "Tokyo        0.1000\n",
      "Toronto      0.0981\n",
      "Berlin       0.0937\n",
      "Dubai        0.0513\n",
      "Sydney       0.0512\n",
      "São Paulo    0.0499\n",
      "Name: proportion, dtype: float64\n",
      "Outdoor events with Clear weather: 83.1%\n",
      "\n",
      "Temperature Distribution:\n",
      "count    5000.000000\n",
      "mean       19.256240\n",
      "std         6.690218\n",
      "min         2.000000\n",
      "25%        14.400000\n",
      "50%        17.900000\n",
      "75%        23.400000\n",
      "max        37.000000\n",
      "Name: temperature, dtype: float64\n",
      "\n",
      "Temperature by City and Weather Condition:\n",
      "                                  mean   min   max\n",
      "city      weather_condition                       \n",
      "Berlin    Clear              17.501706  16.0  19.0\n",
      "          Cloudy             14.458824  13.0  16.0\n",
      "          Rain               12.359677  11.0  13.9\n",
      "          Snow                8.161905   6.1  10.5\n",
      "          Windy              13.332000  12.1  14.8\n",
      "Dubai     Clear              35.489796  34.0  37.0\n",
      "          Cloudy             32.473214  31.0  33.9\n",
      "          Rain               30.607500  29.1  32.0\n",
      "          Snow               26.520000  24.2  28.7\n",
      "          Windy              31.336364  30.1  32.5\n",
      "London    Clear              15.564878  14.0  17.0\n",
      "          Cloudy             12.466162  11.0  14.0\n",
      "          Rain               10.520755   9.1  12.0\n",
      "          Snow                7.000000   4.1   8.7\n",
      "          Windy              11.576923  10.0  12.8\n",
      "Mumbai    Clear              31.538462  30.0  33.0\n",
      "          Cloudy             28.507563  27.0  30.0\n",
      "          Rain               26.490667  25.0  27.9\n",
      "          Snow               22.255556  20.1  24.7\n",
      "          Windy              27.552941  26.3  28.8\n",
      "New York  Clear              18.506061  17.0  20.0\n",
      "          Cloudy             15.433333  14.0  17.0\n",
      "          Rain               13.242188  12.0  14.9\n",
      "          Snow                9.583333   7.5  11.8\n",
      "          Windy              14.492857  13.2  15.9\n",
      "Paris     Clear              19.513109  18.0  21.0\n",
      "          Cloudy             16.548175  15.0  18.0\n",
      "          Rain               14.457143  13.2  16.0\n",
      "          Snow               11.037500   8.5  12.9\n",
      "          Windy              15.575000  14.2  16.5\n",
      "Sydney    Clear              25.394964  24.0  26.9\n",
      "          Cloudy             22.374074  21.0  23.9\n",
      "          Rain               20.444737  19.0  22.0\n",
      "          Snow               16.625000  14.6  18.4\n",
      "          Windy              21.577778  20.1  22.9\n",
      "São Paulo Clear              27.561832  26.0  29.0\n",
      "          Cloudy             24.308065  23.0  26.0\n",
      "          Rain               22.364706  21.0  23.8\n",
      "          Snow               18.758333  16.0  20.9\n",
      "          Windy              23.936364  22.3  24.9\n",
      "Tokyo     Clear              23.435587  22.0  25.0\n",
      "          Cloudy             20.509150  19.1  22.0\n",
      "          Rain               18.605769  17.1  20.0\n",
      "          Snow               14.569231  12.2  16.9\n",
      "          Windy              19.814286  18.6  20.8\n",
      "Toronto   Clear              13.430996  12.0  15.0\n",
      "          Cloudy             10.465812   9.0  11.9\n",
      "          Rain                8.529412   7.0   9.9\n",
      "          Snow                4.009091   2.0   6.6\n",
      "          Windy               9.193750   8.1  10.6\n",
      "\n",
      "Distance Decay Pattern Analysis:\n",
      "  distance_bin  positive_rate\n",
      "0       (0, 5]       0.439968\n",
      "1      (5, 20]       0.354489\n",
      "2     (20, 50]       0.231076\n",
      "3    (50, 100]       0.141732\n",
      "4   (100, 200]       0.100000\n",
      "\n",
      "First bin ((0, 5]): 44.00% positive response rate\n",
      "Last bin ((100, 200]): 10.00% positive response rate\n",
      "Decay ratio: 4.40x\n",
      "Cold-start users with 3+ interactions: 100.0%\n",
      "=== Validation Successful ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129432/1725464201.py:357: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bin_stats = interactions.groupby('distance_bin')['interaction_type'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated:\n",
      "- 10000 users\n",
      "- 5000 events\n",
      "- 42447 interactions (including 25045 cold-start interactions)\n",
      "Files saved to .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import skewnorm, dirichlet\n",
    "import os\n",
    "\n",
    "# Initialize Faker and set seed for reproducibility\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# City coordinates and probabilities\n",
    "cities = ['New York', 'London', 'Paris', 'Tokyo', 'Sydney', 'Berlin', 'Mumbai', 'São Paulo', 'Toronto', 'Dubai']\n",
    "city_probs = [0.2, 0.15, 0.1, 0.1, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05]\n",
    "# Verify that probabilities sum to 1.0\n",
    "if not np.isclose(sum(city_probs), 1.0):\n",
    "    raise ValueError(f\"City probabilities must sum to 1.0, but sum to {sum(city_probs)}\")\n",
    "\n",
    "city_coords = {\n",
    "    'New York': (40.7128, -74.0060), 'London': (51.5074, -0.1278), 'Paris': (48.8566, 2.3522),\n",
    "    'Tokyo': (35.6762, 139.6503), 'Sydney': (-33.8688, 151.2093), 'Berlin': (52.5200, 13.4050),\n",
    "    'Mumbai': (19.0760, 72.8777), 'São Paulo': (-23.5505, -46.6333), 'Toronto': (43.6532, -79.3832),\n",
    "    'Dubai': (25.2048, 55.2708)\n",
    "}\n",
    "\n",
    "def generate_location(city):\n",
    "    \"\"\"Generate random coordinates near a city center, with occasional outliers.\"\"\"\n",
    "    if city not in city_coords:\n",
    "        raise ValueError(f\"Unknown city: {city}\")\n",
    "        \n",
    "    base_lat, base_lon = city_coords[city]\n",
    "    if random.random() < 0.8:\n",
    "        # Most locations are near city center\n",
    "        lat = base_lat + np.random.uniform(-0.1, 0.1)\n",
    "        lon = base_lon + np.random.uniform(-0.1, 0.1)\n",
    "    else:\n",
    "        # Some locations are farther out (suburbs or neighboring areas)\n",
    "        lat = base_lat + np.random.uniform(-2, 2)\n",
    "        lon = base_lon + np.random.uniform(-2, 2)\n",
    "    return lat, lon\n",
    "\n",
    "def generate_users(n_users=20000):\n",
    "    \"\"\"Generate synthetic user data.\"\"\"\n",
    "    if n_users <= 0:\n",
    "        raise ValueError(\"Number of users must be positive\")\n",
    "        \n",
    "    users = []\n",
    "    interests = ['music', 'sports', 'tech', 'food', 'art', 'literature', 'cinema', 'travel', 'fitness', 'fashion']\n",
    "    \n",
    "    for _ in range(n_users):\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Ensure age is reasonable (avoid negative values from skewnorm)\n",
    "        age = max(18, min(100, int(skewnorm.rvs(5, loc=25, scale=15))))\n",
    "        \n",
    "        # Generate weather preference probabilities\n",
    "        weather_probs = dirichlet.rvs([0.3, 0.5, 0.2])[0]\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': fake.uuid4(),\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'weather_preference': np.random.choice(['indoor', 'outdoor', 'any'], p=weather_probs),\n",
    "            'age': age,\n",
    "            'declared_interests': random.sample(interests, k=random.randint(0, min(4, len(interests)))) if random.random() < 0.7 else [],\n",
    "            'signup_date': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'social_connectedness': np.random.poisson(lam=15)\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "def generate_events(n_events=5000):\n",
    "    \"\"\"Generate synthetic event data.\"\"\"\n",
    "    if n_events <= 0:\n",
    "        raise ValueError(\"Number of events must be positive\")\n",
    "        \n",
    "    events = []\n",
    "    # Updated event types\n",
    "    event_types = [\n",
    "        'Education & Learning', 'Technology', 'Seasonal & Festivals', 'Arts & Culture', \n",
    "        'Entertainment', 'Sports & Fitness', 'Business & Networking', 'Health & Wellness', \n",
    "        'Music & Concerts', 'Food & Drink', 'Community & Causes', 'Immersive Experiences'\n",
    "    ]\n",
    "    weather_conditions = ['Clear', 'Rain', 'Snow', 'Cloudy', 'Windy']\n",
    "    weather_probs = [0.5, 0.2, 0.05, 0.2, 0.05]\n",
    "    \n",
    "    # Verify weather probabilities sum to 1\n",
    "    if not np.isclose(sum(weather_probs), 1.0):\n",
    "        raise ValueError(f\"Weather probabilities must sum to 1.0, but sum to {sum(weather_probs)}\")\n",
    "    \n",
    "    for _ in range(n_events):\n",
    "        event_type = np.random.choice(event_types)\n",
    "        city = np.random.choice(cities, p=city_probs)\n",
    "        lat, lon = generate_location(city)\n",
    "        \n",
    "        # Weather conditions with conditional probabilities based on event type\n",
    "        if event_type in ['Sports & Fitness', 'Seasonal & Festivals']:\n",
    "            # Ensure most outdoor events have Clear weather to meet validation requirements\n",
    "            weather_condition = 'Clear' if random.random() < 0.8 else np.random.choice(['Rain', 'Cloudy'])\n",
    "        elif event_type in ['Education & Learning', 'Technology', 'Business & Networking']:\n",
    "            weather_condition = np.random.choice(['Clear', 'Cloudy'])\n",
    "        else:\n",
    "            weather_condition = np.random.choice(weather_conditions, p=weather_probs)\n",
    "        \n",
    "        # Generate temperature based on location and weather condition (Celsius)\n",
    "        base_temp = {\n",
    "            'New York': 15, 'London': 12, 'Paris': 16, 'Tokyo': 20, \n",
    "            'Sydney': 22, 'Berlin': 14, 'Mumbai': 28, 'São Paulo': 24, \n",
    "            'Toronto': 10, 'Dubai': 32\n",
    "        }[city]\n",
    "        \n",
    "        # Adjust temperature based on weather condition\n",
    "        temp_adjustment = {\n",
    "            'Clear': np.random.uniform(2, 5),\n",
    "            'Rain': np.random.uniform(-3, 0),\n",
    "            'Snow': np.random.uniform(-8, -3),\n",
    "            'Cloudy': np.random.uniform(-1, 2),\n",
    "            'Windy': np.random.uniform(-2, 1)\n",
    "        }[weather_condition]\n",
    "        \n",
    "        temperature = round(base_temp + temp_adjustment, 1)\n",
    "        \n",
    "        # Generate start time with reasonable hours based on weekday/weekend\n",
    "        start_time = fake.date_time_between(start_date='now', end_date='+6M')\n",
    "        is_weekend = start_time.weekday() >= 5\n",
    "        hour_choices = [10, 14, 18] if is_weekend else [9, 13, 18, 19]\n",
    "        start_time = start_time.replace(hour=np.random.choice(hour_choices))\n",
    "        \n",
    "        # Generate other event attributes\n",
    "        events.append({\n",
    "            'event_id': fake.uuid4(),\n",
    "            'title': f\"{fake.catch_phrase()} {event_type} in {city}\",\n",
    "            'event_type': event_type,\n",
    "            'location_lat': lat,\n",
    "            'location_lon': lon,\n",
    "            'city': city,\n",
    "            'start_time': start_time,\n",
    "            'duration': np.random.choice([120, 180, 240, 360, 480]),  # Duration in minutes\n",
    "            'weather_condition': weather_condition,\n",
    "            'temperature': temperature,\n",
    "            'historical_attendance_rate': np.random.beta(a=2, b=5) * 100,  # Percentage\n",
    "            'indoor_capability': event_type in ['Education & Learning', 'Technology', 'Business & Networking', \n",
    "                                               'Arts & Culture', 'Entertainment', 'Immersive Experiences']\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def calculate_time_weight(interaction_time, current_time, half_life=30):\n",
    "    time_diff = (current_time - interaction_time).days\n",
    "    return np.exp(np.log(0.5) * time_diff / half_life)\n",
    "\n",
    "def generate_interactions(users, events, n_interactions=100000):\n",
    "    \"\"\"Generate synthetic user-event interactions with strong distance decay pattern.\"\"\"\n",
    "    if n_interactions <= 0:\n",
    "        raise ValueError(\"Number of interactions must be positive\")\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    interactions = []\n",
    "    \n",
    "    # Updated interaction types\n",
    "    interaction_types = ['maybe', 'invited & maybe', 'no', 'yes', 'invited & yes', 'invited & no', 'invited']\n",
    "    \n",
    "    # Track distance distribution for reporting\n",
    "    all_distances = []\n",
    "    \n",
    "    # Sample more potential interactions but only keep ones meeting our criteria\n",
    "    attempts = n_interactions * 5  # Sample more to account for filtering\n",
    "    \n",
    "    for _ in range(attempts):\n",
    "        if len(interactions) >= n_interactions:\n",
    "            break\n",
    "            \n",
    "        # Sample a random user and event\n",
    "        user = users.sample(1).iloc[0]\n",
    "        event = events.sample(1).iloc[0]\n",
    "        \n",
    "        # Calculate distance between user and event\n",
    "        distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                           (event['location_lat'], event['location_lon'])).km\n",
    "        all_distances.append(distance)\n",
    "        \n",
    "        # Much stronger distance decay formula\n",
    "        # Use a steeper exponential decay for distance\n",
    "        distance_score = np.exp(-distance/10)  # Reduced from /5 to /20 for stronger decay\n",
    "        \n",
    "        # Calculate other factors\n",
    "        weather_score = 1.2 if (event['weather_condition'] == 'Clear' and \n",
    "                               user['weather_preference'] in ['outdoor', 'any']) else 0.5\n",
    "        social_score = np.log1p(user['social_connectedness']) / 10\n",
    "        \n",
    "        # Give much more weight to distance in the interaction probability\n",
    "        interaction_prob = 0.85*distance_score + 0.1*weather_score + 0.05*social_score\n",
    "\n",
    "        # Adjust max distance based on interaction probability\n",
    "        max_distance = 50 if random.random() < 0.7 else 300\n",
    "        \n",
    "        # Create interaction with probability more strongly influenced by distance\n",
    "        if distance < max_distance and (random.random() < interaction_prob):\n",
    "            interaction_time = fake.date_time_between(\n",
    "                start_date=event['start_time'] - timedelta(days=30), \n",
    "                end_date=event['start_time']\n",
    "            )\n",
    "            # Add the suggested time weighting\n",
    "            current_time = datetime(2025, 3, 22, 19, 10, 0)  # Use the provided date and time\n",
    "            time_weight = calculate_time_weight(interaction_time, current_time)\n",
    "            interaction_prob *= time_weight\n",
    "            \n",
    "            # Determine probabilities for different interaction types based on distance\n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.15, 0.20, 0.05, 0.25, 0.20, 0.05, 0.10]  # High positive response for very close events\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.20, 0.15, 0.10, 0.20, 0.15, 0.10, 0.10]  # Good mix for nearby events\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.25, 0.10, 0.15, 0.15, 0.10, 0.15, 0.10]  # More maybe/no for medium distance\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.20, 0.05, 0.25, 0.10, 0.05, 0.20, 0.15]  # Higher no rate for longer distance\n",
    "            else:\n",
    "                interaction_type_probs = [0.15, 0.05, 0.30, 0.05, 0.05, 0.25, 0.15]  # Highest no rate for distant events\n",
    "                \n",
    "            interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(interaction_types, p=interaction_type_probs),\n",
    "                'interaction_time': interaction_time,\n",
    "                'distance_to_event': distance\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(interactions)\n",
    "    \n",
    "    # Report distance distribution if enough interactions\n",
    "    if all_distances:\n",
    "        print(\"Distance distribution in sampling:\")\n",
    "        dist_bins = [0, 5, 20, 50, 100, 200, np.inf]\n",
    "        hist, edges = np.histogram(all_distances, bins=dist_bins)\n",
    "        for i in range(len(hist)):\n",
    "            print(f\"{edges[i]:.1f}-{edges[i+1] if edges[i+1] != np.inf else 'inf'} km: {hist[i]} samples ({hist[i]/len(all_distances):.1%})\")\n",
    "    \n",
    "    # Limit to requested number of interactions\n",
    "    if len(result_df) > n_interactions:\n",
    "        result_df = result_df.sample(n_interactions)\n",
    "        \n",
    "    # Report final distance statistics for accepted interactions\n",
    "    if not result_df.empty:\n",
    "        print(\"\\nFinal distance distribution in interactions:\")\n",
    "        print(result_df['distance_to_event'].describe())\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def augment_cold_start(users, events, interactions):\n",
    "    \"\"\"Add interactions for new users to help with cold-start problem.\"\"\"\n",
    "    if users.empty or events.empty:\n",
    "        raise ValueError(\"Users and events DataFrames cannot be empty\")\n",
    "        \n",
    "    # Get users with declared interests (potential cold start users)\n",
    "    cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "    if cold_users.empty:\n",
    "        print(\"Warning: No users with declared interests found for cold start augmentation\")\n",
    "        return interactions\n",
    "        \n",
    "    # Get trending events\n",
    "    trending_events = events.nlargest(min(100, len(events)), 'historical_attendance_rate')\n",
    "    \n",
    "    # Updated interaction types\n",
    "    interaction_types = ['maybe', 'invited & maybe', 'no', 'yes', 'invited & yes', 'invited & no', 'invited']\n",
    "    \n",
    "    cold_interactions = []\n",
    "    for _, user in cold_users.iterrows():\n",
    "        # Ensure sufficient interactions for cold-start users (at least 3)\n",
    "        for _ in range(max(3, random.randint(3, 6))):\n",
    "            event = trending_events.sample(1).iloc[0]\n",
    "            \n",
    "            # Calculate distance\n",
    "            distance = geodesic((user['location_lat'], user['location_lon']), \n",
    "                              (event['location_lat'], event['location_lon'])).km\n",
    "            \n",
    "            # Add similar distance-based probabilities for cold start interactions\n",
    "            if distance <= 5:\n",
    "                interaction_type_probs = [0.15, 0.20, 0.05, 0.25, 0.20, 0.05, 0.10]  # High positive response for very close events\n",
    "            elif distance <= 20:\n",
    "                interaction_type_probs = [0.20, 0.15, 0.10, 0.20, 0.15, 0.10, 0.10]  # Good mix for nearby events\n",
    "            elif distance <= 50:\n",
    "                interaction_type_probs = [0.25, 0.10, 0.15, 0.15, 0.10, 0.15, 0.10]  # More maybe/no for medium distance\n",
    "            elif distance <= 100:\n",
    "                interaction_type_probs = [0.20, 0.05, 0.25, 0.10, 0.05, 0.20, 0.15]  # Higher no rate for longer distance\n",
    "            else:\n",
    "                interaction_type_probs = [0.15, 0.05, 0.30, 0.05, 0.05, 0.25, 0.15]  # Highest no rate for distant events\n",
    "                \n",
    "            cold_interactions.append({\n",
    "                'interaction_id': fake.uuid4(),\n",
    "                'user_id': user['user_id'],\n",
    "                'event_id': event['event_id'],\n",
    "                'interaction_type': np.random.choice(interaction_types, p=interaction_type_probs),\n",
    "                'interaction_time': fake.date_time_between(\n",
    "                    start_date=user['signup_date'], \n",
    "                    end_date=user['signup_date'] + timedelta(days=7)\n",
    "                ),\n",
    "                'distance_to_event': round(distance)\n",
    "            })\n",
    "    \n",
    "    # Combine with original interactions\n",
    "    return pd.concat([interactions, pd.DataFrame(cold_interactions)], ignore_index=True)\n",
    "\n",
    "def validate_data(users, events, interactions):\n",
    "    \"\"\"\n",
    "    Validate the generated data to ensure it meets quality requirements.\n",
    "    Raises AssertionError if validation fails.\n",
    "    \"\"\"\n",
    "    print(f\"Total users: {len(users)}\")\n",
    "    print(f\"Total events: {len(events)}\")\n",
    "    print(f\"Total interactions: {len(interactions)}\")\n",
    "    \n",
    "    # Check user-event ratio\n",
    "    assert len(users) > len(events), \"There should be more users than events\"\n",
    "    \n",
    "    # Check city distribution\n",
    "    city_distribution = users['city'].value_counts(normalize=True)\n",
    "    print(\"City distribution:\")\n",
    "    print(city_distribution)\n",
    "    assert len(city_distribution) == 10, \"Should have 10 cities\"\n",
    "    \n",
    "    # Check weather-event alignment\n",
    "    outdoor_events = events[events['event_type'].isin(['Sports & Fitness', 'Seasonal & Festivals'])]\n",
    "    outdoor_clear_rate = (outdoor_events['weather_condition'] == 'Clear').mean()\n",
    "    print(f\"Outdoor events with Clear weather: {outdoor_clear_rate:.1%}\")\n",
    "    assert outdoor_clear_rate > 0.7, f\"Too few outdoor events have Clear weather ({outdoor_clear_rate:.1%})\"\n",
    "    \n",
    "    # Check temperature distribution\n",
    "    print(\"\\nTemperature Distribution:\")\n",
    "    print(events['temperature'].describe())\n",
    "    \n",
    "    # Consistency check for temperature by city and weather condition\n",
    "    temp_by_city_weather = events.groupby(['city', 'weather_condition'])['temperature'].agg(['mean', 'min', 'max'])\n",
    "    print(\"\\nTemperature by City and Weather Condition:\")\n",
    "    print(temp_by_city_weather)\n",
    "    \n",
    "    # Check distance decay pattern in interaction types\n",
    "    try:\n",
    "        # More robust distance binning approach\n",
    "        distance_bins = [0, 5, 20, 50, 100, 200]\n",
    "        bin_labels = [f\"({distance_bins[i]}, {distance_bins[i+1]}]\" for i in range(len(distance_bins)-1)]\n",
    "        \n",
    "        # Create a categorical bin column\n",
    "        interactions['distance_bin'] = pd.cut(\n",
    "            interactions['distance_to_event'], \n",
    "            bins=distance_bins,\n",
    "            labels=bin_labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        # Compute aggregated positive response rates by distance bin\n",
    "        # Considering 'yes' and 'invited & yes' as positive responses\n",
    "        bin_stats = interactions.groupby('distance_bin')['interaction_type'].apply(\n",
    "            lambda x: (x.isin(['yes', 'invited & yes'])).mean()\n",
    "        ).reset_index(name='positive_rate')\n",
    "        \n",
    "        # Display results for all bins\n",
    "        print(\"\\nDistance Decay Pattern Analysis:\")\n",
    "        print(bin_stats)\n",
    "        \n",
    "        # Only compare bins that have data\n",
    "        valid_bins = bin_stats['distance_bin'].dropna().tolist()\n",
    "        if len(valid_bins) >= 2:\n",
    "            first_bin = valid_bins[0]\n",
    "            last_bin = valid_bins[-1]\n",
    "            \n",
    "            first_bin_rate = bin_stats.loc[bin_stats['distance_bin'] == first_bin, 'positive_rate'].values[0]\n",
    "            last_bin_rate = bin_stats.loc[bin_stats['distance_bin'] == last_bin, 'positive_rate'].values[0]\n",
    "            \n",
    "            print(f\"\\nFirst bin ({first_bin}): {first_bin_rate:.2%} positive response rate\")\n",
    "            print(f\"Last bin ({last_bin}): {last_bin_rate:.2%} positive response rate\")\n",
    "            \n",
    "            if first_bin_rate > 0 and last_bin_rate > 0:\n",
    "                decay_ratio = first_bin_rate / last_bin_rate\n",
    "                print(f\"Decay ratio: {decay_ratio:.2f}x\")\n",
    "                \n",
    "                # Better validation criterion\n",
    "                assert decay_ratio > 1.75, f\"Distance decay ratio ({decay_ratio:.2f}) is too low\"\n",
    "            else:\n",
    "                print(\"Warning: Cannot calculate decay ratio due to zero values\")\n",
    "        else:\n",
    "            print(\"Warning: Not enough distance bins for analysis\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not analyze distance decay pattern: {e}\")\n",
    "        raise  # Re-raise to ensure validation fails if this check fails\n",
    "    \n",
    "    # Check cold-start coverage\n",
    "    try:\n",
    "        cold_users = users[users['declared_interests'].apply(len) > 0]\n",
    "        if not cold_users.empty:\n",
    "            cold_user_ids = set(cold_users['user_id'])\n",
    "            cold_interaction_counts = (interactions[interactions['user_id'].isin(cold_user_ids)]\n",
    "                                      .groupby('user_id').size())\n",
    "            \n",
    "            cold_coverage = (cold_interaction_counts >= 3).mean() if not cold_interaction_counts.empty else 0\n",
    "            print(f\"Cold-start users with 3+ interactions: {cold_coverage:.1%}\")\n",
    "            assert cold_coverage > 0.8, f\"Cold-start coverage insufficient: {cold_coverage:.1%}\"\n",
    "        else:\n",
    "            print(\"Warning: No cold-start users identified\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not validate cold-start coverage: {e}\")\n",
    "        raise  # Re-raise to ensure validation fails\n",
    "\n",
    "def main(output_dir='.', n_users=10000, n_events=5000, n_interactions=100000, validate=True):\n",
    "    \"\"\"Main function to generate and save all synthetic datasets.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Generating {n_users} users...\")\n",
    "        users_df = generate_users(n_users)\n",
    "        \n",
    "        print(f\"Generating {n_events} events...\")\n",
    "        events_df = generate_events(n_events)\n",
    "        \n",
    "        print(f\"Generating {n_interactions} base interactions...\")\n",
    "        interactions_df = generate_interactions(users_df, events_df, n_interactions)\n",
    "        \n",
    "        print(\"Adding cold-start interactions...\")\n",
    "        full_interactions = augment_cold_start(users_df, events_df, interactions_df)\n",
    "        \n",
    "        # Validate data if requested\n",
    "        if validate:\n",
    "            print(\"\\n=== Data Validation ===\")\n",
    "            validate_data(users_df, events_df, full_interactions)\n",
    "            print(\"=== Validation Successful ===\\n\")\n",
    "        \n",
    "        # Save to CSV files\n",
    "        users_df.to_csv(os.path.join(output_dir, 'synthetic_users.csv'), index=False)\n",
    "        events_df.to_csv(os.path.join(output_dir, 'synthetic_events.csv'), index=False)\n",
    "        full_interactions.to_csv(os.path.join(output_dir, 'synthetic_interactions.csv'), index=False)\n",
    "        \n",
    "        print(f\"Successfully generated:\")\n",
    "        print(f\"- {len(users_df)} users\")\n",
    "        print(f\"- {len(events_df)} events\")\n",
    "        print(f\"- {len(full_interactions)} interactions (including {len(full_interactions) - len(interactions_df)} cold-start interactions)\")\n",
    "        print(f\"Files saved to {output_dir}\")\n",
    "        \n",
    "        return users_df, events_df, full_interactions\n",
    "        \n",
    "    except AssertionError as ae:\n",
    "        print(f\"Validation failed: {ae}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data generation: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
