{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "      <th>users_user_lat</th>\n",
       "      <th>users_user_lon</th>\n",
       "      <th>users_user_city</th>\n",
       "      <th>users_user_weather_preference</th>\n",
       "      <th>...</th>\n",
       "      <th>events_event_lat</th>\n",
       "      <th>events_event_lon</th>\n",
       "      <th>events_event_city</th>\n",
       "      <th>events_start_time</th>\n",
       "      <th>events_duration</th>\n",
       "      <th>events_weather_condition</th>\n",
       "      <th>events_temperature</th>\n",
       "      <th>events_attendance_rate</th>\n",
       "      <th>events_event_indoor_capability</th>\n",
       "      <th>interaction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0794ae31-b867-4bd8-84f3-763cdcbb81d6</td>\n",
       "      <td>NW814V</td>\n",
       "      <td>ZG645S</td>\n",
       "      <td>no</td>\n",
       "      <td>2023-06-13 06:55:37.728541+00:00</td>\n",
       "      <td>5810.000000</td>\n",
       "      <td>51.409589</td>\n",
       "      <td>-0.109415</td>\n",
       "      <td>London</td>\n",
       "      <td>outdoor</td>\n",
       "      <td>...</td>\n",
       "      <td>42.145335</td>\n",
       "      <td>-78.925101</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>2025-03-30 10:06:51.523437+00:00</td>\n",
       "      <td>240</td>\n",
       "      <td>Rain</td>\n",
       "      <td>9.1</td>\n",
       "      <td>83.238042</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fa403386-53fc-4f68-8fcf-6791931bde80</td>\n",
       "      <td>UT633Z</td>\n",
       "      <td>RE468E</td>\n",
       "      <td>no</td>\n",
       "      <td>2024-07-14 07:19:15.832112+00:00</td>\n",
       "      <td>7785.000000</td>\n",
       "      <td>35.598507</td>\n",
       "      <td>139.616940</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>indoor</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.875733</td>\n",
       "      <td>151.218054</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>2025-08-14 19:26:31.045513+00:00</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>26.9</td>\n",
       "      <td>75.096523</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bd358101-d7a1-43b7-b12f-c2af4f092990</td>\n",
       "      <td>QZ993B</td>\n",
       "      <td>FI102Q</td>\n",
       "      <td>invited</td>\n",
       "      <td>2023-05-30 03:55:48.888268+00:00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-33.951739</td>\n",
       "      <td>151.180575</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>indoor</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.922051</td>\n",
       "      <td>151.197462</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>2025-05-06 13:09:21.884209+00:00</td>\n",
       "      <td>480</td>\n",
       "      <td>Clear</td>\n",
       "      <td>25.6</td>\n",
       "      <td>80.023992</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61eadaa7-579b-437f-aa60-567d248d4625</td>\n",
       "      <td>GC832P</td>\n",
       "      <td>RB514G</td>\n",
       "      <td>maybe</td>\n",
       "      <td>2024-05-15 02:05:11.333627+00:00</td>\n",
       "      <td>817.000000</td>\n",
       "      <td>52.507163</td>\n",
       "      <td>13.432964</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>indoor</td>\n",
       "      <td>...</td>\n",
       "      <td>51.165868</td>\n",
       "      <td>1.764638</td>\n",
       "      <td>London</td>\n",
       "      <td>2025-05-23 09:08:35.832697+00:00</td>\n",
       "      <td>120</td>\n",
       "      <td>Rain</td>\n",
       "      <td>11.3</td>\n",
       "      <td>75.496348</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OC899Q</td>\n",
       "      <td>CD621U</td>\n",
       "      <td>LD592D</td>\n",
       "      <td>maybe</td>\n",
       "      <td>2025-04-08 01:40:37.708575+00:00</td>\n",
       "      <td>6.407995</td>\n",
       "      <td>48.843322</td>\n",
       "      <td>2.255623</td>\n",
       "      <td>Paris</td>\n",
       "      <td>indoor</td>\n",
       "      <td>...</td>\n",
       "      <td>48.787649</td>\n",
       "      <td>2.278127</td>\n",
       "      <td>Paris</td>\n",
       "      <td>2025-04-18 13:22:40.369846+00:00</td>\n",
       "      <td>180</td>\n",
       "      <td>Clear</td>\n",
       "      <td>18.9</td>\n",
       "      <td>22.043940</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id user_id event_id interaction_type  \\\n",
       "0  0794ae31-b867-4bd8-84f3-763cdcbb81d6  NW814V   ZG645S               no   \n",
       "1  fa403386-53fc-4f68-8fcf-6791931bde80  UT633Z   RE468E               no   \n",
       "2  bd358101-d7a1-43b7-b12f-c2af4f092990  QZ993B   FI102Q          invited   \n",
       "3  61eadaa7-579b-437f-aa60-567d248d4625  GC832P   RB514G            maybe   \n",
       "4                                OC899Q  CD621U   LD592D            maybe   \n",
       "\n",
       "                   interaction_time  interaction_distance_to_event  \\\n",
       "0  2023-06-13 06:55:37.728541+00:00                    5810.000000   \n",
       "1  2024-07-14 07:19:15.832112+00:00                    7785.000000   \n",
       "2  2023-05-30 03:55:48.888268+00:00                       4.000000   \n",
       "3  2024-05-15 02:05:11.333627+00:00                     817.000000   \n",
       "4  2025-04-08 01:40:37.708575+00:00                       6.407995   \n",
       "\n",
       "   users_user_lat  users_user_lon users_user_city  \\\n",
       "0       51.409589       -0.109415          London   \n",
       "1       35.598507      139.616940           Tokyo   \n",
       "2      -33.951739      151.180575          Sydney   \n",
       "3       52.507163       13.432964          Berlin   \n",
       "4       48.843322        2.255623           Paris   \n",
       "\n",
       "  users_user_weather_preference  ...  events_event_lat events_event_lon  \\\n",
       "0                       outdoor  ...         42.145335       -78.925101   \n",
       "1                        indoor  ...        -33.875733       151.218054   \n",
       "2                        indoor  ...        -33.922051       151.197462   \n",
       "3                        indoor  ...         51.165868         1.764638   \n",
       "4                        indoor  ...         48.787649         2.278127   \n",
       "\n",
       "  events_event_city                 events_start_time events_duration  \\\n",
       "0           Toronto  2025-03-30 10:06:51.523437+00:00             240   \n",
       "1            Sydney  2025-08-14 19:26:31.045513+00:00             180   \n",
       "2            Sydney  2025-05-06 13:09:21.884209+00:00             480   \n",
       "3            London  2025-05-23 09:08:35.832697+00:00             120   \n",
       "4             Paris  2025-04-18 13:22:40.369846+00:00             180   \n",
       "\n",
       "  events_weather_condition  events_temperature  events_attendance_rate  \\\n",
       "0                     Rain                 9.1               83.238042   \n",
       "1                    Clear                26.9               75.096523   \n",
       "2                    Clear                25.6               80.023992   \n",
       "3                     Rain                11.3               75.496348   \n",
       "4                    Clear                18.9               22.043940   \n",
       "\n",
       "  events_event_indoor_capability interaction_label  \n",
       "0                           True                 0  \n",
       "1                          False                 0  \n",
       "2                          False                 0  \n",
       "3                          False                 1  \n",
       "4                          False                 1  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/hospworks_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['interaction_id', 'user_id', 'event_id', 'interaction_type',\n",
       "       'interaction_time', 'interaction_distance_to_event', 'users_user_lat',\n",
       "       'users_user_lon', 'users_user_city', 'users_user_weather_preference',\n",
       "       'users_age', 'users_user_interests', 'users_signup_date',\n",
       "       'users_social_connectedness', 'events_title', 'events_event_type',\n",
       "       'events_event_lat', 'events_event_lon', 'events_event_city',\n",
       "       'events_start_time', 'events_duration', 'events_weather_condition',\n",
       "       'events_temperature', 'events_attendance_rate',\n",
       "       'events_event_indoor_capability', 'interaction_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75202"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"interaction_label\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-09-22 19:37:51.648789+0000', tz='UTC')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"events_start_time\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165788"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:1000,:]\n",
    "data.to_csv(\"test_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "interactions_df = pd.read_csv(\"/home/nkama/masters_thesis_project/interactions.csv\")\n",
    "users_df = pd.read_csv(\"/home/nkama/masters_thesis_project/users.csv\")\n",
    "events_df = pd.read_csv(\"/home/nkama/masters_thesis_project/events.csv\")\n",
    "interactions_df.head(2)\n",
    "interactions_df[\"interaction_label\"] = interactions_df['interaction_type'].apply(\n",
    "    lambda x: 1 if x in ['maybe', 'invited & maybe', 'yes', 'invited & yes'] else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.drop(columns=\"Unnamed: 0\", inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "      <th>interaction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CJ011J</td>\n",
       "      <td>PG158Y</td>\n",
       "      <td>PP391M</td>\n",
       "      <td>invited &amp; maybe</td>\n",
       "      <td>2025-08-14 19:16:49.777185</td>\n",
       "      <td>10.675627</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO426K</td>\n",
       "      <td>HL067E</td>\n",
       "      <td>RC216V</td>\n",
       "      <td>invited &amp; yes</td>\n",
       "      <td>2025-06-10 13:20:14.014146</td>\n",
       "      <td>6.735536</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interaction_id user_id event_id interaction_type  \\\n",
       "0         CJ011J  PG158Y   PP391M  invited & maybe   \n",
       "1         NO426K  HL067E   RC216V    invited & yes   \n",
       "\n",
       "             interaction_time  interaction_distance_to_event  \\\n",
       "0  2025-08-14 19:16:49.777185                      10.675627   \n",
       "1  2025-06-10 13:20:14.014146                       6.735536   \n",
       "\n",
       "   interaction_label  \n",
       "0                  1  \n",
       "1                  1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.to_csv(\"interactions_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_common(interactions_df, users_df, events_df):\n",
    " \n",
    "    # Create copies of the input dataframes\n",
    "    interactions_df = interactions_df.copy()\n",
    "    users_df = users_df.copy()\n",
    "    events_df = events_df.copy()\n",
    "\n",
    "    # Drop rows with missing user_id or event_id\n",
    "    interactions_df = interactions_df.dropna(subset=[\"user_id\", \"event_id\", \"interaction_label\"])\n",
    "    \n",
    "    # Convert distance_to_event to float\n",
    "    interactions_df[\"interaction_distance_to_event\"] = interactions_df[\"interaction_distance_to_event\"].fillna(0).astype(float)\n",
    "    \n",
    "    # Ensure correct types\n",
    "    for df in [interactions_df, users_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    \n",
    "    for df in [interactions_df, events_df]:\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "\n",
    "    interactions_df[\"interaction_label\"] = interactions_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Convert to string type for TF-IDF fields\n",
    "    events_df[\"title\"] = events_df[\"title\"].fillna(\"\").astype(str)\n",
    "    users_df[\"user_interests\"] = users_df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "    users_df[\"age\"] = users_df[\"age\"].fillna(0).astype(float)\n",
    "\n",
    "    # Ensure all numeric fields are float - fixed to reference events_df instead of df\n",
    "    numeric_cols = [\"duration\", \"temperature\", \"attendance_rate\"]\n",
    "    for col in numeric_cols:\n",
    "        events_df[col] = pd.to_numeric(events_df[col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "    # Return all three dataframes\n",
    "    return interactions_df, users_df, events_df\n",
    "\n",
    "interactions_df, users_df, events_df = preprocess_common(interactions_df, users_df, events_df)\n",
    "len(interactions_df) + len(users_df) + len(events_df)\n",
    "merged_df = interactions_df.merge(events_df,on=\"event_id\")\\\n",
    "    .merge(users_df, on=\"user_id\")\n",
    "\n",
    "merged_df.head()\n",
    "                                  \n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"Unnamed: 0_x\", \"Unnamed: 0_y\", \"Unnamed: 0\"])\n",
    "merged_df.head(2)\n",
    "\n",
    "\n",
    "merged_df = merged_df.rename(columns={\"interaction_distance_to_event\": \"distance_to_event\"})\n",
    "merged_df.head(2)\n",
    "merged_df.to_csv(\"merged_interaction_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š LightFM with Metadata Results:\n",
      "AUC: 0.3800\n",
      "MAP: 0.3858\n",
      "Precision@5: 0.2000\n",
      "Recall@5: 0.0001\n",
      "NDCG@5: 0.1461\n",
      "Precision@10: 0.2000\n",
      "Recall@10: 0.0001\n",
      "NDCG@10: 0.1732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Second version\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset as LFMData\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, ndcg_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"merged_interaction_df.csv\")\n",
    "df[\"interaction_label\"] = df[\"interaction_label\"].astype(int)\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"interaction_label\"])\n",
    "\n",
    "# --------------------------\n",
    "# Build user/item metadata as feature dicts\n",
    "# --------------------------\n",
    "\n",
    "def extract_user_features(df):\n",
    "    tfidf = TfidfVectorizer(max_features=50)\n",
    "    tfidf_matrix = tfidf.fit_transform(df[\"user_interests\"].fillna(\"\"))\n",
    "    interest_features = tfidf.get_feature_names_out()\n",
    "    \n",
    "    user_features = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        feats = [f\"age:{int(row['age']//10)*10}\", f\"weather_pref:{row['user_weather_preference']}\", f\"social:{int(row['social_connectedness']//5)*5}\"]\n",
    "        tfidf_feats = tfidf_matrix[idx].toarray().flatten()\n",
    "        feats += [f\"interest:{interest_features[i]}\" for i in tfidf_feats.nonzero()[0]]\n",
    "        user_features[row[\"user_id\"]] = feats\n",
    "    return user_features\n",
    "\n",
    "def extract_item_features(df):\n",
    "    tfidf = TfidfVectorizer(max_features=50)\n",
    "    tfidf_matrix = tfidf.fit_transform(df[\"title\"].fillna(\"\"))\n",
    "    title_features = tfidf.get_feature_names_out()\n",
    "    \n",
    "    item_features = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        feats = [\n",
    "            f\"event_type:{row['event_type']}\",\n",
    "            f\"indoor:{row['event_indoor_capability']}\",\n",
    "            f\"temperature:{int(row['temperature']//5)*5}\",\n",
    "            f\"duration:{int(row['duration']//60)}h\",\n",
    "        ]\n",
    "        tfidf_feats = tfidf_matrix[idx].toarray().flatten()\n",
    "        feats += [f\"title:{title_features[i]}\" for i in tfidf_feats.nonzero()[0]]\n",
    "        item_features[row[\"event_id\"]] = feats\n",
    "    return item_features\n",
    "\n",
    "user_features_dict = extract_user_features(df)\n",
    "item_features_dict = extract_item_features(df)\n",
    "\n",
    "# --------------------------\n",
    "# Build LightFM Dataset\n",
    "# --------------------------\n",
    "dataset = LFMData()\n",
    "dataset.fit(\n",
    "    users=df[\"user_id\"].unique(),\n",
    "    items=df[\"event_id\"].unique(),\n",
    "    user_features={f for feats in user_features_dict.values() for f in feats},\n",
    "    item_features={f for feats in item_features_dict.values() for f in feats}\n",
    ")\n",
    "\n",
    "# Interactions\n",
    "interactions, _ = dataset.build_interactions([\n",
    "    (row[\"user_id\"], row[\"event_id\"], row[\"interaction_label\"]) for _, row in train_df.iterrows()\n",
    "])\n",
    "\n",
    "# Feature matrices\n",
    "user_features = dataset.build_user_features([(uid, feats) for uid, feats in user_features_dict.items()])\n",
    "item_features = dataset.build_item_features([(iid, feats) for iid, feats in item_features_dict.items()])\n",
    "\n",
    "# --------------------------\n",
    "# Train LightFM\n",
    "# --------------------------\n",
    "model = LightFM(loss='warp', random_state=42)\n",
    "model.fit(interactions, user_features=user_features, item_features=item_features, epochs=10, num_threads=4)\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate\n",
    "# --------------------------\n",
    "# Map IDs\n",
    "user_map, _, item_map, _ = dataset.mapping()\n",
    "test_df = test_df[test_df[\"user_id\"].isin(user_map) & test_df[\"event_id\"].isin(item_map)]\n",
    "test_df[\"user_idx\"] = test_df[\"user_id\"].map(user_map)\n",
    "test_df[\"item_idx\"] = test_df[\"event_id\"].map(item_map)\n",
    "\n",
    "# Predict\n",
    "user_ids = test_df[\"user_idx\"].values\n",
    "item_ids = test_df[\"item_idx\"].values\n",
    "\n",
    "test_df[\"lightfm_score\"] = model.predict(\n",
    "    user_ids, item_ids,\n",
    "    user_features=user_features,\n",
    "    item_features=item_features\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Ranking Evaluation\n",
    "# --------------------------\n",
    "def evaluate_all(y_true, y_score, k_values=[5, 10]):\n",
    "    results = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_score),\n",
    "        \"MAP\": average_precision_score(y_true, y_score)\n",
    "    }\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_score = np.array(y_score)\n",
    "    for k in k_values:\n",
    "        sorted_idx = np.argsort(y_score)[::-1][:k]\n",
    "        top_k_true = y_true[sorted_idx]\n",
    "        precision = np.mean(top_k_true)\n",
    "        recall = np.sum(top_k_true) / np.sum(y_true)\n",
    "        ndcg = ndcg_score([y_true], [y_score], k=k)\n",
    "        results[f\"Precision@{k}\"] = precision\n",
    "        results[f\"Recall@{k}\"] = recall\n",
    "        results[f\"NDCG@{k}\"] = ndcg\n",
    "    return results\n",
    "\n",
    "metrics = evaluate_all(test_df[\"interaction_label\"], test_df[\"lightfm_score\"])\n",
    "print(\"ðŸ“Š LightFM with Metadata Results:\")\n",
    "for metric, val in metrics.items():\n",
    "    print(f\"{metric}: {val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Content-Based model...\n",
      "Training SVD model...\n",
      "Training Hybrid (SVD + Content + Weather) model with strict validation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Content-Based</th>\n",
       "      <td>0.735287</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.910474</td>\n",
       "      <td>0.906715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybrid</th>\n",
       "      <td>0.591013</td>\n",
       "      <td>0.572386</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.921602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUC       MAP  Precision@5  Recall@5  NDCG@5  \\\n",
       "Content-Based  0.735287  0.712871          1.0  0.000083     1.0   \n",
       "SVD            0.910474  0.906715          1.0  0.000083     1.0   \n",
       "Hybrid         0.591013  0.572386          1.0  0.000329     1.0   \n",
       "\n",
       "               Precision@10  Recall@10   NDCG@10  \n",
       "Content-Based           1.0   0.000165  1.000000  \n",
       "SVD                     1.0   0.000165  1.000000  \n",
       "Hybrid                  0.9   0.000591  0.921602  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Enhanced benchmarking evaluation and training pipeline with complete metrics and weather features included\n",
    "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from surprise import Dataset, Reader, SVD\n",
    "# from lightfm import LightFM\n",
    "# from lightfm.data import Dataset as LFMData\n",
    "# from lightfm.evaluation import precision_at_k, auc_score\n",
    "# from scipy.sparse import hstack\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# # --- Shared Evaluation ---\n",
    "# def compute_ranking_metrics(y_true, y_score, k=10):\n",
    "#     # Sort indices by descending score\n",
    "#     sorted_indices = np.argsort(y_score)[::-1]\n",
    "#     # Get top-k true labels\n",
    "#     top_k = np.array(y_true)[sorted_indices][:k]\n",
    "#     # Calculate precision\n",
    "#     precision = np.mean(top_k)\n",
    "#     # Calculate recall\n",
    "#     recall = np.sum(top_k) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "#     # Calculate DCG\n",
    "#     dcg = np.sum(top_k / np.log2(np.arange(2, len(top_k) + 2)))\n",
    "#     # Calculate ideal DCG\n",
    "#     ideal_k = min(int(np.sum(y_true)), k)\n",
    "#     idcg = np.sum([1 / np.log2(i + 2) for i in range(ideal_k)])\n",
    "#     # Calculate NDCG\n",
    "#     ndcg = dcg / idcg if idcg > 0 else 0\n",
    "#     return precision, recall, ndcg\n",
    "\n",
    "# def compute_all_metrics(y_true, y_score):\n",
    "#     # Ensure there are enough positive examples for AUC calculation\n",
    "#     if len(np.unique(y_true)) < 2:\n",
    "#         metrics = {\n",
    "#             \"AUC\": np.nan,\n",
    "#             \"MAP\": np.nan\n",
    "#         }\n",
    "#     else:\n",
    "#         metrics = {\n",
    "#             \"AUC\": roc_auc_score(y_true, y_score),\n",
    "#             \"MAP\": average_precision_score(y_true, y_score)\n",
    "#         }\n",
    "    \n",
    "#     for k in [5, 10]:\n",
    "#         p, r, n = compute_ranking_metrics(y_true, y_score, k)\n",
    "#         metrics[f\"Precision@{k}\"] = p\n",
    "#         metrics[f\"Recall@{k}\"] = r\n",
    "#         metrics[f\"NDCG@{k}\"] = n\n",
    "#     return metrics\n",
    "\n",
    "# # --- Content-Based Model ---\n",
    "# def train_content_model(df):\n",
    "#     # Handle potential NaN values properly\n",
    "#     df_clean = df.copy()\n",
    "#     df_clean[\"title\"] = df_clean[\"title\"].fillna(\"\")\n",
    "#     df_clean[\"user_interests\"] = df_clean[\"user_interests\"].fillna(\"\")\n",
    "\n",
    "#     # TF-IDF features\n",
    "#     tfidf_title = TfidfVectorizer(max_features=100)\n",
    "#     tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "#     tfidf_title_mat = tfidf_title.fit_transform(df_clean[\"title\"])\n",
    "#     tfidf_interests_mat = tfidf_interests.fit_transform(df_clean[\"user_interests\"])\n",
    "\n",
    "#     # Numeric features\n",
    "#     numeric_cols = [\"distance_to_event\", \"duration\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "#     df_clean[numeric_cols] = df_clean[numeric_cols].fillna(0)\n",
    "#     X_numeric = StandardScaler().fit_transform(df_clean[numeric_cols])\n",
    "\n",
    "#     # Combine all features\n",
    "#     X = hstack([tfidf_title_mat, tfidf_interests_mat, X_numeric]).toarray()\n",
    "#     y = df_clean[\"interaction_label\"].astype(int)\n",
    "\n",
    "#     # Train CatBoost\n",
    "#     model = CatBoostClassifier(\n",
    "#         iterations=200,\n",
    "#         depth=6,\n",
    "#         learning_rate=0.1,\n",
    "#         loss_function='Logloss',\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     model.fit(X, y)\n",
    "#     scores = model.predict_proba(X)[:, 1]\n",
    "\n",
    "#     return model, compute_all_metrics(y, scores)\n",
    "\n",
    "# # --- SVD Model ---\n",
    "# def train_svd(df):\n",
    "#     # Prepare data for SVD\n",
    "#     reader = Reader(rating_scale=(0, 1))\n",
    "#     df_svd = df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    \n",
    "#     # Convert all to strings to ensure compatibility\n",
    "#     df_svd[\"user_id\"] = df_svd[\"user_id\"].astype(str)\n",
    "#     df_svd[\"event_id\"] = df_svd[\"event_id\"].astype(str)\n",
    "#     df_svd[\"interaction_label\"] = df_svd[\"interaction_label\"].astype(float)\n",
    "    \n",
    "#     data = Dataset.load_from_df(df_svd, reader)\n",
    "#     trainset = data.build_full_trainset()\n",
    "    \n",
    "#     # Train SVD model\n",
    "#     svd = SVD(n_epochs=20).fit(trainset)\n",
    "    \n",
    "#     # Generate predictions for all user-item pairs\n",
    "#     df_copy = df.copy()\n",
    "#     df_copy[\"svd_score\"] = df_copy.apply(\n",
    "#         lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, \n",
    "#         axis=1\n",
    "#     )\n",
    "    \n",
    "#     return svd, compute_all_metrics(df_copy[\"interaction_label\"].astype(int), df_copy[\"svd_score\"])\n",
    "\n",
    "\n",
    "# # --- Final Hybrid Training with Train/Test Separation ---\n",
    "# def hybrid_model(train_df, val_df):\n",
    "#     # Step 1: Train SVD model on train_df\n",
    "#     reader = Reader(rating_scale=(0, 1))\n",
    "#     train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "#     trainset = train_svd_data.build_full_trainset()\n",
    "#     svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "\n",
    "#     # Step 2: Compute SVD scores for both train and val\n",
    "#     train_df = train_df.copy()\n",
    "#     val_df = val_df.copy()\n",
    "#     train_df[\"svd_score\"] = train_df.apply(\n",
    "#         lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "#     )\n",
    "#     val_df[\"svd_score\"] = val_df.apply(\n",
    "#         lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "#     )\n",
    "\n",
    "#     # Step 3: TF-IDF + numeric features\n",
    "#     tfidf_title = TfidfVectorizer(max_features=50)\n",
    "#     tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "#     tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "#     tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "\n",
    "#     X_train_text = hstack([\n",
    "#         tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "#         tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "#     ])\n",
    "#     X_val_text = hstack([\n",
    "#         tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "#         tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "#     ])\n",
    "\n",
    "#     numeric_cols = [\"distance_to_event\", \"duration\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "#     train_df[numeric_cols] = train_df[numeric_cols].fillna(0)\n",
    "#     val_df[numeric_cols] = val_df[numeric_cols].fillna(0)\n",
    "\n",
    "#     scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "#     X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "#     X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "\n",
    "#     # Combine all features\n",
    "#     X_train = hstack([X_train_text, X_train_numeric]).toarray()\n",
    "#     X_val = hstack([X_val_text, X_val_numeric]).toarray()\n",
    "#     y_train = train_df[\"interaction_label\"].astype(int)\n",
    "#     y_val = val_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "#     model = CatBoostClassifier(\n",
    "#         iterations=200,\n",
    "#         depth=6,\n",
    "#         learning_rate=0.1,\n",
    "#         loss_function='Logloss',\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     scores = model.predict_proba(X_val)[:, 1]\n",
    "#     return model, compute_all_metrics(y_val, scores)\n",
    "\n",
    "# \"âœ… Hybrid model updated to use CatBoost for better non-linear modeling.\"\n",
    "\n",
    "# # Final all-model benchmark comparison with strict train/validation split and consistent evaluation\n",
    "# def compare_all_models_strict(train_df, val_df):\n",
    "#     train_df = train_df.copy()\n",
    "#     val_df = val_df.copy()\n",
    "\n",
    "#     # Ensure correct types and fill missing\n",
    "#     for df in [train_df, val_df]:\n",
    "#         df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "#         df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "#         df[\"interaction_label\"] = df[\"interaction_label\"].astype(int)\n",
    "#         df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "#         df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "\n",
    "#     print(\"Training Content-Based model...\")\n",
    "#     _, content_scores = train_content_model(train_df)\n",
    "\n",
    "#     print(\"Training SVD model...\")\n",
    "#     _, svd_scores = train_svd(train_df)\n",
    "\n",
    "#     print(\"Training Hybrid (SVD + Content + Weather) model with strict validation...\")\n",
    "#     _, hybrid_scores = hybrid_model(train_df, val_df)\n",
    "\n",
    "  \n",
    "\n",
    "#     # Assemble final results\n",
    "#     results_df = pd.DataFrame({\n",
    "#         \"Content-Based\": content_scores,\n",
    "#         \"SVD\": svd_scores,\n",
    "#         \"Hybrid\": hybrid_scores,\n",
    "\n",
    "#     })\n",
    "\n",
    "#     return results_df.T\n",
    "\n",
    "# \"âœ… All-model benchmark comparison function finalized with strict train/val split and metric evaluation.\"\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# benchmark_results = compare_all_models_strict(train_df, val_df)\n",
    "# display(benchmark_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_content_model(train_df, val_df):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    # Clean and prepare training data\n",
    "    train_clean = train_df.copy()\n",
    "    train_clean[\"title\"] = train_clean[\"title\"].fillna(\"\")\n",
    "    train_clean[\"user_interests\"] = train_clean[\"user_interests\"].fillna(\"\")\n",
    "    \n",
    "    # Clean and prepare validation data\n",
    "    val_clean = val_df.copy()\n",
    "    val_clean[\"title\"] = val_clean[\"title\"].fillna(\"\")\n",
    "    val_clean[\"user_interests\"] = val_clean[\"user_interests\"].fillna(\"\")\n",
    "\n",
    "    # TF-IDF features - fit only on training data\n",
    "    tfidf_title = TfidfVectorizer(max_features=100)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=100)\n",
    "    tfidf_title_mat_train = tfidf_title.fit_transform(train_clean[\"title\"])\n",
    "    tfidf_interests_mat_train = tfidf_interests.fit_transform(train_clean[\"user_interests\"])\n",
    "    \n",
    "    # Transform validation data using fitted vectorizers\n",
    "    tfidf_title_mat_val = tfidf_title.transform(val_clean[\"title\"])\n",
    "    tfidf_interests_mat_val = tfidf_interests.transform(val_clean[\"user_interests\"])\n",
    "\n",
    "    # Numeric features\n",
    "    numeric_cols = [\"distance_to_event\", \"duration\", \"temperature\", \"age\", \"attendance_rate\"]\n",
    "    train_clean[numeric_cols] = train_clean[numeric_cols].fillna(0)\n",
    "    val_clean[numeric_cols] = val_clean[numeric_cols].fillna(0)\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    scaler = StandardScaler().fit(train_clean[numeric_cols])\n",
    "    X_numeric_train = scaler.transform(train_clean[numeric_cols])\n",
    "    X_numeric_val = scaler.transform(val_clean[numeric_cols])\n",
    "    \n",
    "    # Categorical features with OneHotEncoder\n",
    "    categorical_cols = ['events_weather_condition', 'events_event_indoor_capability', 'users_user_weather_preference']\n",
    "    \n",
    "    # Fill missing values in categorical columns\n",
    "    for col in categorical_cols:\n",
    "        train_clean[col] = train_clean[col].fillna('unknown')\n",
    "        val_clean[col] = val_clean[col].fillna('unknown')\n",
    "    \n",
    "    # Create OneHotEncoder for categorical features\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_clean[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_clean[categorical_cols])\n",
    "\n",
    "    # Combine all features\n",
    "    X_train = hstack([tfidf_title_mat_train, tfidf_interests_mat_train, X_numeric_train, X_cat_train]).toarray()\n",
    "    X_val = hstack([tfidf_title_mat_val, tfidf_interests_mat_val, X_numeric_val, X_cat_val]).toarray()\n",
    "    \n",
    "    y_train = train_clean[\"interaction_label\"].astype(int)\n",
    "    y_val = val_clean[\"interaction_label\"].astype(int)\n",
    "\n",
    "    # Train CatBoost on training data only\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    val_scores = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    return model, compute_all_metrics(y_val, val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svd(train_df, val_df):\n",
    "    # Prepare training data for SVD\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd = train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].copy()\n",
    "    \n",
    "    # Convert all to strings to ensure compatibility\n",
    "    train_svd[\"user_id\"] = train_svd[\"user_id\"].astype(str)\n",
    "    train_svd[\"event_id\"] = train_svd[\"event_id\"].astype(str)\n",
    "    train_svd[\"interaction_label\"] = train_svd[\"interaction_label\"].astype(float)\n",
    "    \n",
    "    data = Dataset.load_from_df(train_svd, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    # Train SVD model on training data only\n",
    "    svd = SVD(n_epochs=50).fit(trainset)\n",
    "    \n",
    "    # Generate predictions for validation data\n",
    "    val_copy = val_df.copy()\n",
    "    val_copy[\"svd_score\"] = val_copy.apply(\n",
    "        lambda row: svd.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return svd, compute_all_metrics(val_copy[\"interaction_label\"].astype(int), val_copy[\"svd_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'âœ… Hybrid model updated to use CatBoost for better non-linear modeling.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hybrid_model(train_df, val_df):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    # Step 1: Train SVD model on train_df\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    train_svd_data = Dataset.load_from_df(train_df[[\"user_id\", \"event_id\", \"interaction_label\"]].astype(str), reader)\n",
    "    trainset = train_svd_data.build_full_trainset()\n",
    "    svd_model = SVD(n_epochs=20).fit(trainset)\n",
    "\n",
    "    # Step 2: Compute SVD scores for both train and val\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    train_df[\"svd_score\"] = train_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "    val_df[\"svd_score\"] = val_df.apply(\n",
    "        lambda row: svd_model.predict(str(row[\"user_id\"]), str(row[\"event_id\"])).est, axis=1\n",
    "    )\n",
    "\n",
    "    # Step 3: TF-IDF + numeric features\n",
    "    tfidf_title = TfidfVectorizer(max_features=50)\n",
    "    tfidf_interests = TfidfVectorizer(max_features=50)\n",
    "    tfidf_title.fit(train_df[\"title\"].fillna(\"\"))\n",
    "    tfidf_interests.fit(train_df[\"user_interests\"].fillna(\"\"))\n",
    "\n",
    "    X_train_text = hstack([\n",
    "        tfidf_title.transform(train_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(train_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "    X_val_text = hstack([\n",
    "        tfidf_title.transform(val_df[\"title\"].fillna(\"\")),\n",
    "        tfidf_interests.transform(val_df[\"user_interests\"].fillna(\"\"))\n",
    "    ])\n",
    "\n",
    "    # Numeric features including SVD score\n",
    "    numeric_cols = [\"distance_to_event\", \"duration\", \"temperature\", \"age\", \"attendance_rate\", \"svd_score\"]\n",
    "    train_df[numeric_cols] = train_df[numeric_cols].fillna(0)\n",
    "    val_df[numeric_cols] = val_df[numeric_cols].fillna(0)\n",
    "\n",
    "    scaler = StandardScaler().fit(train_df[numeric_cols])\n",
    "    X_train_numeric = scaler.transform(train_df[numeric_cols])\n",
    "    X_val_numeric = scaler.transform(val_df[numeric_cols])\n",
    "    \n",
    "    # Categorical features with OneHotEncoder\n",
    "    categorical_cols = ['events_weather_condition', 'events_event_indoor_capability', 'users_user_weather_preference']\n",
    "    \n",
    "    # Fill missing values in categorical columns\n",
    "    for col in categorical_cols:\n",
    "        train_df[col] = train_df[col].fillna('unknown')\n",
    "        val_df[col] = val_df[col].fillna('unknown')\n",
    "    \n",
    "    # Create OneHotEncoder for categorical features\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = encoder.fit_transform(train_df[categorical_cols])\n",
    "    X_cat_val = encoder.transform(val_df[categorical_cols])\n",
    "\n",
    "    # Combine all features\n",
    "    X_train = hstack([X_train_text, X_train_numeric, X_cat_train]).toarray()\n",
    "    X_val = hstack([X_val_text, X_val_numeric, X_cat_val]).toarray()\n",
    "    \n",
    "    y_train = train_df[\"interaction_label\"].astype(int)\n",
    "    y_val = val_df[\"interaction_label\"].astype(int)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    scores = model.predict_proba(X_val)[:, 1]\n",
    "    return model, compute_all_metrics(y_val, scores)\n",
    "'âœ… Hybrid model updated to use CatBoost for better non-linear modeling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models_strict(train_df, val_df):\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "\n",
    "    # Ensure correct types and fill missing\n",
    "    for df in [train_df, val_df]:\n",
    "        df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "        df[\"event_id\"] = df[\"event_id\"].astype(str)\n",
    "        df[\"interaction_label\"] = df[\"interaction_label\"].astype(int)\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "        df[\"user_interests\"] = df[\"user_interests\"].fillna(\"\").astype(str)\n",
    "        \n",
    "        # Ensure categorical columns exist\n",
    "        for col in ['events_weather_condition', 'events_event_indoor_capability', 'users_user_weather_preference']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"unknown\"\n",
    "\n",
    "    print(\"Training Content-Based model...\")\n",
    "    _, content_scores = train_content_model(train_df, val_df)\n",
    "\n",
    "    print(\"Training SVD model...\")\n",
    "    _, svd_scores = train_svd(train_df, val_df)\n",
    "\n",
    "    print(\"Training Hybrid (SVD + Content + Weather) model with strict validation...\")\n",
    "    _, hybrid_scores = hybrid_model(train_df, val_df)\n",
    "\n",
    "    # Assemble final results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Content-Based\": content_scores,\n",
    "        \"SVD\": svd_scores,\n",
    "        \"Hybrid\": hybrid_scores,\n",
    "    })\n",
    "\n",
    "    return results_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Content-Based model...\n",
      "Training SVD model...\n",
      "Training Hybrid (SVD + Content + Weather) model with strict validation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>MAP</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Content-Based</th>\n",
       "      <td>0.703276</td>\n",
       "      <td>0.670024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.933746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.622998</td>\n",
       "      <td>0.576177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.870125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybrid</th>\n",
       "      <td>0.592051</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.654809</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.709740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUC       MAP  Precision@5  Recall@5    NDCG@5  \\\n",
       "Content-Based  0.703276  0.670024          1.0  0.000329  1.000000   \n",
       "SVD            0.622998  0.576177          1.0  0.000329  1.000000   \n",
       "Hybrid         0.592051  0.570906          0.6  0.000197  0.654809   \n",
       "\n",
       "               Precision@10  Recall@10   NDCG@10  \n",
       "Content-Based           0.9   0.000591  0.933746  \n",
       "SVD                     0.8   0.000526  0.870125  \n",
       "Hybrid                  0.7   0.000460  0.709740  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_all_models_strict(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather Model Scores:\n",
    "AUC: 0.7011\n",
    "Average Precision (MAP): 0.6712\n",
    "Precision@5: 0.8000\n",
    "Recall@5: 0.0005\n",
    "NDCG@5: 0.9344\n",
    "Precision@10: 0.8000\n",
    "Recall@10: 0.0011\n",
    "NDCG@10: 0.8546\n",
    "\n",
    "No-Weather Model Scores:\n",
    "AUC: 0.7064\n",
    "Average Precision (MAP): 0.6755\n",
    "Precision@5: 0.8000\n",
    "Recall@5: 0.0005\n",
    "NDCG@5: 0.6608\n",
    "Precision@10: 0.9000\n",
    "Recall@10: 0.0012\n",
    "NDCG@10: 0.7799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_lightfm_data(df):\n",
    "    # Create TF-IDF models\n",
    "    user_tfidf = TfidfVectorizer(max_features=100)\n",
    "    item_tfidf = TfidfVectorizer(max_features=100)\n",
    "    \n",
    "    user_tfidf.fit(df['users_user_interests'].fillna(''))\n",
    "    item_tfidf.fit((df['events_title'] + ' ' + df['events_event_type']).fillna(''))\n",
    "    \n",
    "    dataset = Dataset()\n",
    "    dataset.fit(\n",
    "        users=df['user_id'].unique(),\n",
    "        items=df['event_id'].unique(),\n",
    "        user_features=user_tfidf.get_feature_names_out().tolist(),\n",
    "        item_features=item_tfidf.get_feature_names_out().tolist()\n",
    "    )\n",
    "    \n",
    "    return dataset, user_tfidf, item_tfidf\n",
    "\n",
    "def train_lightfm_model(df):\n",
    "    dataset, user_tfidf, item_tfidf = prepare_lightfm_data(df)\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build user content tuples\n",
    "    user_features_matrix = user_tfidf.transform(train['users_user_interests'].fillna(''))\n",
    "    user_features_data = [\n",
    "        (uid, user_tfidf.get_feature_names_out()[user_features_matrix[i].nonzero()[1]].tolist())\n",
    "        for i, uid in enumerate(train['user_id'])\n",
    "    ]\n",
    "\n",
    "    # Build item content tuples\n",
    "    item_texts = (train['events_title'] + ' ' + train['events_event_type']).fillna('')\n",
    "    item_features_matrix = item_tfidf.transform(item_texts)\n",
    "    item_features_data = [\n",
    "        (eid, item_tfidf.get_feature_names_out()[item_features_matrix[i].nonzero()[1]].tolist())\n",
    "        for i, eid in enumerate(train['event_id'])\n",
    "    ]\n",
    "\n",
    "    # Build features\n",
    "    user_features = dataset.build_user_features(user_features_data)\n",
    "    item_features = dataset.build_item_features(item_features_data)\n",
    "\n",
    "    # Build interactions\n",
    "    interactions, _ = dataset.build_interactions([\n",
    "        (row['user_id'], row['event_id'])\n",
    "        for _, row in train.iterrows()\n",
    "    ])\n",
    "\n",
    "    # Train model\n",
    "    model = LightFM(loss='warp')\n",
    "    model.fit(\n",
    "        interactions,\n",
    "        user_features=user_features,\n",
    "        item_features=item_features,\n",
    "        epochs=20\n",
    "    )\n",
    "\n",
    "    # Filter test for known users/items\n",
    "    known_users = dict(dataset.mapping()[0])\n",
    "    known_items = dict(dataset.mapping()[2])\n",
    "    test_filtered = test[\n",
    "        test['user_id'].isin(known_users) & test['event_id'].isin(known_items)\n",
    "    ]\n",
    "\n",
    "    if not test_filtered.empty:\n",
    "        user_ids = test_filtered['user_id'].map(known_users).values\n",
    "        item_ids = test_filtered['event_id'].map(known_items).values\n",
    "\n",
    "        scores = model.predict(\n",
    "            user_ids,\n",
    "            item_ids,\n",
    "            user_features=user_features,\n",
    "            item_features=item_features\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'model': model,\n",
    "            'metrics': compute_all_metrics(test_filtered['interaction_label'].values, scores)\n",
    "        }\n",
    "\n",
    "    print(\"âš ï¸ No valid test samples after filtering.\")\n",
    "    return {'model': model, 'metrics': {}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_lightfm_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': 0.3519315613644091,\n",
       " 'MAP': 0.36895397602797764,\n",
       " 'Precision@5': 0.0,\n",
       " 'Recall@5': 0.0,\n",
       " 'NDCG@5': 0.0,\n",
       " 'Precision@10': 0.1,\n",
       " 'Recall@10': 6.676904587033452e-05,\n",
       " 'NDCG@10': 0.06362078819895173}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "scores[\"metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
