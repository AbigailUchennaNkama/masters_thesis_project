{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf960c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:03:29,351 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-04-20 12:03:29,718 INFO: Initializing external client\n",
      "2025-04-20 12:03:29,724 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-04-20 12:03:30,641 WARNING: UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:03:31,553 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import joblib\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, ndcg_score\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import joblib\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, ndcg_score\n",
    "\n",
    "# Login to Hopsworks and get project\n",
    "project = hopsworks.login()\n",
    "mr = project.get_model_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0ecb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs = project.get_feature_store()\n",
    "# Load from Hopsworks feature groups\n",
    "users_fg = fs.get_feature_group(name=\"users\", version=1)\n",
    "events_fg = fs.get_feature_group(name=\"events\", version=1)\n",
    "interactions_fg = fs.get_feature_group(name=\"interactions\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d45de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.79s) \n"
     ]
    }
   ],
   "source": [
    "users_df = users_fg.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcd7ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>interaction_time</th>\n",
       "      <th>interaction_distance_to_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28f8230c-a22c-4827-b808-5bccd8207cb1</td>\n",
       "      <td>IY531R</td>\n",
       "      <td>WH966B</td>\n",
       "      <td>no</td>\n",
       "      <td>2023-04-17 01:38:57.844458+00:00</td>\n",
       "      <td>5736.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4140ef03-afce-4fe5-a9a9-8e43fd45dfaa</td>\n",
       "      <td>LT819S</td>\n",
       "      <td>OI841N</td>\n",
       "      <td>invited</td>\n",
       "      <td>2024-05-28 12:43:38.312350+00:00</td>\n",
       "      <td>7630.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BI623H</td>\n",
       "      <td>UP287J</td>\n",
       "      <td>IO571I</td>\n",
       "      <td>yes</td>\n",
       "      <td>2025-08-21 22:51:24.898618+00:00</td>\n",
       "      <td>14.52132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08e342ae-f9b5-4358-9f41-cf89671b9ae2</td>\n",
       "      <td>RG723K</td>\n",
       "      <td>YW813D</td>\n",
       "      <td>invited &amp; no</td>\n",
       "      <td>2023-05-22 11:28:30.845737+00:00</td>\n",
       "      <td>7007.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7cfbb86b-6d7a-4f1a-b05f-ffa78bc41ffb</td>\n",
       "      <td>DM205I</td>\n",
       "      <td>RM794Y</td>\n",
       "      <td>invited &amp; no</td>\n",
       "      <td>2023-10-31 00:39:12.438860+00:00</td>\n",
       "      <td>6019.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         interaction_id user_id event_id interaction_type  \\\n",
       "0  28f8230c-a22c-4827-b808-5bccd8207cb1  IY531R   WH966B               no   \n",
       "1  4140ef03-afce-4fe5-a9a9-8e43fd45dfaa  LT819S   OI841N          invited   \n",
       "2                                BI623H  UP287J   IO571I              yes   \n",
       "3  08e342ae-f9b5-4358-9f41-cf89671b9ae2  RG723K   YW813D     invited & no   \n",
       "4  7cfbb86b-6d7a-4f1a-b05f-ffa78bc41ffb  DM205I   RM794Y     invited & no   \n",
       "\n",
       "                  interaction_time  interaction_distance_to_event  \n",
       "0 2023-04-17 01:38:57.844458+00:00                     5736.00000  \n",
       "1 2024-05-28 12:43:38.312350+00:00                     7630.00000  \n",
       "2 2025-08-21 22:51:24.898618+00:00                       14.52132  \n",
       "3 2023-05-22 11:28:30.845737+00:00                     7007.00000  \n",
       "4 2023-10-31 00:39:12.438860+00:00                     6019.00000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362a12",
   "metadata": {},
   "source": [
    "Users_data:\n",
    "\n",
    "user_id\tuser_lat\tuser_lon\tuser_city\tuser_weather_preference\tage\tuser_interests\tsignup_date\tsocial_connectedness\n",
    "0\tJI904B\t36.122607\t138.598711\tTokyo\toutdoor\t27\tmusic\t2023-07-24 10:35:22.959064+00:00\t11\n",
    "1\tYF384K\t-33.897451\t151.157340\tSydney\tany\t26\tmusic cinema sports\t2023-04-24 20:46:40.735141+00:00\t17\n",
    "2\tRJ912O\t40.593968\t-72.327571\tNew York\tindoor\t24\ttravel\t2023-10-13 11:37:21.333403+00:00\t16\n",
    "\n",
    "events_data:\n",
    "event_id\ttitle\tevent_type\tevent_lat\tevent_lon\tevent_city\tstart_time\tduration\tweather_condition\ttemperature\tattendance_rate\tevent_indoor_capability\n",
    "0\tCA178V\tseamless leadingedge timeframe arts culture in...\tArts & Culture\t44.019579\t-80.891209\tToronto\t2025-06-14 10:23:15.662617+00:00\t180\tClear\t12.3\t5.226955\tTrue\n",
    "1\tTF630U\tvisionary maximized definition community cause...\tCommunity & Causes\t40.812661\t-74.051607\tNew York\t2025-04-13 14:02:14.486362+00:00\t360\tClear\t18.7\t8.091905\tFalse\n",
    "2\tDY324Y\tupsized local function business networking in ...\tBusiness & Networking\t40.747403\t-73.953319\tNew York\t2025-05-17 14:11:30.484025+00:00\t240\tCloudy\t16.9\t31.945106\tTrue\n",
    "\n",
    "interactions_data:\n",
    "interaction_id\tuser_id\tevent_id\tinteraction_type\tinteraction_time\tinteraction_distance_to_event\n",
    "0\t28f8230c-a22c-4827-b808-5bccd8207cb1\tIY531R\tWH966B\tno\t2023-04-17 01:38:57.844458+00:00\t5736.00000\n",
    "1\t4140ef03-afce-4fe5-a9a9-8e43fd45dfaa\tLT819S\tOI841N\tinvited\t2024-05-28 12:43:38.312350+00:00\t7630.00000\n",
    "2\tBI623H\tUP287J\tIO571I\tyes\t2025-08-21 22:51:24.898618+00:00\t14.52132\n",
    "3\t08e342ae-f9b5-4358-9f41-cf89671b9ae2\tRG723K\tYW813D\tinvited & no\t2023-05-22 11:28:30.845737+00:00\t7007.00000\n",
    "4\t7cfbb86b-6d7a-4f1a-b05f-ffa78bc41ffb\tDM205I\tRM794Y\tinvited & no\t2023-10-31 00:39:12.438860+00:00\t6019.00000\n",
    "\n",
    "# Define features for query and candidate towers (retrieval model only)\n",
    "QUERY_FEATURES = [\"user_id\", \"user_city\", \"age\", \"user_interests\"]\n",
    "CANDIDATE_FEATURES = [\"event_id\", \"event_type\", \"event_city\"]\n",
    "\n",
    "\n",
    "# Create Ranking Feature group\n",
    "events_interactions_df = pd.merge(\n",
    "    interactions_df, \n",
    "    events_df, \n",
    "    on='event_id', \n",
    "    how='inner',\n",
    "    suffixes=('', '_event')  # Add suffix for event columns\n",
    ")\n",
    "\n",
    "ranking_df = pd.merge(\n",
    "    events_interactions_df, \n",
    "    users_df, \n",
    "    on='user_id', \n",
    "    how='inner',\n",
    "    suffixes=('', '_user')  # Add suffix for user columns\n",
    ")\n",
    "\n",
    "ranking_df['label'] = ranking_df['interaction_type'].apply(\n",
    "    lambda x: 1 if x in ['maybe', 'invited & maybe', 'yes', 'invited & yes'] else 0\n",
    ")\n",
    "\n",
    "ranking_df_with_weather = ranking_df.drop(\n",
    "    ['interaction_id', 'interaction_type','interaction_time',\\\n",
    "     'start_time', 'signup_date','social_connectedness'], \n",
    "    axis=1, \n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "ranking_fg_weather = fs.get_or_create_feature_group(\n",
    "    name=\"ranking-with-weather\",\n",
    "    description=\"Ranking Data with weather conditions.\",\n",
    "    version=1,\n",
    "    primary_key=[\"user_id\", \"video_id\"],\n",
    "    online_enabled=True,\n",
    ")\n",
    "\n",
    "ranking_fg_weather.insert(ranking_df_with_weather)\n",
    "print('Done ✅')\n",
    "\n",
    "# create ranking data without weather information\n",
    "ranking_df_without_weather = ranking_df.drop(['interaction_id', 'interaction_type',\n",
    "       'interaction_time', 'start_time','weather_condition',\n",
    "       'temperature', 'weather_preference',\n",
    "       'signup_date', 'social_connectedness'])\n",
    "\n",
    "ranking_fg_without_weather = fs.get_or_create_feature_group(\n",
    "    name=\"ranking-without-weather\",\n",
    "    description=\"Ranking Data without weather conditions.\",\n",
    "    version=1,\n",
    "    primary_key=[\"user_id\", \"video_id\"],\n",
    "    online_enabled=True,\n",
    ")\n",
    "\n",
    "ranking_fg_without_weather.insert(ranking_df_without_weather)\n",
    "print('Done ✅')\n",
    "\n",
    "print(\"Feature backfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7ee8cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.87s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (3.19s) \n"
     ]
    }
   ],
   "source": [
    "events_df = events_fg.read()\n",
    "interactions_df = interactions_fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "781c4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "query_model = mr.get_model(\"query_model\", version=1).download()\n",
    "candidate_model = mr.get_model(\"candidate_model\", version=1).download()\n",
    "weather_ranking_model = mr.get_model(\"weather_ranking_model\", version=1).download()\n",
    "no_weather_ranking_model = mr.get_model(\"no_weather_ranking_model\", version=1).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "270f73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each user, generate top-K candidate events using the retrieval model\n",
    "def generate_top_k_candidates(user_df, k=100):\n",
    "    user_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"user_id\": user_df[\"user_id\"].values,\n",
    "        \"user_city\": user_df[\"user_city\"].values,\n",
    "        \"age\": user_df[\"age\"].values,\n",
    "        \"user_index\": user_df.index.values\n",
    "    }).batch(128)\n",
    "    \n",
    "    user_embs = query_model(user_ds)\n",
    "    event_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"event_id\": events_df[\"event_id\"].values,\n",
    "        \"event_city\": events_df[\"event_city\"].values,\n",
    "        \"event_type\": events_df[\"event_type\"].values,\n",
    "        \"event_index\": events_df.index.values\n",
    "    }).batch(128)\n",
    "    \n",
    "    event_embs = candidate_model(event_ds)\n",
    "    \n",
    "    scores = tf.linalg.matmul(user_embs, event_embs, transpose_b=True)\n",
    "    top_k_idx = tf.math.top_k(scores, k=k).indices.numpy()\n",
    "    \n",
    "    candidates = []\n",
    "    for u_idx, event_indices in enumerate(top_k_idx):\n",
    "        user_id = user_df.iloc[u_idx][\"user_id\"]\n",
    "        for idx in event_indices:\n",
    "            event_id = events_df.iloc[idx][\"event_id\"]\n",
    "            candidates.append((user_id, event_id))\n",
    "    \n",
    "    return pd.DataFrame(candidates, columns=[\"user_id\", \"event_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa674c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label = 1 if (user, event) in interactions\n",
    "def create_ranking_dataset(candidates_df):\n",
    "    interactions_set = set(zip(interactions_df[\"user_id\"], interactions_df[\"event_id\"]))\n",
    "    candidates_df[\"label\"] = candidates_df.apply(\n",
    "        lambda row: 1 if (row[\"user_id\"], row[\"event_id\"]) in interactions_set else 0, axis=1)\n",
    "    \n",
    "    df = candidates_df.merge(users_df, on=\"user_id\").merge(events_df, on=\"event_id\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "852e18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_weather_ranking_model(ranking_df):\n",
    "    y_true = ranking_df[\"label\"].astype(int).values\n",
    "    X = ranking_df.drop(columns=[\"label\", \"user_id\", \"event_id\"])\n",
    "    y_score = weather_ranking_model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_score),\n",
    "        \"MAP\": average_precision_score(y_true, y_score)\n",
    "    }\n",
    "    \n",
    "    for k in [5, 10, 20]:\n",
    "        sorted_idx = np.argsort(-y_score)\n",
    "        relevance = y_true[sorted_idx]\n",
    "        top_k = relevance[:k]\n",
    "        metrics[f\"Precision@{k}\"] = top_k.mean()\n",
    "        metrics[f\"Recall@{k}\"] = top_k.sum() / max(np.sum(relevance), 1)\n",
    "        metrics[f\"NDCG@{k}\"] = ndcg_score([relevance], [y_score], k=k)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a6a917",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m top_k_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_top_k_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43musers_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ranking_df \u001b[38;5;241m=\u001b[39m create_ranking_dataset(top_k_candidates)\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_weather_ranking_model(ranking_df)\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mgenerate_top_k_candidates\u001b[0;34m(user_df, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_top_k_candidates\u001b[39m(user_df, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      3\u001b[0m     user_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices({\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_city\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_city\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      8\u001b[0m     })\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     user_embs \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     event_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices({\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: events_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_city\u001b[39m\u001b[38;5;124m\"\u001b[39m: events_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_city\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: events_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: events_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     16\u001b[0m     })\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     18\u001b[0m     event_embs \u001b[38;5;241m=\u001b[39m candidate_model(event_ds)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run pipeline\n",
    "top_k_candidates = generate_top_k_candidates(users_df)\n",
    "ranking_df = create_ranking_dataset(top_k_candidates)\n",
    "results = evaluate_weather_ranking_model(ranking_df)\n",
    "\n",
    "for metric, score in results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa92870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import joblib\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import hopsworks\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "# Initialize Hopsworks connection\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Load feature groups\n",
    "users_fg = fs.get_feature_group(name=\"users\", version=1)\n",
    "events_fg = fs.get_feature_group(name=\"events\", version=1)\n",
    "interactions_fg = fs.get_feature_group(name=\"interactions\", version=1)\n",
    "\n",
    "# Read data with specific columns\n",
    "users_df = users_fg.read(online=False)[['user_id', 'city', 'age', 'interests']]\n",
    "events_df = events_fg.read(online=False)[['event_id', 'type', 'city', 'datetime', 'latitude', 'longitude']]\n",
    "interactions_df = interactions_fg.read(online=False)[['user_id', 'event_id', 'timestamp']]\n",
    "\n",
    "# Load models\n",
    "query_model = mr.get_model(\"query_model\", version=1).download()\n",
    "candidate_model = mr.get_model(\"candidate_model\", version=1).download()\n",
    "weather_ranking_model = mr.get_model(\"weather_ranking_model\", version=1).download()\n",
    "no_weather_ranking_model = mr.get_model(\"no_weather_ranking_model\", version=1).download()\n",
    "\n",
    "# Feature preprocessing\n",
    "event_type_encoder = LabelEncoder().fit(events_df['type'])\n",
    "user_city_encoder = LabelEncoder().fit(users_df['city'])\n",
    "event_city_encoder = LabelEncoder().fit(events_df['city'])\n",
    "\n",
    "def calculate_distance(row):\n",
    "    user_coords = (row['user_lat'], row['user_lon'])\n",
    "    event_coords = (row['event_lat'], row['event_lon'])\n",
    "    return great_circle(user_coords, event_coords).km\n",
    "\n",
    "def generate_top_k_candidates(user_df, k=100):\n",
    "    # Create TensorFlow datasets\n",
    "    user_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"user_id\": user_df[\"user_id\"].values,\n",
    "        \"city\": user_city_encoder.transform(user_df[\"city\"]),\n",
    "        \"age\": user_df[\"age\"].values.astype(np.float32)\n",
    "    }).batch(128)\n",
    "    \n",
    "    event_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"event_id\": events_df[\"event_id\"].values,\n",
    "        \"type\": event_type_encoder.transform(events_df[\"type\"]),\n",
    "        \"city\": event_city_encoder.transform(events_df[\"city\"])\n",
    "    }).batch(128)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    user_embs = query_model(user_ds)\n",
    "    event_embs = candidate_model(event_ds)\n",
    "    \n",
    "    # Calculate scores and get top K\n",
    "    scores = tf.linalg.matmul(user_embs, event_embs, transpose_b=True)\n",
    "    top_k_indices = tf.math.top_k(scores, k=k).indices.numpy()\n",
    "    \n",
    "    # Create candidate DataFrame\n",
    "    candidates = []\n",
    "    for user_idx, event_indices in enumerate(top_k_indices):\n",
    "        user_id = user_df.iloc[user_idx][\"user_id\"]\n",
    "        for event_idx in event_indices:\n",
    "            event_id = events_df.iloc[event_idx][\"event_id\"]\n",
    "            candidates.append((user_id, event_id))\n",
    "    \n",
    "    return pd.DataFrame(candidates, columns=[\"user_id\", \"event_id\"])\n",
    "\n",
    "def create_ranking_dataset(candidates_df):\n",
    "    # Create labels\n",
    "    interactions_set = set(zip(interactions_df[\"user_id\"], interactions_df[\"event_id\"]))\n",
    "    candidates_df[\"label\"] = candidates_df.apply(\n",
    "        lambda row: 1 if (row[\"user_id\"], row[\"event_id\"]) in interactions_set else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Merge with user and event features\n",
    "    df = candidates_df.merge(\n",
    "        users_df.rename(columns={'city': 'user_city', 'latitude': 'user_lat', 'longitude': 'user_lon'}),\n",
    "        on=\"user_id\"\n",
    "    ).merge(\n",
    "        events_df.rename(columns={'city': 'event_city', 'type': 'event_type', \n",
    "                                'latitude': 'event_lat', 'longitude': 'event_lon'}),\n",
    "        on=\"event_id\"\n",
    "    )\n",
    "    \n",
    "    # Calculate interaction distance\n",
    "    df['interaction_distance_to_event'] = df.apply(calculate_distance, axis=1)\n",
    "    \n",
    "    # Convert categorical features\n",
    "    df['event_type'] = event_type_encoder.transform(df['event_type'])\n",
    "    df['user_city'] = user_city_encoder.transform(df['user_city'])\n",
    "    df['event_city'] = event_city_encoder.transform(df['event_city'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate_ranking_model(model, ranking_df):\n",
    "    # Prepare features\n",
    "    features = ranking_df[['interaction_distance_to_event', 'event_type', \n",
    "                          'user_city', 'event_city', 'age']]\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_score = model.predict_proba(features)[:, 1]\n",
    "    y_true = ranking_df[\"label\"].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_score),\n",
    "        \"MAP\": average_precision_score(y_true, y_score)\n",
    "    }\n",
    "    \n",
    "    # Calculate top-K metrics\n",
    "    for k in [5, 10, 20]:\n",
    "        top_k_mask = np.argsort(-y_score)[:k]\n",
    "        top_k_labels = y_true[top_k_mask]\n",
    "        \n",
    "        metrics[f\"Precision@{k}\"] = top_k_labels.mean()\n",
    "        metrics[f\"Recall@{k}\"] = top_k_labels.sum() / max(y_true.sum(), 1)\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        ideal_sorted = np.sort(y_true)[::-1]\n",
    "        dcg = sum((2**rel - 1) / np.log2(i+2) for i, rel in enumerate(top_k_labels))\n",
    "        idcg = sum((2**rel - 1) / np.log2(i+2) for i, rel in enumerate(ideal_sorted[:k]))\n",
    "        metrics[f\"NDCG@{k}\"] = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Execute pipeline\n",
    "print(\"Generating candidates...\")\n",
    "top_k_candidates = generate_top_k_candidates(users_df)\n",
    "\n",
    "print(\"Creating ranking dataset...\")\n",
    "ranking_df = create_ranking_dataset(top_k_candidates)\n",
    "\n",
    "print(\"Evaluating weather ranking model...\")\n",
    "weather_results = evaluate_ranking_model(weather_ranking_model, ranking_df)\n",
    "\n",
    "print(\"\\nWeather Model Metrics:\")\n",
    "for metric, score in weather_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating no-weather ranking model...\")\n",
    "no_weather_results = evaluate_ranking_model(no_weather_ranking_model, ranking_df)\n",
    "\n",
    "print(\"\\nNo-Weather Model Metrics:\")\n",
    "for metric, score in no_weather_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776683fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7e187",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea87fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, ndcg_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from geopy.distance import great_circle\n",
    "import hopsworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "836d3b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:28:42,378 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-04-20 12:28:42,412 INFO: Initializing external client\n",
      "2025-04-20 12:28:42,413 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:28:43,811 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hopsworks connection\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4656eb01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m query_model \u001b[38;5;241m=\u001b[39m \u001b[43mmr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m()\n\u001b[1;32m      3\u001b[0m candidate_model \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mget_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      4\u001b[0m weather_ranking_model \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mget_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_ranking_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load models\n",
    "query_model = mr.get_model(\"query_model\", version=1).load()\n",
    "candidate_model = mr.get_model(\"candidate_model\", version=1).load()\n",
    "weather_ranking_model = mr.get_model(\"weather_ranking_model\", version=1).load()\n",
    "no_weather_ranking_model = mr.get_model(\"no_weather_ranking_model\", version=1).load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621b677",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c4c99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bef304f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import ndcg_score\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6e252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:40:07,781 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-04-20 13:40:07,847 INFO: Initializing external client\n",
      "2025-04-20 13:40:07,854 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-04-20 13:40:08,525 WARNING: UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:40:09,509 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Hopsworks\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Load feature views\n",
    "weather_rank_fv = fs.get_feature_view(\"weather_ranking\", version=1)\n",
    "no_weather_rank_fv = fs.get_feature_view(\"no_weather_ranking\", version=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "EMBED_DIM = 32\n",
    "RETRIEVAL_K = 100\n",
    "RANKING_K = 10\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TFSMLayer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load models from Hopsworks Model Registry\"\"\"\n",
    "    project = hopsworks.login()\n",
    "    mr = project.get_model_registry()\n",
    "    \n",
    "    # Load retrieval components using TFSMLayer\n",
    "    query_model = tf.keras.layers.TFSMLayer(\n",
    "        mr.get_model(\"query_model\", version=1).download(),\n",
    "        call_endpoint='serving_default'\n",
    "    )\n",
    "    \n",
    "    candidate_model = tf.keras.layers.TFSMLayer(\n",
    "        mr.get_model(\"candidate_model\", version=1).download(),\n",
    "        call_endpoint='serving_default'\n",
    "    )\n",
    "    \n",
    "    # Load CatBoost models with explicit file paths\n",
    "    weather_model_path = mr.get_model(\"weather_ranking_model\", version=2).download()\n",
    "    weather_model = joblib.load(os.path.join(weather_model_path, \"weather_ranking_model.pkl\"))\n",
    "    \n",
    "    no_weather_model_path = mr.get_model(\"no_weather_ranking_model\", version=2).download()\n",
    "    no_weather_model = joblib.load(os.path.join(no_weather_model_path, \"no_weather_ranking_model.pkl\"))\n",
    "    \n",
    "    return query_model, candidate_model, weather_model, no_weather_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_candidates(query_model, candidate_model, users_df, events_df, k=RETRIEVAL_K):\n",
    "    \"\"\"Generate top-K candidates using Two-Tower model\"\"\"\n",
    "    # Create TensorFlow datasets\n",
    "    user_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"user_id\": users_df[\"user_id\"],\n",
    "        \"user_city\": users_df[\"user_city\"],\n",
    "        \"age\": users_df[\"age\"].astype(np.float32),\n",
    "        \"user_interests\": users_df[\"user_interests\"]\n",
    "    }).batch(128)\n",
    "    \n",
    "    event_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"event_id\": events_df[\"event_id\"],\n",
    "        \"event_type\": events_df[\"event_type\"],\n",
    "        \"event_city\": events_df[\"event_city\"]\n",
    "    }).batch(128)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    user_embs = query_model(user_ds)\n",
    "    event_embs = candidate_model(event_ds)\n",
    "    \n",
    "    # Calculate scores and get top K\n",
    "    scores = tf.linalg.matmul(user_embs, event_embs, transpose_b=True)\n",
    "    top_k_idx = tf.math.top_k(scores, k=k).indices.numpy()\n",
    "    \n",
    "    # Create candidate DataFrame\n",
    "    candidates = []\n",
    "    for user_idx, event_indices in enumerate(top_k_idx):\n",
    "        user_id = users_df.iloc[user_idx][\"user_id\"]\n",
    "        for event_idx in event_indices:\n",
    "            event_id = events_df.iloc[event_idx][\"event_id\"]\n",
    "            candidates.append((user_id, event_id))\n",
    "    \n",
    "    return pd.DataFrame(candidates, columns=[\"user_id\", \"event_id\"])\n",
    "\n",
    "def rank_candidates(ranking_model, candidates_df, feature_view):\n",
    "    \"\"\"Rank candidates using CatBoost model\"\"\"\n",
    "    # Get ranking features\n",
    "    features = feature_view.get_batch_data(candidates_df)\n",
    "    \n",
    "    # Generate predictions\n",
    "    if 'weather_condition' in features.columns:  # Weather model\n",
    "        features = features[['interaction_distance_to_event', 'event_type', \n",
    "                            'user_city', 'event_city', 'age', 'weather_condition',\n",
    "                            'temperature', 'attendance_rate']]\n",
    "    else:  # No-weather model\n",
    "        features = features[['interaction_distance_to_event', 'event_type',\n",
    "                            'user_city', 'event_city', 'age', 'attendance_rate']]\n",
    "    \n",
    "    rank_pool = Pool(features, cat_features=features.select_dtypes(include=[\"object\"]).columns.tolist())\n",
    "    return ranking_model.predict_proba(rank_pool)[:, 1]\n",
    "\n",
    "def evaluate_ranking(y_true, y_score, k=RANKING_K):\n",
    "    \"\"\"Calculate ranking metrics including NDCG@k\"\"\"\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_score),\n",
    "        \"Average Precision\": average_precision_score(y_true, y_score)\n",
    "    }\n",
    "    \n",
    "    sorted_idx = np.argsort(-y_score)\n",
    "    for k in [5, 10, 20]:\n",
    "        top_k = y_true[sorted_idx][:k]\n",
    "        metrics[f\"NDCG@{k}\"] = ndcg_score([y_true], [y_score], k=k)\n",
    "        metrics[f\"Precision@{k}\"] = top_k.mean()\n",
    "        metrics[f\"Recall@{k}\"] = top_k.sum() / max(y_true.sum(), 1)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efa747a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.77s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.42s) \n",
      "2025-04-20 14:53:13,076 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-04-20 14:53:13,089 INFO: Initializing external client\n",
      "2025-04-20 14:53:13,090 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-04-20 14:53:13,575 WARNING: UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 14:53:14,349 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data and models\n",
    "users_df = fs.get_feature_group(\"users\", 1).read()\n",
    "events_df = fs.get_feature_group(\"events\", 1).read()\n",
    "query_model, candidate_model, weather_model, no_weather_model = load_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe14291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate candidates\n",
    "candidates_df = generate_top_k_candidates(user_df, k=100)\n",
    "\n",
    "# Create ranking dataset\n",
    "ranking_df = weather_rank_fv.get_batch_data(candidates_df)\n",
    "no_weather_df = no_weather_rank_fv.get_batch_data(candidates_df)\n",
    "\n",
    "# Rank candidates\n",
    "weather_scores = rank_candidates(weather_model, ranking_df, weather_rank_fv)\n",
    "no_weather_scores = rank_candidates(no_weather_model, no_weather_df, no_weather_rank_fv)\n",
    "\n",
    "# Get ground truth\n",
    "interactions = fs.get_feature_group(\"interactions\", 1).read()\n",
    "y_true = candidates_df.merge(interactions, on=[\"user_id\", \"event_id\"], how=\"left\")[\"label\"].fillna(0)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Weather Model Metrics:\")\n",
    "print(evaluate_ranking(y_true, weather_scores))\n",
    "\n",
    "print(\"\\nNo-Weather Model Metrics:\")\n",
    "print(evaluate_ranking(y_true, no_weather_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/no_weather_ranking_model.pkl\n",
    "/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/weather_ranking_model.pkl\n",
    "/home/nkama/masters_thesis_project/thesis/sythesize_from_scratch/retrieve_rank_evaluate_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cd3c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, ndcg_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import Pool\n",
    "import hopsworks\n",
    "\n",
    "# Constants\n",
    "EMBED_DIM = 32\n",
    "RETRIEVAL_K = 100\n",
    "RANKING_K = 10\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load models from Hopsworks Model Registry\"\"\"\n",
    "    project = hopsworks.login()\n",
    "    mr = project.get_model_registry()\n",
    "    \n",
    "    # Load TensorFlow SavedModels using TFSMLayer\n",
    "    query_model_path = mr.get_model(\"query_model\", version=1).download()\n",
    "    query_model = tf.keras.layers.TFSMLayer(\n",
    "        query_model_path,\n",
    "        call_endpoint='serving_default'\n",
    "    )\n",
    "    \n",
    "    candidate_model_path = mr.get_model(\"candidate_model\", version=1).download()\n",
    "    candidate_model = tf.keras.layers.TFSMLayer(\n",
    "        candidate_model_path,\n",
    "        call_endpoint='serving_default'\n",
    "    )\n",
    "    \n",
    "    # Load CatBoost models with explicit paths\n",
    "    def load_catboost(model_name, version):\n",
    "        model_path = Path(mr.get_model(model_name, version=version).download())\n",
    "        model_file = next((f for f in model_path.iterdir() if f.suffix == '.pkl'), None)\n",
    "        if not model_file:\n",
    "            raise ValueError(f\"No .pkl file found in {model_path}\")\n",
    "        return joblib.load(model_path / model_file)\n",
    "    \n",
    "    weather_model = load_catboost(\"weather_ranking_model\", 1)\n",
    "    no_weather_model = load_catboost(\"no_weather_ranking_model\", 1)\n",
    "    \n",
    "    return query_model, candidate_model, weather_model, no_weather_model\n",
    "\n",
    "\n",
    "def generate_candidates(query_model, candidate_model, users_df, events_df, k=RETRIEVAL_K):\n",
    "    \"\"\"Generate top-K candidates using Two-Tower model\"\"\"\n",
    "    # Create TensorFlow datasets with original feature names\n",
    "    user_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"user_id\": users_df[\"user_id\"].values,\n",
    "        \"user_city\": users_df[\"user_city\"].values,\n",
    "        \"age\": users_df[\"age\"].values.astype(np.float32),\n",
    "        \"user_interests\": users_df[\"user_interests\"].values\n",
    "    }).batch(128)\n",
    "    \n",
    "    event_ds = tf.data.Dataset.from_tensor_slices({\n",
    "        \"event_id\": events_df[\"event_id\"].values,\n",
    "        \"event_type\": events_df[\"event_type\"].values,\n",
    "        \"event_city\": events_df[\"event_city\"].values\n",
    "    }).batch(128)\n",
    "    \n",
    "    # Generate embeddings using explicit keyword arguments\n",
    "    user_embs = []\n",
    "    for batch in user_ds:\n",
    "        user_embs.append(query_model(\n",
    "            user_id=batch['user_id'],\n",
    "            user_city=batch['user_city'],\n",
    "            age=batch['age'],\n",
    "            user_interests=batch['user_interests']\n",
    "        ))\n",
    "    user_embs = tf.concat(user_embs, axis=0)\n",
    "    \n",
    "    event_embs = []\n",
    "    for batch in event_ds:\n",
    "        event_embs.append(candidate_model(\n",
    "            event_id=batch['event_id'],\n",
    "            event_type=batch['event_type'],\n",
    "            event_city=batch['event_city']\n",
    "        ))\n",
    "    event_embs = tf.concat(event_embs, axis=0)\n",
    "    \n",
    "    # Rest of the function remains unchanged\n",
    "    scores = tf.linalg.matmul(user_embs, event_embs, transpose_b=True)\n",
    "    top_k_idx = tf.math.top_k(scores, k=k).indices.numpy()\n",
    "    \n",
    "    candidates = []\n",
    "    for user_idx, event_indices in enumerate(top_k_idx):\n",
    "        user_id = users_df.iloc[user_idx][\"user_id\"]\n",
    "        candidates.extend([(user_id, events_df.iloc[idx][\"event_id\"]) for idx in event_indices])\n",
    "    \n",
    "    return pd.DataFrame(candidates, columns=[\"user_id\", \"event_id\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rank_candidates(ranking_model, candidates_df, feature_view):\n",
    "    \"\"\"Rank candidates with proper feature preprocessing\"\"\"\n",
    "    # Get ranking features\n",
    "    features = feature_view.get_batch_data(candidates_df)\n",
    "    \n",
    "    # Handle missing weather features\n",
    "    if 'weather_condition' in features.columns:\n",
    "        features = features[['interaction_distance_to_event', 'event_type', \n",
    "                           'user_city', 'event_city', 'age', 'weather_condition',\n",
    "                           'temperature', 'attendance_rate']].fillna(method='ffill')\n",
    "    else:\n",
    "        features = features[['interaction_distance_to_event', 'event_type',\n",
    "                           'user_city', 'event_city', 'age', 'attendance_rate']]\n",
    "    \n",
    "    # Convert categorical features\n",
    "    cat_features = features.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    return ranking_model.predict_proba(Pool(features, cat_features=cat_features))[:, 1]\n",
    "\n",
    "def evaluate_ranking(y_true, y_score, k_values=[5, 10, 20]):\n",
    "    \"\"\"Calculate ranking metrics with proper NDCG calculation\"\"\"\n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(y_true, y_score),\n",
    "        \"MAP\": average_precision_score(y_true, y_score)\n",
    "    }\n",
    "    \n",
    "    sorted_idx = np.argsort(-y_score)\n",
    "    for k in k_values:\n",
    "        top_k = y_true[sorted_idx][:k]\n",
    "        metrics[f\"Precision@{k}\"] = top_k.mean()\n",
    "        metrics[f\"Recall@{k}\"] = top_k.sum() / max(y_true.sum(), 1)\n",
    "        metrics[f\"NDCG@{k}\"] = ndcg_score([y_true], [y_score], k=k)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Hopsworks connection\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Load data\n",
    "users_df = fs.get_feature_group(\"users\", 1).read()\n",
    "events_df = fs.get_feature_group(\"events\", 1).read()\n",
    "interactions_df = fs.get_feature_group(\"interactions\", 1).read()\n",
    "\n",
    "# Load models\n",
    "query_model, candidate_model, weather_model, no_weather_model = load_models()\n",
    "\n",
    "\n",
    "# Generate candidates\n",
    "candidates_df = generate_candidates(query_model, candidate_model, users_df, events_df)\n",
    "\n",
    "\n",
    "# Create ranking datasets\n",
    "weather_rank_fv = fs.get_feature_view(\"weather_ranking\", 2)\n",
    "no_weather_rank_fv = fs.get_feature_view(\"no_weather_ranking\", 2)\n",
    "\n",
    "# Get ground truth labels\n",
    "interactions_set = set(zip(interactions_df[\"user_id\"], interactions_df[\"event_id\"]))\n",
    "candidates_df[\"label\"] = candidates_df.apply(\n",
    "    lambda row: 1 if (row[\"user_id\"], row[\"event_id\"]) in interactions_set else 0, axis=1)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating Weather Model...\")\n",
    "weather_scores = rank_candidates(weather_model, candidates_df, weather_rank_fv)\n",
    "print(evaluate_ranking(candidates_df[\"label\"].values, weather_scores))\n",
    "\n",
    "print(\"\\nEvaluating No-Weather Model...\")\n",
    "no_weather_scores = rank_candidates(no_weather_model, candidates_df, no_weather_rank_fv)\n",
    "print(evaluate_ranking(candidates_df[\"label\"].values, no_weather_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdada5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Hopsworks connection\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "users_df = fs.get_feature_group(\"users\", 1).read()\n",
    "events_df = fs.get_feature_group(\"events\", 1).read()\n",
    "interactions_df = fs.get_feature_group(\"interactions\", 1).read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29032602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 15:13:18,137 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-04-20 15:13:18,145 INFO: Initializing external client\n",
      "2025-04-20 15:13:18,146 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 15:13:19,463 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load models\n",
    "query_model, candidate_model, weather_model, no_weather_model = load_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7864205b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate candidates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m candidates_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 70\u001b[0m, in \u001b[0;36mgenerate_candidates\u001b[0;34m(query_model, candidate_model, users_df, events_df, k)\u001b[0m\n\u001b[1;32m     68\u001b[0m user_embs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m user_ds:\n\u001b[0;32m---> 70\u001b[0m     user_embs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_city\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_city\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_interests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_interests\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     76\u001b[0m user_embs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(user_embs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     78\u001b[0m event_embs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/inspect.py:3179\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3176\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3177\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/inspect.py:3094\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3092\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3093\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 3094\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3096\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate candidates\n",
    "candidates_df = generate_candidates(query_model, candidate_model, users_df, events_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create ranking datasets\n",
    "weather_rank_fv = fs.get_feature_view(\"weather_ranking\", 2)\n",
    "no_weather_rank_fv = fs.get_feature_view(\"no_weather_ranking\", 2)\n",
    "\n",
    "# Get ground truth labels\n",
    "interactions_set = set(zip(interactions_df[\"user_id\"], interactions_df[\"event_id\"]))\n",
    "candidates_df[\"label\"] = candidates_df.apply(\n",
    "    lambda row: 1 if (row[\"user_id\"], row[\"event_id\"]) in interactions_set else 0, axis=1)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating Weather Model...\")\n",
    "weather_scores = rank_candidates(weather_model, candidates_df, weather_rank_fv)\n",
    "print(evaluate_ranking(candidates_df[\"label\"].values, weather_scores))\n",
    "\n",
    "print(\"\\nEvaluating No-Weather Model...\")\n",
    "no_weather_scores = rank_candidates(no_weather_model, candidates_df, no_weather_rank_fv)\n",
    "print(evaluate_ranking(candidates_df[\"label\"].values, no_weather_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9bc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b205a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688a699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8f869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0758bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TFSMLayer\n",
    "from catboost import Pool\n",
    "import hopsworks\n",
    "import joblib\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize Hopsworks connections\n",
    "        self.project = hopsworks.login()\n",
    "        self.fs = self.project.get_feature_store()\n",
    "        self.mr = self.project.get_model_registry()\n",
    "        \n",
    "        # Load feature views\n",
    "        self.users_fv = self.fs.get_feature_view(\"users\", 1)\n",
    "        self.events_fv = self.fs.get_feature_view(\"events\", 1)\n",
    "        self.interactions_fv = self.fs.get_feature_group(\"interactions\", 1)\n",
    "        self.candidate_embeddings_fv = self.fs.get_feature_view(\"events_candidate_embeddings\", 1)\n",
    "        \n",
    "        # Load models\n",
    "        self.query_model = self.load_tf_model(\"query_model\", 1)\n",
    "        self.candidate_model = self.load_tf_model(\"candidate_model\", 1)\n",
    "        self.weather_model = self.load_catboost(\"weather_ranking_model\", 1)\n",
    "        self.no_weather_model = self.load_catboost(\"no_weather_ranking_model\", 1)\n",
    "\n",
    "    def load_tf_model(self, name, version):\n",
    "        model_path = self.mr.get_model(name, version).download()\n",
    "        return TFSMLayer(model_path, call_endpoint='serving_default')\n",
    "\n",
    "    # Load CatBoost models with explicit paths\n",
    "    def load_catboost(self, model_name, version):\n",
    "        model_path = Path(mr.get_model(model_name, version=version).download())\n",
    "        model_file = next((f for f in model_path.iterdir() if f.suffix == '.pkl'), None)\n",
    "        if not model_file:\n",
    "            raise ValueError(f\"No .pkl file found in {model_path}\")\n",
    "        return joblib.load(model_path / model_file)\n",
    "    \n",
    "   \n",
    "\n",
    "    def generate_candidates(self, user_id, k=100):\n",
    "        # Get user features\n",
    "        user_features = self.users_fv.get_feature_vector({\"user_id\": user_id})\n",
    "\n",
    "        # Generate user embedding using explicit keyword arguments\n",
    "        user_embedding = self.query_model(\n",
    "            user_id=user_features['user_id'],\n",
    "            user_city=user_features['user_city'],\n",
    "            age=user_features['age'],\n",
    "            user_interests=user_features['user_interests']\n",
    "        )\n",
    "\n",
    "        # Retrieve candidate event IDs from interactions\n",
    "        seen_events = self.interactions_fv.get_feature_vector(\n",
    "            {\"user_id\": user_id}, \n",
    "            return_type=\"pandas\"\n",
    "        )[\"event_id\"].tolist()\n",
    "\n",
    "        # Create entry list for candidate embeddings using event_id\n",
    "        candidate_entries = [\n",
    "            {\"event_id\": event_id} \n",
    "            for event_id in self.events_fv.get_feature_vectors().event_id\n",
    "            if event_id not in seen_events\n",
    "        ]\n",
    "\n",
    "        # Retrieve candidate embeddings with proper entry parameter\n",
    "        candidate_embeddings = self.candidate_embeddings_fv.get_feature_vectors(\n",
    "            entry=candidate_entries,\n",
    "            allow_missing=True  # Optional: handle missing entries gracefully\n",
    "        )\n",
    "\n",
    "        # Rest of the candidate generation logic\n",
    "        scores = tf.linalg.matmul(user_embedding, candidate_embeddings, transpose_b=True)\n",
    "        top_k_idx = tf.math.top_k(scores, k=k).indices.numpy()\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            [(user_id, candidate_embeddings.iloc[idx][\"event_id\"]) \n",
    "                for idx in top_k_idx],\n",
    "            columns=[\"user_id\", \"event_id\"]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def rank_candidates(self, candidates_df, weather_data=None):\n",
    "        # Get ranking features from feature store\n",
    "        features = self.fs.get_batch_data(candidates_df)\n",
    "        \n",
    "        # Prepare feature set based on weather availability\n",
    "        if weather_data:\n",
    "            features = features[['interaction_distance', 'event_type', 'user_city',\n",
    "                               'event_city', 'age', 'weather_condition', 'temperature']]\n",
    "            model = self.weather_model\n",
    "        else:\n",
    "            features = features[['interaction_distance', 'event_type', \n",
    "                               'user_city', 'event_city', 'age']]\n",
    "            model = self.no_weather_model\n",
    "        \n",
    "        # Create CatBoost pool\n",
    "        pool = Pool(features, cat_features=features.select_dtypes(\"object\").columns.tolist())\n",
    "        return model.predict_proba(pool)[:, 1]\n",
    "\n",
    "    def evaluate(self, y_true, y_score, k_values=[5, 10, 20]):\n",
    "        metrics = {\n",
    "            \"AUC\": roc_auc_score(y_true, y_score),\n",
    "            \"MAP\": average_precision_score(y_true, y_score)\n",
    "        }\n",
    "        \n",
    "        sorted_idx = np.argsort(-y_score)\n",
    "        for k in k_values:\n",
    "            top_k = y_true[sorted_idx][:k]\n",
    "            metrics[f\"Precision@{k}\"] = top_k.mean()\n",
    "            metrics[f\"Recall@{k}\"] = top_k.sum() / max(y_true.sum(), 1)\n",
    "            metrics[f\"NDCG@{k}\"] = ndcg_score([y_true], [y_score], k=k)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "501fed60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:53:43,301 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-04-20 16:53:43,319 INFO: Initializing external client\n",
      "2025-04-20 16:53:43,321 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-04-20 16:53:43,965 WARNING: UserWarning: The installed hopsworks client version 4.1.8 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:53:44,769 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1220788\n",
      "2025-04-20 16:53:59,541 WARNING: VersionWarning: No training dataset version was provided to initialise serving. Defaulting to version 1.\n",
      "\n",
      "2025-04-20 16:54:01,575 WARNING: VersionWarning: No training dataset version was provided to initialise serving. Defaulting to version 1.\n",
      "\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "The required argument `entries` is missing. If the feature view includes only on-demand features, entries may be left empty or set to None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCD639A\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Example user from interactions\u001b[39;00m\n\u001b[1;32m      6\u001b[0m users_df \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39musers_fv\u001b[38;5;241m.\u001b[39mget_feature_vector({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_id}, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m events_df \u001b[38;5;241m=\u001b[39m \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents_fv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate candidates using Two-Tower model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m candidates_df \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mgenerate_candidates(\n\u001b[1;32m     11\u001b[0m     query_model\u001b[38;5;241m=\u001b[39mrs\u001b[38;5;241m.\u001b[39mquery_model,\n\u001b[1;32m     12\u001b[0m     candidate_model\u001b[38;5;241m=\u001b[39mrs\u001b[38;5;241m.\u001b[39mcandidate_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsfs/feature_view.py:751\u001b[0m, in \u001b[0;36mFeatureView.get_feature_vectors\u001b[0;34m(self, entry, passed_features, external, return_type, allow_missing, force_rest_client, force_sql_client, transform, request_parameters, transformation_context)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _entry \u001b[38;5;129;01min\u001b[39;00m entry:\n\u001b[1;32m    749\u001b[0m         vector_db_features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_vector_db_result(_entry))\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassed_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassed_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_db_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_db_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_rest_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_rest_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_sql_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_sql_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsfs/core/vector_server.py:495\u001b[0m, in \u001b[0;36mVectorServer.get_feature_vectors\u001b[0;34m(self, entries, return_type, passed_features, vector_db_features, request_parameters, allow_missing, force_rest_client, force_sql_client, transform, transformation_context)\u001b[0m\n\u001b[1;32m    484\u001b[0m     entries \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    485\u001b[0m         [[] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(request_parameters)]\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request_parameters, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m [[]]\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (idx, entry), passed, vector_features \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mzip_longest(\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28menumerate\u001b[39m(entries),\n\u001b[1;32m    492\u001b[0m     passed_features,\n\u001b[1;32m    493\u001b[0m     vector_db_features,\n\u001b[1;32m    494\u001b[0m ):\n\u001b[0;32m--> 495\u001b[0m     rondb_entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_entry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassed_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_db_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rondb_entry) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         rondb_entries\u001b[38;5;241m.\u001b[39mappend(rondb_entry)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/thesisenv/lib/python3.10/site-packages/hsfs/core/vector_server.py:1440\u001b[0m, in \u001b[0;36mVectorServer.validate_entry\u001b[0;34m(self, entry, allow_missing, passed_features, vector_db_features)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1440\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mFeatureStoreException(\n\u001b[1;32m   1441\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe required argument `entries` is missing. If the feature view includes only on-demand features, entries may be left empty or set to None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1442\u001b[0m         )\n\u001b[1;32m   1444\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking keys in entry are valid serving keys.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: The required argument `entries` is missing. If the feature view includes only on-demand features, entries may be left empty or set to None."
     ]
    }
   ],
   "source": [
    "# Initialize recommendation system with fixed model loading\n",
    "rs = RecommendationSystem()\n",
    "\n",
    "# Get sample user and event data from Hopsworks\n",
    "user_id = \"CD639A\"  # Example user from interactions\n",
    "users_df = rs.users_fv.get_feature_vector({\"user_id\": user_id}, return_type=\"pandas\")\n",
    "events_df = rs.events_fv.get_feature_vectors()\n",
    "\n",
    "# Generate candidates using Two-Tower model\n",
    "candidates_df = rs.generate_candidates(\n",
    "    query_model=rs.query_model,\n",
    "    candidate_model=rs.candidate_model,\n",
    "    users_df=users_df,\n",
    "    events_df=events_df,\n",
    "    k=100\n",
    ")\n",
    "\n",
    "# Get weather data (example)\n",
    "weather_data = {\n",
    "    'temperature': 18.5,\n",
    "    'weather_condition': 'sunny'\n",
    "}\n",
    "\n",
    "# Rank candidates with weather context\n",
    "weather_scores = rs.rank_candidates(candidates_df, weather_data=weather_data)\n",
    "\n",
    "# Rank candidates without weather context\n",
    "no_weather_scores = rs.rank_candidates(candidates_df)\n",
    "\n",
    "# Combine results\n",
    "final_recommendations = candidates_df.copy()\n",
    "final_recommendations['weather_score'] = weather_scores\n",
    "final_recommendations['no_weather_score'] = no_weather_scores\n",
    "\n",
    "# Get ground truth from recent interactions\n",
    "y_true = rs.interactions_fv.select([\"user_id\", \"event_id\", \"label\"])\\\n",
    "                          .filter(f\"user_id = '{user_id}'\")\\\n",
    "                          .read()[\"label\"].values\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = rs.evaluate(y_true, weather_scores)\n",
    "print(f\"Weather Model Metrics:\\n{metrics}\")\n",
    "\n",
    "metrics = rs.evaluate(y_true, no_weather_scores)\n",
    "print(f\"\\nNo-Weather Model Metrics:\\n{metrics}\")\n",
    "\n",
    "# Display top 5 recommendations\n",
    "print(\"\\nTop Weather-Based Recommendations:\")\n",
    "print(final_recommendations.sort_values('weather_score', ascending=False).head(5))\n",
    "\n",
    "print(\"\\nTop Non-Weather Recommendations:\")\n",
    "print(final_recommendations.sort_values('no_weather_score', ascending=False).head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
